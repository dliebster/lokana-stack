apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/path: /metrics
      prometheus.io/port: "9402"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-03-18T00:41:51Z"
    generateName: cert-manager-67c98b89c8-
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.14.4
      pod-template-hash: 67c98b89c8
    name: cert-manager-67c98b89c8-5qg58
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cert-manager-67c98b89c8
      uid: 320ac473-9c04-48d6-a5ad-cdd020308953
    resourceVersion: "19889"
    uid: 8f853385-ffef-4cbd-bbf1-f5136ae54fbb
  spec:
    containers:
    - args:
      - --v=2
      - --cluster-resource-namespace=$(POD_NAMESPACE)
      - --leader-election-namespace=kube-system
      - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.14.4
      - --max-concurrent-challenges=60
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/jetstack/cert-manager-controller:v1.14.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          path: /livez
          port: http-healthz
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: cert-manager-controller
      ports:
      - containerPort: 9402
        name: http-metrics
        protocol: TCP
      - containerPort: 9403
        name: http-healthz
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hql8v
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    nodeName: hippy02
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: cert-manager
    serviceAccountName: cert-manager
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-hql8v
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T00:41:51Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T00:41:55Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T00:41:55Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T00:41:51Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4ec202db7836c0db20f4e291f1b6f1cbb474da9d59d155cda36d704a2974394a
      image: quay.io/jetstack/cert-manager-controller:v1.14.4
      imageID: quay.io/jetstack/cert-manager-controller@sha256:5cffa969fd30ce6a760994d30e7cccb3626abc8015d813de52f8cfa9ff862de9
      lastState: {}
      name: cert-manager-controller
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T00:41:54Z"
    hostIP: 192.168.0.206
    phase: Running
    podIP: 10.42.2.2
    podIPs:
    - ip: 10.42.2.2
    qosClass: BestEffort
    startTime: "2024-03-18T00:41:51Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-18T00:41:51Z"
    generateName: cert-manager-cainjector-5c5695d979-
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cainjector
      app.kubernetes.io/version: v1.14.4
      pod-template-hash: 5c5695d979
    name: cert-manager-cainjector-5c5695d979-xpz97
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cert-manager-cainjector-5c5695d979
      uid: 22cd30fc-4f70-4420-841d-cd82b330fca2
    resourceVersion: "19876"
    uid: 0e72fc75-e357-4f95-b26d-b8e9170399ee
  spec:
    containers:
    - args:
      - --v=2
      - --leader-election-namespace=kube-system
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/jetstack/cert-manager-cainjector:v1.14.4
      imagePullPolicy: IfNotPresent
      name: cert-manager-cainjector
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-p6hxs
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    nodeName: hippy01
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: cert-manager-cainjector
    serviceAccountName: cert-manager-cainjector
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-p6hxs
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T00:41:51Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T00:41:54Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T00:41:54Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T00:41:51Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9e5dd3ffc58cf087c985dc085aca15bf031e0e3217911d2a292fac1bc5aeec30
      image: quay.io/jetstack/cert-manager-cainjector:v1.14.4
      imageID: quay.io/jetstack/cert-manager-cainjector@sha256:30286297e5b4b71a86759d297a8109c6a1649fdc68d28f618d87edf12a2da417
      lastState: {}
      name: cert-manager-cainjector
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T00:41:54Z"
    hostIP: 192.168.0.205
    phase: Running
    podIP: 10.42.1.2
    podIPs:
    - ip: 10.42.1.2
    qosClass: BestEffort
    startTime: "2024-03-18T00:41:51Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-19T02:11:48Z"
    generateName: cert-manager-webhook-7f9f8648b9-
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.14.4
      pod-template-hash: 7f9f8648b9
    name: cert-manager-webhook-7f9f8648b9-gkr5l
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cert-manager-webhook-7f9f8648b9
      uid: 0bbdb1e3-9836-4122-b3a5-c2e99455a5a1
    resourceVersion: "446768"
    uid: 09e93ecf-e984-44d1-803b-5b30554a5a98
  spec:
    containers:
    - args:
      - --v=2
      - --secure-port=10250
      - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
      - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
      - --dynamic-serving-dns-names=cert-manager-webhook
      - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
      - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/jetstack/cert-manager-webhook:v1.14.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: 6080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: cert-manager-webhook
      ports:
      - containerPort: 10250
        name: https
        protocol: TCP
      - containerPort: 6080
        name: healthcheck
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 6080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hqgq4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    nodeName: hippy01
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: cert-manager-webhook
    serviceAccountName: cert-manager-webhook
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-hqgq4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T02:11:48Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T02:12:03Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T02:12:03Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T02:11:48Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e6f134e4bea867f7e3f67a909445bf98f2177a3f128ecabdd707068b7a3047a5
      image: quay.io/jetstack/cert-manager-webhook:v1.14.4
      imageID: quay.io/jetstack/cert-manager-webhook@sha256:11f7e7c462da3c0329e0a1e695a7bd37d6b3c28312d4edd4cc8d36f70ecbfa63
      lastState: {}
      name: cert-manager-webhook
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-19T02:11:55Z"
    hostIP: 192.168.0.205
    phase: Running
    podIP: 10.42.1.28
    podIPs:
    - ip: 10.42.1.28
    qosClass: BestEffort
    startTime: "2024-03-19T02:11:48Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-17T23:33:22Z"
    generateName: nfs-subdir-external-provisioner-6697cf4995-
    labels:
      app: nfs-subdir-external-provisioner
      pod-template-hash: 6697cf4995
      release: nfs-subdir-external-provisioner
    name: nfs-subdir-external-provisioner-6697cf4995-f5kmp
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: nfs-subdir-external-provisioner-6697cf4995
      uid: 4725af7a-4503-4190-84b7-802dbc7efe62
    resourceVersion: "65890"
    uid: 4608cc47-857e-411c-a9a5-409f421d8e5a
  spec:
    containers:
    - env:
      - name: PROVISIONER_NAME
        value: cluster.local/nfs-subdir-external-provisioner
      - name: NFS_SERVER
        value: pi4.willow.net
      - name: NFS_PATH
        value: /mnt/a/nfs
      image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
      imagePullPolicy: IfNotPresent
      name: nfs-subdir-external-provisioner
      resources: {}
      securityContext: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /persistentvolumes
        name: nfs-subdir-external-provisioner-root
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hvql9
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: hippy
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: nfs-subdir-external-provisioner
    serviceAccountName: nfs-subdir-external-provisioner
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: nfs-subdir-external-provisioner-root
      nfs:
        path: /mnt/a/nfs
        server: pi4.willow.net
    - name: kube-api-access-hvql9
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:33:22Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T03:43:45Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T03:43:45Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:33:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://eb3b8402513b1d55d4c6ccfd0e4afe7133cb37dac6b72ffd146dda58ff16705d
      image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
      imageID: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner@sha256:63d5e04551ec8b5aae83b6f35938ca5ddc50a88d85492d9731810c31591fa4c9
      lastState: {}
      name: nfs-subdir-external-provisioner
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T03:43:44Z"
    hostIP: 192.168.0.201
    phase: Running
    podIP: 10.42.3.8
    podIPs:
    - ip: 10.42.3.8
    qosClass: BestEffort
    startTime: "2024-03-17T23:33:22Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-18T00:38:23Z"
    generateName: whoami-6bc856bfcd-
    labels:
      app: whoami
      pod-template-hash: 6bc856bfcd
    name: whoami-6bc856bfcd-vlw94
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: whoami-6bc856bfcd
      uid: 029e5201-b44e-4326-aa42-dae82babc8b0
    resourceVersion: "18950"
    uid: 9cbc1548-de1e-459f-b9c5-b8a6c6968ac5
  spec:
    containers:
    - image: traefik/whoami:v1.9.0
      imagePullPolicy: IfNotPresent
      name: whoami
      ports:
      - containerPort: 80
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-klnln
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: puck
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-klnln
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T00:38:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T00:38:27Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T00:38:27Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T00:38:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ea3101aa3f893fef5777007d75d37f6002e9e77ac1fb09133f75462232c279b7
      image: docker.io/traefik/whoami:v1.9.0
      imageID: docker.io/traefik/whoami@sha256:dbfd77023731193339012eab6ae35f474ea47d993a55a57eeeab94630733a9b0
      lastState: {}
      name: whoami
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T00:38:26Z"
    hostIP: 192.168.0.203
    phase: Running
    podIP: 10.42.5.3
    podIPs:
    - ip: 10.42.5.3
    qosClass: BestEffort
    startTime: "2024-03-18T00:38:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-17T23:20:09Z"
    generateName: coredns-6799fbcd5-
    labels:
      k8s-app: kube-dns
      pod-template-hash: 6799fbcd5
    name: coredns-6799fbcd5-kf5tg
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-6799fbcd5
      uid: 66189769-29ba-4bcf-b2c7-24b3ab53c578
    resourceVersion: "527"
    uid: 439dc40a-1b74-4472-b77d-9a2be25acfda
  spec:
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: rancher/mirrored-coredns-coredns:1.10.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 2
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /etc/coredns/custom
        name: custom-config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zg5wp
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: j1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          k8s-app: kube-dns
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: DoNotSchedule
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        - key: NodeHosts
          path: NodeHosts
        name: coredns
      name: config-volume
    - configMap:
        defaultMode: 420
        name: coredns-custom
        optional: true
      name: custom-config-volume
    - name: kube-api-access-zg5wp
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:09Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:16Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:16Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:09Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://dc54ad9d3ebd0f695877ba7c693369280a2ae0a3cecaa70fcde82a13efcff7c5
      image: docker.io/rancher/mirrored-coredns-coredns:1.10.1
      imageID: docker.io/rancher/mirrored-coredns-coredns@sha256:a11fafae1f8037cbbd66c5afa40ba2423936b72b4fd50a7034a7e8b955163594
      lastState: {}
      name: coredns
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-17T23:20:15Z"
    hostIP: 192.168.0.200
    phase: Running
    podIP: 10.42.0.2
    podIPs:
    - ip: 10.42.0.2
    qosClass: Burstable
    startTime: "2024-03-17T23:20:09Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      helmcharts.helm.cattle.io/configHash: SHA256=E3B0C44298FC1C149AFBF4C8996FB92427AE41E4649B934CA495991B7852B855
    creationTimestamp: "2024-03-17T23:20:08Z"
    generateName: helm-install-traefik-crd-
    labels:
      batch.kubernetes.io/controller-uid: 68d9457c-4a78-4154-95f3-5111f190c6cb
      batch.kubernetes.io/job-name: helm-install-traefik-crd
      controller-uid: 68d9457c-4a78-4154-95f3-5111f190c6cb
      helmcharts.helm.cattle.io/chart: traefik-crd
      job-name: helm-install-traefik-crd
    name: helm-install-traefik-crd-9rbsh
    namespace: kube-system
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: helm-install-traefik-crd
      uid: 68d9457c-4a78-4154-95f3-5111f190c6cb
    resourceVersion: "697"
    uid: b6f6e71b-bb8a-4f15-a321-facac0566b24
  spec:
    containers:
    - args:
      - install
      env:
      - name: NAME
        value: traefik-crd
      - name: VERSION
      - name: REPO
      - name: HELM_DRIVER
        value: secret
      - name: CHART_NAMESPACE
        value: kube-system
      - name: CHART
        value: https://%{KUBERNETES_API}%/static/charts/traefik-crd-25.0.2+up25.0.0.tgz
      - name: HELM_VERSION
      - name: TARGET_NAMESPACE
        value: kube-system
      - name: AUTH_PASS_CREDENTIALS
        value: "false"
      - name: NO_PROXY
        value: .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
      - name: FAILURE_POLICY
        value: reinstall
      image: rancher/klipper-helm:v0.8.2-build20230815
      imagePullPolicy: IfNotPresent
      name: helm
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bc475
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: j1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: helm-traefik-crd
    serviceAccountName: helm-traefik-crd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: values
      secret:
        defaultMode: 420
        secretName: chart-values-traefik-crd
    - configMap:
        defaultMode: 420
        name: chart-content-traefik-crd
      name: content
    - name: kube-api-access-bc475
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:08Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:25Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:25Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:08Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c8a0eb90b0a2a88434e720ae665faddaf6c74cf3c1cecf5ae0b7583c37349f76
      image: docker.io/rancher/klipper-helm:v0.8.2-build20230815
      imageID: docker.io/rancher/klipper-helm@sha256:b0b0c4f73f2391697edb52adffe4fc490de1c8590606024515bb906b2813554a
      lastState: {}
      name: helm
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://c8a0eb90b0a2a88434e720ae665faddaf6c74cf3c1cecf5ae0b7583c37349f76
          exitCode: 0
          finishedAt: "2024-03-17T23:20:24Z"
          message: |
            Installing helm_v3 chart
          reason: Completed
          startedAt: "2024-03-17T23:20:21Z"
    hostIP: 192.168.0.200
    phase: Succeeded
    podIP: 10.42.0.4
    podIPs:
    - ip: 10.42.0.4
    qosClass: BestEffort
    startTime: "2024-03-17T23:20:08Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      helmcharts.helm.cattle.io/configHash: SHA256=38EF8D37A494EC895F55E5B340E99DD0C12D75DDBBB11C66A2C5946E4702F0F8
    creationTimestamp: "2024-03-17T23:20:08Z"
    generateName: helm-install-traefik-
    labels:
      batch.kubernetes.io/controller-uid: 654ab3fc-52d6-44db-a0a3-66e7fd4bcc41
      batch.kubernetes.io/job-name: helm-install-traefik
      controller-uid: 654ab3fc-52d6-44db-a0a3-66e7fd4bcc41
      helmcharts.helm.cattle.io/chart: traefik
      job-name: helm-install-traefik
    name: helm-install-traefik-gt9bb
    namespace: kube-system
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: helm-install-traefik
      uid: 654ab3fc-52d6-44db-a0a3-66e7fd4bcc41
    resourceVersion: "720"
    uid: 1e9db985-f4d9-412d-af39-503f108c1113
  spec:
    containers:
    - args:
      - install
      - --set-string
      - global.systemDefaultRegistry=
      env:
      - name: NAME
        value: traefik
      - name: VERSION
      - name: REPO
      - name: HELM_DRIVER
        value: secret
      - name: CHART_NAMESPACE
        value: kube-system
      - name: CHART
        value: https://%{KUBERNETES_API}%/static/charts/traefik-25.0.2+up25.0.0.tgz
      - name: HELM_VERSION
      - name: TARGET_NAMESPACE
        value: kube-system
      - name: AUTH_PASS_CREDENTIALS
        value: "false"
      - name: NO_PROXY
        value: .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
      - name: FAILURE_POLICY
        value: reinstall
      image: rancher/klipper-helm:v0.8.2-build20230815
      imagePullPolicy: IfNotPresent
      name: helm
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-p8w9l
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: j1
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: helm-traefik
    serviceAccountName: helm-traefik
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: values
      secret:
        defaultMode: 420
        secretName: chart-values-traefik
    - configMap:
        defaultMode: 420
        name: chart-content-traefik
      name: content
    - name: kube-api-access-p8w9l
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:08Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:28Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:28Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:08Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a480452a54103293afcb7521bfd3d32e925cd6ebb2c56a88174262b50f84ba72
      image: docker.io/rancher/klipper-helm:v0.8.2-build20230815
      imageID: docker.io/rancher/klipper-helm@sha256:b0b0c4f73f2391697edb52adffe4fc490de1c8590606024515bb906b2813554a
      lastState: {}
      name: helm
      ready: false
      restartCount: 1
      started: false
      state:
        terminated:
          containerID: containerd://a480452a54103293afcb7521bfd3d32e925cd6ebb2c56a88174262b50f84ba72
          exitCode: 0
          finishedAt: "2024-03-17T23:20:27Z"
          message: |
            Installing helm_v3 chart
          reason: Completed
          startedAt: "2024-03-17T23:20:24Z"
    hostIP: 192.168.0.200
    phase: Succeeded
    podIP: 10.42.0.5
    podIPs:
    - ip: 10.42.0.5
    qosClass: BestEffort
    startTime: "2024-03-17T23:20:08Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-17T23:20:09Z"
    generateName: local-path-provisioner-84db5d44d9-
    labels:
      app: local-path-provisioner
      pod-template-hash: 84db5d44d9
    name: local-path-provisioner-84db5d44d9-7h9fq
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: local-path-provisioner-84db5d44d9
      uid: 717b3c8e-a1b0-48ab-9129-cb97ef18b36b
    resourceVersion: "531"
    uid: 254019a4-5cba-4873-a33a-94bc57d7a201
  spec:
    containers:
    - command:
      - local-path-provisioner
      - start
      - --config
      - /etc/config/config.json
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: rancher/local-path-provisioner:v0.0.24
      imagePullPolicy: IfNotPresent
      name: local-path-provisioner
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config/
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-69f86
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: j1
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: local-path-provisioner-service-account
    serviceAccountName: local-path-provisioner-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: local-path-config
      name: config-volume
    - name: kube-api-access-69f86
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:09Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:16Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:16Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:09Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4364b9bdc81855e3505e1f178da205d7be9f91d4ac4d687f774c66eb569c371e
      image: docker.io/rancher/local-path-provisioner:v0.0.24
      imageID: docker.io/rancher/local-path-provisioner@sha256:5bb33992a4ec3034c28b5e0b3c4c2ac35d3613b25b79455eb4b1a95adc82cdc0
      lastState: {}
      name: local-path-provisioner
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-17T23:20:15Z"
    hostIP: 192.168.0.200
    phase: Running
    podIP: 10.42.0.6
    podIPs:
    - ip: 10.42.0.6
    qosClass: BestEffort
    startTime: "2024-03-17T23:20:09Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-17T23:20:09Z"
    generateName: metrics-server-67c658944b-
    labels:
      k8s-app: metrics-server
      pod-template-hash: 67c658944b
    name: metrics-server-67c658944b-klwnb
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: metrics-server-67c658944b
      uid: b4917a1b-8536-416c-953e-122100c81f3b
    resourceVersion: "748"
    uid: 68393e09-7121-4c18-a867-0e85d32177d2
  spec:
    containers:
    - args:
      - --cert-dir=/tmp
      - --secure-port=10250
      - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      - --kubelet-use-node-status-port
      - --metric-resolution=15s
      - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305
      image: rancher/mirrored-metrics-server:v0.6.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: https
          scheme: HTTPS
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: metrics-server
      ports:
      - containerPort: 10250
        name: https
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: https
          scheme: HTTPS
        periodSeconds: 2
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 1000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-85ms5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: j1
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: metrics-server
    serviceAccountName: metrics-server
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp-dir
    - name: kube-api-access-85ms5
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:09Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:33Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:33Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:09Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a44d1dd3567e74867899b5d3220b9b842bf0f68a92693d160457e170366f8126
      image: docker.io/rancher/mirrored-metrics-server:v0.6.3
      imageID: docker.io/rancher/mirrored-metrics-server@sha256:c2dfd72bafd6406ed306d9fbd07f55c496b004293d13d3de88a4567eacc36558
      lastState: {}
      name: metrics-server
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-17T23:20:17Z"
    hostIP: 192.168.0.200
    phase: Running
    podIP: 10.42.0.3
    podIPs:
    - ip: 10.42.0.3
    qosClass: Burstable
    startTime: "2024-03-17T23:20:09Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-17T23:20:27Z"
    generateName: svclb-traefik-2f617606-
    labels:
      app: svclb-traefik-2f617606
      controller-revision-hash: 5fff58c5df
      pod-template-generation: "1"
      svccontroller.k3s.cattle.io/svcname: traefik
      svccontroller.k3s.cattle.io/svcnamespace: kube-system
    name: svclb-traefik-2f617606-9pmcq
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: svclb-traefik-2f617606
      uid: d1658021-af5b-4bd7-aadd-91227774b527
    resourceVersion: "731"
    uid: eb244612-f202-4984-ba6e-dd1f9f3337aa
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - j1
    automountServiceAccountToken: false
    containers:
    - env:
      - name: SRC_PORT
        value: "80"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "80"
      - name: DEST_IPS
        value: 10.43.238.91
      image: rancher/klipper-lb:v0.4.4
      imagePullPolicy: IfNotPresent
      name: lb-tcp-80
      ports:
      - containerPort: 80
        hostPort: 80
        name: lb-tcp-80
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    - env:
      - name: SRC_PORT
        value: "443"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "443"
      - name: DEST_IPS
        value: 10.43.238.91
      image: rancher/klipper-lb:v0.4.4
      imagePullPolicy: IfNotPresent
      name: lb-tcp-443
      ports:
      - containerPort: 443
        hostPort: 443
        name: lb-tcp-443
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: j1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      sysctls:
      - name: net.ipv4.ip_forward
        value: "1"
    serviceAccount: svclb
    serviceAccountName: svclb
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:27Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:31Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:31Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:27Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://2b79275a8a6869ebc98aabf205cb56af8900f1f5b4297984f2df1304d8a01d32
      image: docker.io/rancher/klipper-lb:v0.4.4
      imageID: docker.io/rancher/klipper-lb@sha256:d6780e97ac25454b56f88410b236d52572518040f11d0db5c6baaac0d2fcf860
      lastState: {}
      name: lb-tcp-443
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-17T23:20:31Z"
    - containerID: containerd://c2fc6a28dae95fe6f590dcfa71b0e8bd48d9e7d3cc1034adde8fc58ae9b9b2d0
      image: docker.io/rancher/klipper-lb:v0.4.4
      imageID: docker.io/rancher/klipper-lb@sha256:d6780e97ac25454b56f88410b236d52572518040f11d0db5c6baaac0d2fcf860
      lastState: {}
      name: lb-tcp-80
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-17T23:20:31Z"
    hostIP: 192.168.0.200
    phase: Running
    podIP: 10.42.0.7
    podIPs:
    - ip: 10.42.0.7
    qosClass: BestEffort
    startTime: "2024-03-17T23:20:27Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-17T23:21:59Z"
    generateName: svclb-traefik-2f617606-
    labels:
      app: svclb-traefik-2f617606
      controller-revision-hash: 5fff58c5df
      pod-template-generation: "1"
      svccontroller.k3s.cattle.io/svcname: traefik
      svccontroller.k3s.cattle.io/svcnamespace: kube-system
    name: svclb-traefik-2f617606-cf2pf
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: svclb-traefik-2f617606
      uid: d1658021-af5b-4bd7-aadd-91227774b527
    resourceVersion: "1414"
    uid: 3213e9ea-99f1-4839-babe-0ce9869dbb20
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - pi
    automountServiceAccountToken: false
    containers:
    - env:
      - name: SRC_PORT
        value: "80"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "80"
      - name: DEST_IPS
        value: 10.43.238.91
      image: rancher/klipper-lb:v0.4.4
      imagePullPolicy: IfNotPresent
      name: lb-tcp-80
      ports:
      - containerPort: 80
        hostPort: 80
        name: lb-tcp-80
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    - env:
      - name: SRC_PORT
        value: "443"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "443"
      - name: DEST_IPS
        value: 10.43.238.91
      image: rancher/klipper-lb:v0.4.4
      imagePullPolicy: IfNotPresent
      name: lb-tcp-443
      ports:
      - containerPort: 443
        hostPort: 443
        name: lb-tcp-443
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: pi
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      sysctls:
      - name: net.ipv4.ip_forward
        value: "1"
    serviceAccount: svclb
    serviceAccountName: svclb
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:22:00Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:22:11Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:22:11Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:21:59Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://0ff2e2b952ddfa3bbe4ab254e1af0152e91030d53faadff71eafb63fa1715b94
      image: docker.io/rancher/klipper-lb:v0.4.4
      imageID: docker.io/rancher/klipper-lb@sha256:d6780e97ac25454b56f88410b236d52572518040f11d0db5c6baaac0d2fcf860
      lastState: {}
      name: lb-tcp-443
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-17T23:22:10Z"
    - containerID: containerd://3b9e3948caa21f527c3f6dd5f9177a18d45a07e1c2a570c663e331dcf57ce71f
      image: docker.io/rancher/klipper-lb:v0.4.4
      imageID: docker.io/rancher/klipper-lb@sha256:d6780e97ac25454b56f88410b236d52572518040f11d0db5c6baaac0d2fcf860
      lastState: {}
      name: lb-tcp-80
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-17T23:22:10Z"
    hostIP: 192.168.0.204
    phase: Running
    podIP: 10.42.6.2
    podIPs:
    - ip: 10.42.6.2
    qosClass: BestEffort
    startTime: "2024-03-17T23:22:00Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-17T23:21:42Z"
    generateName: svclb-traefik-2f617606-
    labels:
      app: svclb-traefik-2f617606
      controller-revision-hash: 5fff58c5df
      pod-template-generation: "1"
      svccontroller.k3s.cattle.io/svcname: traefik
      svccontroller.k3s.cattle.io/svcnamespace: kube-system
    name: svclb-traefik-2f617606-f5cfn
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: svclb-traefik-2f617606
      uid: d1658021-af5b-4bd7-aadd-91227774b527
    resourceVersion: "471998"
    uid: d55ed22d-7209-4070-b360-fcdca85ae059
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - lenny
    automountServiceAccountToken: false
    containers:
    - env:
      - name: SRC_PORT
        value: "80"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "80"
      - name: DEST_IPS
        value: 10.43.238.91
      image: rancher/klipper-lb:v0.4.4
      imagePullPolicy: IfNotPresent
      name: lb-tcp-80
      ports:
      - containerPort: 80
        hostPort: 80
        name: lb-tcp-80
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    - env:
      - name: SRC_PORT
        value: "443"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "443"
      - name: DEST_IPS
        value: 10.43.238.91
      image: rancher/klipper-lb:v0.4.4
      imagePullPolicy: IfNotPresent
      name: lb-tcp-443
      ports:
      - containerPort: 443
        hostPort: 443
        name: lb-tcp-443
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: lenny
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      sysctls:
      - name: net.ipv4.ip_forward
        value: "1"
    serviceAccount: svclb
    serviceAccountName: svclb
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:21:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T03:48:21Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T03:48:21Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:21:42Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://77f0ed04a3adcacd13da0d83d00595e2e09de0435bbd8969a9f9f036820ee451
      image: docker.io/rancher/klipper-lb:v0.4.4
      imageID: docker.io/rancher/klipper-lb@sha256:d6780e97ac25454b56f88410b236d52572518040f11d0db5c6baaac0d2fcf860
      lastState:
        terminated:
          containerID: containerd://8661c41a780f0909631c46169119efc7bc2a8b7a0d400139bddb46b6bf4ed0b3
          exitCode: 255
          finishedAt: "2024-03-19T03:48:18Z"
          reason: Unknown
          startedAt: "2024-03-17T23:21:47Z"
      name: lb-tcp-443
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2024-03-19T03:48:21Z"
    - containerID: containerd://e7367a249d0762ab1bfd57365df81d8cdecc906ff6f70bb747b66ebb79325555
      image: docker.io/rancher/klipper-lb:v0.4.4
      imageID: docker.io/rancher/klipper-lb@sha256:d6780e97ac25454b56f88410b236d52572518040f11d0db5c6baaac0d2fcf860
      lastState:
        terminated:
          containerID: containerd://c317ad86631a8c372fd171a548a28732f0ef144f11dbae043633e1f0ae9da8b0
          exitCode: 255
          finishedAt: "2024-03-19T03:48:18Z"
          reason: Unknown
          startedAt: "2024-03-17T23:21:47Z"
      name: lb-tcp-80
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2024-03-19T03:48:21Z"
    hostIP: 192.168.0.202
    phase: Running
    podIP: 10.42.4.32
    podIPs:
    - ip: 10.42.4.32
    qosClass: BestEffort
    startTime: "2024-03-17T23:21:43Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-17T23:21:44Z"
    generateName: svclb-traefik-2f617606-
    labels:
      app: svclb-traefik-2f617606
      controller-revision-hash: 5fff58c5df
      pod-template-generation: "1"
      svccontroller.k3s.cattle.io/svcname: traefik
      svccontroller.k3s.cattle.io/svcnamespace: kube-system
    name: svclb-traefik-2f617606-hbchp
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: svclb-traefik-2f617606
      uid: d1658021-af5b-4bd7-aadd-91227774b527
    resourceVersion: "1297"
    uid: 060c92f1-0b2c-4849-b018-a32770000a29
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - puck
    automountServiceAccountToken: false
    containers:
    - env:
      - name: SRC_PORT
        value: "80"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "80"
      - name: DEST_IPS
        value: 10.43.238.91
      image: rancher/klipper-lb:v0.4.4
      imagePullPolicy: IfNotPresent
      name: lb-tcp-80
      ports:
      - containerPort: 80
        hostPort: 80
        name: lb-tcp-80
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    - env:
      - name: SRC_PORT
        value: "443"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "443"
      - name: DEST_IPS
        value: 10.43.238.91
      image: rancher/klipper-lb:v0.4.4
      imagePullPolicy: IfNotPresent
      name: lb-tcp-443
      ports:
      - containerPort: 443
        hostPort: 443
        name: lb-tcp-443
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: puck
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      sysctls:
      - name: net.ipv4.ip_forward
        value: "1"
    serviceAccount: svclb
    serviceAccountName: svclb
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:21:45Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:21:50Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:21:50Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:21:44Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4e9f80448e95046df112efcb872b5f1080349cb0e91003bd97785eba983a214f
      image: docker.io/rancher/klipper-lb:v0.4.4
      imageID: docker.io/rancher/klipper-lb@sha256:d6780e97ac25454b56f88410b236d52572518040f11d0db5c6baaac0d2fcf860
      lastState: {}
      name: lb-tcp-443
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-17T23:21:50Z"
    - containerID: containerd://708082de3f1a6b823727550d96c6793951b1abac81d00241910f3e1a983f346c
      image: docker.io/rancher/klipper-lb:v0.4.4
      imageID: docker.io/rancher/klipper-lb@sha256:d6780e97ac25454b56f88410b236d52572518040f11d0db5c6baaac0d2fcf860
      lastState: {}
      name: lb-tcp-80
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-17T23:21:50Z"
    hostIP: 192.168.0.203
    phase: Running
    podIP: 10.42.5.2
    podIPs:
    - ip: 10.42.5.2
    qosClass: BestEffort
    startTime: "2024-03-17T23:21:45Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-17T23:20:42Z"
    generateName: svclb-traefik-2f617606-
    labels:
      app: svclb-traefik-2f617606
      controller-revision-hash: 5fff58c5df
      pod-template-generation: "1"
      svccontroller.k3s.cattle.io/svcname: traefik
      svccontroller.k3s.cattle.io/svcnamespace: kube-system
    name: svclb-traefik-2f617606-ngzhp
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: svclb-traefik-2f617606
      uid: d1658021-af5b-4bd7-aadd-91227774b527
    resourceVersion: "864"
    uid: 30886b5e-609a-4dd8-9047-ceb7d95ede97
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - hippy01
    automountServiceAccountToken: false
    containers:
    - env:
      - name: SRC_PORT
        value: "80"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "80"
      - name: DEST_IPS
        value: 10.43.238.91
      image: rancher/klipper-lb:v0.4.4
      imagePullPolicy: IfNotPresent
      name: lb-tcp-80
      ports:
      - containerPort: 80
        hostPort: 80
        name: lb-tcp-80
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    - env:
      - name: SRC_PORT
        value: "443"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "443"
      - name: DEST_IPS
        value: 10.43.238.91
      image: rancher/klipper-lb:v0.4.4
      imagePullPolicy: IfNotPresent
      name: lb-tcp-443
      ports:
      - containerPort: 443
        hostPort: 443
        name: lb-tcp-443
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: hippy01
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      sysctls:
      - name: net.ipv4.ip_forward
        value: "1"
    serviceAccount: svclb
    serviceAccountName: svclb
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:42Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:45Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:45Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:42Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://515121d536e0cdefd1c04bc2c74c468613d996db07fc605938a37fd3e221ad6a
      image: docker.io/rancher/klipper-lb:v0.4.4
      imageID: docker.io/rancher/klipper-lb@sha256:d6780e97ac25454b56f88410b236d52572518040f11d0db5c6baaac0d2fcf860
      lastState: {}
      name: lb-tcp-443
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-17T23:20:44Z"
    - containerID: containerd://b1c78f47296a10e79c213ac7b18905f90dab1a3f00ade4e0f28e4a8f23256d4f
      image: docker.io/rancher/klipper-lb:v0.4.4
      imageID: docker.io/rancher/klipper-lb@sha256:d6780e97ac25454b56f88410b236d52572518040f11d0db5c6baaac0d2fcf860
      lastState: {}
      name: lb-tcp-80
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-17T23:20:44Z"
    hostIP: 192.168.0.205
    phase: Running
    podIP: 10.42.3.2
    podIPs:
    - ip: 10.42.3.2
    qosClass: BestEffort
    startTime: "2024-03-17T23:20:42Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-17T23:21:41Z"
    generateName: svclb-traefik-2f617606-
    labels:
      app: svclb-traefik-2f617606
      controller-revision-hash: 5fff58c5df
      pod-template-generation: "1"
      svccontroller.k3s.cattle.io/svcname: traefik
      svccontroller.k3s.cattle.io/svcnamespace: kube-system
    name: svclb-traefik-2f617606-ntzhs
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: svclb-traefik-2f617606
      uid: d1658021-af5b-4bd7-aadd-91227774b527
    resourceVersion: "1217"
    uid: 239dc31c-1510-45a9-b9fc-ae09068a012a
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - hippy
    automountServiceAccountToken: false
    containers:
    - env:
      - name: SRC_PORT
        value: "80"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "80"
      - name: DEST_IPS
        value: 10.43.238.91
      image: rancher/klipper-lb:v0.4.4
      imagePullPolicy: IfNotPresent
      name: lb-tcp-80
      ports:
      - containerPort: 80
        hostPort: 80
        name: lb-tcp-80
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    - env:
      - name: SRC_PORT
        value: "443"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "443"
      - name: DEST_IPS
        value: 10.43.238.91
      image: rancher/klipper-lb:v0.4.4
      imagePullPolicy: IfNotPresent
      name: lb-tcp-443
      ports:
      - containerPort: 443
        hostPort: 443
        name: lb-tcp-443
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: hippy
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      sysctls:
      - name: net.ipv4.ip_forward
        value: "1"
    serviceAccount: svclb
    serviceAccountName: svclb
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:21:42Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:21:43Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:21:43Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:21:41Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9fc810434db0d9c51fda28a74e35a4d5318c2dbf2f01f9de62a20e3f113555e9
      image: docker.io/rancher/klipper-lb:v0.4.4
      imageID: docker.io/rancher/klipper-lb@sha256:d6780e97ac25454b56f88410b236d52572518040f11d0db5c6baaac0d2fcf860
      lastState: {}
      name: lb-tcp-443
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-17T23:21:42Z"
    - containerID: containerd://f739d437bfcb4e4e1dc8945e6578a09a29b227680419e36423632a79080e93f3
      image: docker.io/rancher/klipper-lb:v0.4.4
      imageID: docker.io/rancher/klipper-lb@sha256:d6780e97ac25454b56f88410b236d52572518040f11d0db5c6baaac0d2fcf860
      lastState: {}
      name: lb-tcp-80
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-17T23:21:42Z"
    hostIP: 192.168.0.201
    phase: Running
    podIP: 10.42.5.2
    podIPs:
    - ip: 10.42.5.2
    qosClass: BestEffort
    startTime: "2024-03-17T23:21:42Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-17T23:20:55Z"
    generateName: svclb-traefik-2f617606-
    labels:
      app: svclb-traefik-2f617606
      controller-revision-hash: 5fff58c5df
      pod-template-generation: "1"
      svccontroller.k3s.cattle.io/svcname: traefik
      svccontroller.k3s.cattle.io/svcnamespace: kube-system
    name: svclb-traefik-2f617606-pqxt6
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: svclb-traefik-2f617606
      uid: d1658021-af5b-4bd7-aadd-91227774b527
    resourceVersion: "980"
    uid: b997bd67-10b8-40ba-9cb0-7076f513978c
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - hippy02
    automountServiceAccountToken: false
    containers:
    - env:
      - name: SRC_PORT
        value: "80"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "80"
      - name: DEST_IPS
        value: 10.43.238.91
      image: rancher/klipper-lb:v0.4.4
      imagePullPolicy: IfNotPresent
      name: lb-tcp-80
      ports:
      - containerPort: 80
        hostPort: 80
        name: lb-tcp-80
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    - env:
      - name: SRC_PORT
        value: "443"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "443"
      - name: DEST_IPS
        value: 10.43.238.91
      image: rancher/klipper-lb:v0.4.4
      imagePullPolicy: IfNotPresent
      name: lb-tcp-443
      ports:
      - containerPort: 443
        hostPort: 443
        name: lb-tcp-443
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: hippy02
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      sysctls:
      - name: net.ipv4.ip_forward
        value: "1"
    serviceAccount: svclb
    serviceAccountName: svclb
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:57Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:57Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:55Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9ec6c6020112619068540d67350d0ce039a6b5ea4da3a0cef1f008394549ad6e
      image: docker.io/rancher/klipper-lb:v0.4.4
      imageID: docker.io/rancher/klipper-lb@sha256:d6780e97ac25454b56f88410b236d52572518040f11d0db5c6baaac0d2fcf860
      lastState: {}
      name: lb-tcp-443
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-17T23:20:57Z"
    - containerID: containerd://76481b532b5d8ccae01b40491551e28d5068f27782c26734e6995b58beb73732
      image: docker.io/rancher/klipper-lb:v0.4.4
      imageID: docker.io/rancher/klipper-lb@sha256:d6780e97ac25454b56f88410b236d52572518040f11d0db5c6baaac0d2fcf860
      lastState: {}
      name: lb-tcp-80
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-17T23:20:57Z"
    hostIP: 192.168.0.206
    phase: Running
    podIP: 10.42.4.2
    podIPs:
    - ip: 10.42.4.2
    qosClass: BestEffort
    startTime: "2024-03-17T23:20:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/path: /metrics
      prometheus.io/port: "9100"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-03-17T23:20:27Z"
    generateName: traefik-f4564c4f4-
    labels:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: traefik
      helm.sh/chart: traefik-25.0.2_up25.0.0
      pod-template-hash: f4564c4f4
    name: traefik-f4564c4f4-h2mxq
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: traefik-f4564c4f4
      uid: 9651d3ea-a83f-4cfe-b0e5-3e17bb07a5d7
    resourceVersion: "759"
    uid: b012342a-7e93-4051-9d4f-fe8032dea107
  spec:
    containers:
    - args:
      - --global.checknewversion
      - --global.sendanonymoususage
      - --entrypoints.metrics.address=:9100/tcp
      - --entrypoints.traefik.address=:9000/tcp
      - --entrypoints.web.address=:8000/tcp
      - --entrypoints.websecure.address=:8443/tcp
      - --api.dashboard=true
      - --ping=true
      - --metrics.prometheus=true
      - --metrics.prometheus.entrypoint=metrics
      - --providers.kubernetescrd
      - --providers.kubernetesingress
      - --providers.kubernetesingress.ingressendpoint.publishedservice=kube-system/traefik
      - --entrypoints.websecure.http.tls=true
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: rancher/mirrored-library-traefik:2.10.5
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /ping
          port: 9000
          scheme: HTTP
        initialDelaySeconds: 2
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      name: traefik
      ports:
      - containerPort: 9100
        name: metrics
        protocol: TCP
      - containerPort: 9000
        name: traefik
        protocol: TCP
      - containerPort: 8000
        name: web
        protocol: TCP
      - containerPort: 8443
        name: websecure
        protocol: TCP
      readinessProbe:
        failureThreshold: 1
        httpGet:
          path: /ping
          port: 9000
          scheme: HTTP
        initialDelaySeconds: 2
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /data
        name: data
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hjq2k
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: j1
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroupChangePolicy: OnRootMismatch
      runAsGroup: 65532
      runAsNonRoot: true
      runAsUser: 65532
    serviceAccount: traefik
    serviceAccountName: traefik
    terminationGracePeriodSeconds: 60
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: data
    - emptyDir: {}
      name: tmp
    - name: kube-api-access-hjq2k
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:27Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:35Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:35Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-17T23:20:27Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://7a776d47ccf4876b0144482a5f6ac5efa45b86b2483d2387c2dcd31b76436098
      image: docker.io/rancher/mirrored-library-traefik:2.10.5
      imageID: docker.io/rancher/mirrored-library-traefik@sha256:ca9c8fbe001070c546a75184e3fd7f08c3e47dfc1e89bff6fe2edd302accfaec
      lastState: {}
      name: traefik
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-17T23:20:33Z"
    hostIP: 192.168.0.200
    phase: Running
    podIP: 10.42.0.8
    podIPs:
    - ip: 10.42.0.8
    qosClass: BestEffort
    startTime: "2024-03-17T23:20:27Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3
      checksum/sc-dashboard-provider-config: 593c0a8778b83f11fe80ccb21dfb20bc46705e2be3178df1dc4c89d164c8cd9c
      checksum/secret: 7d9b71b08e8c49db87a134b77770b68f4b5a1d4241569c6f9644b04db3290619
      kubectl.kubernetes.io/default-container: grafana
    creationTimestamp: "2024-03-18T22:04:49Z"
    generateName: grafana-87fc8898f-
    labels:
      app.kubernetes.io/instance: grafana
      app.kubernetes.io/name: grafana
      pod-template-hash: 87fc8898f
    name: grafana-87fc8898f-22g5v
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: grafana-87fc8898f
      uid: ee5d2f78-4591-443d-a74f-e9ecb2672bf9
    resourceVersion: "375449"
    uid: c47489c5-d73d-471d-bc88-9d5463a4a121
  spec:
    automountServiceAccountToken: true
    containers:
    - env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: GF_SECURITY_ADMIN_USER
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: grafana
      - name: GF_SECURITY_ADMIN_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: grafana
      - name: GF_PATHS_DATA
        value: /var/lib/grafana/
      - name: GF_PATHS_LOGS
        value: /var/log/grafana
      - name: GF_PATHS_PLUGINS
        value: /var/lib/grafana/plugins
      - name: GF_PATHS_PROVISIONING
        value: /etc/grafana/provisioning
      image: docker.io/grafana/grafana:10.4.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /api/health
          port: 3000
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      name: grafana
      ports:
      - containerPort: 3000
        name: grafana
        protocol: TCP
      - containerPort: 9094
        name: gossip-tcp
        protocol: TCP
      - containerPort: 9094
        name: gossip-udp
        protocol: UDP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /api/health
          port: 3000
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/grafana/grafana.ini
        name: config
        subPath: grafana.ini
      - mountPath: /var/lib/grafana
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xncg4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: puck
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 472
      runAsGroup: 472
      runAsNonRoot: true
      runAsUser: 472
    serviceAccount: grafana
    serviceAccountName: grafana
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: grafana
      name: config
    - emptyDir: {}
      name: storage
    - name: kube-api-access-xncg4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T22:04:49Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T22:05:09Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T22:05:09Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T22:04:49Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://2a578a243ef9f355bfdfede6caae4ae18fdd6885874fc1527ce050de6c0285d3
      image: docker.io/grafana/grafana:10.4.0
      imageID: docker.io/grafana/grafana@sha256:f9811e4e687ffecf1a43adb9b64096c50bc0d7a782f8608530f478b6542de7d5
      lastState: {}
      name: grafana
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T22:04:58Z"
    hostIP: 192.168.0.203
    phase: Running
    podIP: 10.42.5.14
    podIPs:
    - ip: 10.42.5.14
    qosClass: BestEffort
    startTime: "2024-03-18T22:04:49Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 2c5e95cf016dd90c7bd90fa0fb16372cc8765ad669608df363d1f2d73f08832b
    creationTimestamp: "2024-03-18T23:26:12Z"
    generateName: loki-backend-
    labels:
      app.kubernetes.io/component: backend
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
      app.kubernetes.io/part-of: memberlist
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: loki-backend-67bbcff8d9
      statefulset.kubernetes.io/pod-name: loki-backend-0
    name: loki-backend-0
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: loki-backend
      uid: 19214106-2191-4caa-a5ef-4a111f76c2a1
    resourceVersion: "402220"
    uid: a7b4c477-7151-409c-932e-ef4d3570f662
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: backend
              app.kubernetes.io/instance: loki
              app.kubernetes.io/name: loki
          topologyKey: kubernetes.io/hostname
    automountServiceAccountToken: true
    containers:
    - env:
      - name: METHOD
        value: WATCH
      - name: LABEL
        value: loki_rule
      - name: FOLDER
        value: /rules
      - name: RESOURCE
        value: both
      - name: WATCH_SERVER_TIMEOUT
        value: "60"
      - name: WATCH_CLIENT_TIMEOUT
        value: "60"
      - name: LOG_LEVEL
        value: INFO
      image: kiwigrid/k8s-sidecar:1.24.3
      imagePullPolicy: IfNotPresent
      name: loki-sc-rules
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /rules
        name: sc-rules-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xj8cd
        readOnly: true
    - args:
      - -config.file=/etc/loki/config/config.yaml
      - -target=backend
      - -legacy-read-mode=false
      image: docker.io/grafana/loki:2.9.4
      imagePullPolicy: IfNotPresent
      name: loki
      ports:
      - containerPort: 3100
        name: http-metrics
        protocol: TCP
      - containerPort: 9095
        name: grpc
        protocol: TCP
      - containerPort: 7946
        name: http-memberlist
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/loki/config
        name: config
      - mountPath: /etc/loki/runtime-config
        name: runtime-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/loki
        name: data
      - mountPath: /rules
        name: sc-rules-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xj8cd
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: loki-backend-0
    nodeName: hippy
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: loki
    serviceAccountName: loki
    subdomain: loki-backend-headless
    terminationGracePeriodSeconds: 300
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-loki-backend-0
    - emptyDir: {}
      name: tmp
    - configMap:
        defaultMode: 420
        items:
        - key: config.yaml
          path: config.yaml
        name: loki
      name: config
    - configMap:
        defaultMode: 420
        name: loki-runtime
      name: runtime-config
    - emptyDir: {}
      name: sc-rules-volume
    - name: kube-api-access-xj8cd
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:14Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:55Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:55Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:14Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b027af81623a31a907818ecca55eda5ad771b775d7d722e34865eeb5ef026fb9
      image: docker.io/grafana/loki:2.9.4
      imageID: docker.io/grafana/loki@sha256:f379a20ce9dd815884ed6446aad8819b81a8ba4d36b548ca14be8cecbc6cbca0
      lastState: {}
      name: loki
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:16Z"
    - containerID: containerd://e2311a6f3ec9eb1c65a31006c6a5409fb23c3bb3ca6a2febe49ce6c3494af74e
      image: docker.io/kiwigrid/k8s-sidecar:1.24.3
      imageID: docker.io/kiwigrid/k8s-sidecar@sha256:5af76eebbba79edf4f7471bf1c3d5f2b40858114730c92d95eafe5716abe1fe8
      lastState: {}
      name: loki-sc-rules
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:16Z"
    hostIP: 192.168.0.201
    phase: Running
    podIP: 10.42.3.49
    podIPs:
    - ip: 10.42.3.49
    qosClass: BestEffort
    startTime: "2024-03-18T23:26:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 2c5e95cf016dd90c7bd90fa0fb16372cc8765ad669608df363d1f2d73f08832b
    creationTimestamp: "2024-03-18T23:26:12Z"
    generateName: loki-backend-
    labels:
      app.kubernetes.io/component: backend
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
      app.kubernetes.io/part-of: memberlist
      apps.kubernetes.io/pod-index: "1"
      controller-revision-hash: loki-backend-67bbcff8d9
      statefulset.kubernetes.io/pod-name: loki-backend-1
    name: loki-backend-1
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: loki-backend
      uid: 19214106-2191-4caa-a5ef-4a111f76c2a1
    resourceVersion: "402230"
    uid: 6c167480-d3e1-425f-9955-36214de8053d
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: backend
              app.kubernetes.io/instance: loki
              app.kubernetes.io/name: loki
          topologyKey: kubernetes.io/hostname
    automountServiceAccountToken: true
    containers:
    - env:
      - name: METHOD
        value: WATCH
      - name: LABEL
        value: loki_rule
      - name: FOLDER
        value: /rules
      - name: RESOURCE
        value: both
      - name: WATCH_SERVER_TIMEOUT
        value: "60"
      - name: WATCH_CLIENT_TIMEOUT
        value: "60"
      - name: LOG_LEVEL
        value: INFO
      image: kiwigrid/k8s-sidecar:1.24.3
      imagePullPolicy: IfNotPresent
      name: loki-sc-rules
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /rules
        name: sc-rules-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-474qq
        readOnly: true
    - args:
      - -config.file=/etc/loki/config/config.yaml
      - -target=backend
      - -legacy-read-mode=false
      image: docker.io/grafana/loki:2.9.4
      imagePullPolicy: IfNotPresent
      name: loki
      ports:
      - containerPort: 3100
        name: http-metrics
        protocol: TCP
      - containerPort: 9095
        name: grpc
        protocol: TCP
      - containerPort: 7946
        name: http-memberlist
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/loki/config
        name: config
      - mountPath: /etc/loki/runtime-config
        name: runtime-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/loki
        name: data
      - mountPath: /rules
        name: sc-rules-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-474qq
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: loki-backend-1
    nodeName: hippy02
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: loki
    serviceAccountName: loki
    subdomain: loki-backend-headless
    terminationGracePeriodSeconds: 300
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-loki-backend-1
    - emptyDir: {}
      name: tmp
    - configMap:
        defaultMode: 420
        items:
        - key: config.yaml
          path: config.yaml
        name: loki
      name: config
    - configMap:
        defaultMode: 420
        name: loki-runtime
      name: runtime-config
    - emptyDir: {}
      name: sc-rules-volume
    - name: kube-api-access-474qq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:15Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:55Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:55Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:15Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a58df35dc33f67cef4346c9096a4f7dfe4d2f4adc33c9516425278dcfea59c0a
      image: docker.io/grafana/loki:2.9.4
      imageID: docker.io/grafana/loki@sha256:f379a20ce9dd815884ed6446aad8819b81a8ba4d36b548ca14be8cecbc6cbca0
      lastState: {}
      name: loki
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:21Z"
    - containerID: containerd://ed0e59cb997a6e43e69b1610af2c27e781add11ee2d9a9b9b75ecfd1f78837ba
      image: docker.io/kiwigrid/k8s-sidecar:1.24.3
      imageID: docker.io/kiwigrid/k8s-sidecar@sha256:5af76eebbba79edf4f7471bf1c3d5f2b40858114730c92d95eafe5716abe1fe8
      lastState: {}
      name: loki-sc-rules
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:21Z"
    hostIP: 192.168.0.206
    phase: Running
    podIP: 10.42.2.30
    podIPs:
    - ip: 10.42.2.30
    qosClass: BestEffort
    startTime: "2024-03-18T23:26:15Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 2c5e95cf016dd90c7bd90fa0fb16372cc8765ad669608df363d1f2d73f08832b
    creationTimestamp: "2024-03-18T23:26:12Z"
    generateName: loki-backend-
    labels:
      app.kubernetes.io/component: backend
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
      app.kubernetes.io/part-of: memberlist
      apps.kubernetes.io/pod-index: "2"
      controller-revision-hash: loki-backend-67bbcff8d9
      statefulset.kubernetes.io/pod-name: loki-backend-2
    name: loki-backend-2
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: loki-backend
      uid: 19214106-2191-4caa-a5ef-4a111f76c2a1
    resourceVersion: "402246"
    uid: e9236ed4-3f80-444c-956f-369f8aa804bb
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: backend
              app.kubernetes.io/instance: loki
              app.kubernetes.io/name: loki
          topologyKey: kubernetes.io/hostname
    automountServiceAccountToken: true
    containers:
    - env:
      - name: METHOD
        value: WATCH
      - name: LABEL
        value: loki_rule
      - name: FOLDER
        value: /rules
      - name: RESOURCE
        value: both
      - name: WATCH_SERVER_TIMEOUT
        value: "60"
      - name: WATCH_CLIENT_TIMEOUT
        value: "60"
      - name: LOG_LEVEL
        value: INFO
      image: kiwigrid/k8s-sidecar:1.24.3
      imagePullPolicy: IfNotPresent
      name: loki-sc-rules
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /rules
        name: sc-rules-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nq8km
        readOnly: true
    - args:
      - -config.file=/etc/loki/config/config.yaml
      - -target=backend
      - -legacy-read-mode=false
      image: docker.io/grafana/loki:2.9.4
      imagePullPolicy: IfNotPresent
      name: loki
      ports:
      - containerPort: 3100
        name: http-metrics
        protocol: TCP
      - containerPort: 9095
        name: grpc
        protocol: TCP
      - containerPort: 7946
        name: http-memberlist
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/loki/config
        name: config
      - mountPath: /etc/loki/runtime-config
        name: runtime-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/loki
        name: data
      - mountPath: /rules
        name: sc-rules-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nq8km
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: loki-backend-2
    nodeName: puck
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: loki
    serviceAccountName: loki
    subdomain: loki-backend-headless
    terminationGracePeriodSeconds: 300
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-loki-backend-2
    - emptyDir: {}
      name: tmp
    - configMap:
        defaultMode: 420
        items:
        - key: config.yaml
          path: config.yaml
        name: loki
      name: config
    - configMap:
        defaultMode: 420
        name: loki-runtime
      name: runtime-config
    - emptyDir: {}
      name: sc-rules-volume
    - name: kube-api-access-nq8km
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:14Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:56Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:56Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:14Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e2518e5a9eba3bd2db9be60a784c13bbc598e02bdb10a3af0dfef565f4aeddfd
      image: docker.io/grafana/loki:2.9.4
      imageID: docker.io/grafana/loki@sha256:f379a20ce9dd815884ed6446aad8819b81a8ba4d36b548ca14be8cecbc6cbca0
      lastState: {}
      name: loki
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:17Z"
    - containerID: containerd://dc5a294590dc028c669163ee609ccc4d8f520d7b3adf5f6017f2fd87c657a481
      image: docker.io/kiwigrid/k8s-sidecar:1.24.3
      imageID: docker.io/kiwigrid/k8s-sidecar@sha256:5af76eebbba79edf4f7471bf1c3d5f2b40858114730c92d95eafe5716abe1fe8
      lastState: {}
      name: loki-sc-rules
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:17Z"
    hostIP: 192.168.0.203
    phase: Running
    podIP: 10.42.5.30
    podIPs:
    - ip: 10.42.5.30
    qosClass: BestEffort
    startTime: "2024-03-18T23:26:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-18T23:26:12Z"
    generateName: loki-canary-
    labels:
      app.kubernetes.io/component: canary
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
      controller-revision-hash: 67c968dbcc
      pod-template-generation: "1"
    name: loki-canary-2b4l2
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: loki-canary
      uid: 8384bad9-e6d9-4b64-bc59-f690b5111634
    resourceVersion: "472092"
    uid: db59d4e7-d109-4bb8-a625-5a7c2d9965d2
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - lenny
    containers:
    - args:
      - -addr=loki-gateway.monitoring.svc.cluster.local.:80
      - -labelname=pod
      - -labelvalue=$(POD_NAME)
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: docker.io/grafana/loki-canary:2.9.4
      imagePullPolicy: IfNotPresent
      name: loki-canary
      ports:
      - containerPort: 3500
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /metrics
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ssrzd
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: lenny
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: loki-canary
    serviceAccountName: loki-canary
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-ssrzd
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T03:48:40Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T03:48:40Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e13968bb59b46215b10f96602077a0c22230a87f4aebc3fcd2d89bca17d7af59
      image: docker.io/grafana/loki-canary:2.9.4
      imageID: docker.io/grafana/loki-canary@sha256:8e64c64be8a5c551148ed0679b30a0623e5132995f6ae88b8ea9dd1e5cfcb3a3
      lastState:
        terminated:
          containerID: containerd://af1e588ead12ed0a5d0a594658e5702ebe4c051e7c80a84605528caadd1f4bef
          exitCode: 255
          finishedAt: "2024-03-19T03:48:18Z"
          reason: Unknown
          startedAt: "2024-03-18T23:26:13Z"
      name: loki-canary
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2024-03-19T03:48:20Z"
    hostIP: 192.168.0.202
    phase: Running
    podIP: 10.42.4.31
    podIPs:
    - ip: 10.42.4.31
    qosClass: BestEffort
    startTime: "2024-03-18T23:26:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-18T23:26:11Z"
    generateName: loki-canary-
    labels:
      app.kubernetes.io/component: canary
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
      controller-revision-hash: 67c968dbcc
      pod-template-generation: "1"
    name: loki-canary-9jptj
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: loki-canary
      uid: 8384bad9-e6d9-4b64-bc59-f690b5111634
    resourceVersion: "402057"
    uid: a07adbee-e003-4984-b96c-2aea46835b99
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - hippy02
    containers:
    - args:
      - -addr=loki-gateway.monitoring.svc.cluster.local.:80
      - -labelname=pod
      - -labelvalue=$(POD_NAME)
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: docker.io/grafana/loki-canary:2.9.4
      imagePullPolicy: IfNotPresent
      name: loki-canary
      ports:
      - containerPort: 3500
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /metrics
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zrt46
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: hippy02
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: loki-canary
    serviceAccountName: loki-canary
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-zrt46
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:32Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:32Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:11Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://3b14bf2fa1925be0c991b741233bd43d168c13a222b1feff9426ff4313c5d3c4
      image: docker.io/grafana/loki-canary:2.9.4
      imageID: docker.io/grafana/loki-canary@sha256:8e64c64be8a5c551148ed0679b30a0623e5132995f6ae88b8ea9dd1e5cfcb3a3
      lastState: {}
      name: loki-canary
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:13Z"
    hostIP: 192.168.0.206
    phase: Running
    podIP: 10.42.2.27
    podIPs:
    - ip: 10.42.2.27
    qosClass: BestEffort
    startTime: "2024-03-18T23:26:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-18T23:26:12Z"
    generateName: loki-canary-
    labels:
      app.kubernetes.io/component: canary
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
      controller-revision-hash: 67c968dbcc
      pod-template-generation: "1"
    name: loki-canary-b5tmk
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: loki-canary
      uid: 8384bad9-e6d9-4b64-bc59-f690b5111634
    resourceVersion: "402079"
    uid: e081d08d-1e16-4737-8b6a-bdb1830068f5
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - hippy
    containers:
    - args:
      - -addr=loki-gateway.monitoring.svc.cluster.local.:80
      - -labelname=pod
      - -labelvalue=$(POD_NAME)
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: docker.io/grafana/loki-canary:2.9.4
      imagePullPolicy: IfNotPresent
      name: loki-canary
      ports:
      - containerPort: 3500
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /metrics
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bk4qz
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: hippy
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: loki-canary
    serviceAccountName: loki-canary
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-bk4qz
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:33Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:33Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e985615e4aeb584bab4acebd2a01985a16dfc230627275a29c77ea3f26dfd65a
      image: docker.io/grafana/loki-canary:2.9.4
      imageID: docker.io/grafana/loki-canary@sha256:8e64c64be8a5c551148ed0679b30a0623e5132995f6ae88b8ea9dd1e5cfcb3a3
      lastState: {}
      name: loki-canary
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:13Z"
    hostIP: 192.168.0.201
    phase: Running
    podIP: 10.42.3.46
    podIPs:
    - ip: 10.42.3.46
    qosClass: BestEffort
    startTime: "2024-03-18T23:26:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-18T23:26:12Z"
    generateName: loki-canary-
    labels:
      app.kubernetes.io/component: canary
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
      controller-revision-hash: 67c968dbcc
      pod-template-generation: "1"
    name: loki-canary-fp9dl
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: loki-canary
      uid: 8384bad9-e6d9-4b64-bc59-f690b5111634
    resourceVersion: "402083"
    uid: 36b605ad-a0ea-40a4-be7e-0109b92a5dea
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - puck
    containers:
    - args:
      - -addr=loki-gateway.monitoring.svc.cluster.local.:80
      - -labelname=pod
      - -labelvalue=$(POD_NAME)
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: docker.io/grafana/loki-canary:2.9.4
      imagePullPolicy: IfNotPresent
      name: loki-canary
      ports:
      - containerPort: 3500
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /metrics
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5s9wj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: puck
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: loki-canary
    serviceAccountName: loki-canary
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-5s9wj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:33Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:33Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://6ffa0845956b5a029d6304f401e027621da51b9975ffaa33d754a677b78aa547
      image: docker.io/grafana/loki-canary:2.9.4
      imageID: docker.io/grafana/loki-canary@sha256:8e64c64be8a5c551148ed0679b30a0623e5132995f6ae88b8ea9dd1e5cfcb3a3
      lastState: {}
      name: loki-canary
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:14Z"
    hostIP: 192.168.0.203
    phase: Running
    podIP: 10.42.5.27
    podIPs:
    - ip: 10.42.5.27
    qosClass: BestEffort
    startTime: "2024-03-18T23:26:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-18T23:26:11Z"
    generateName: loki-canary-
    labels:
      app.kubernetes.io/component: canary
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
      controller-revision-hash: 67c968dbcc
      pod-template-generation: "1"
    name: loki-canary-lhhj5
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: loki-canary
      uid: 8384bad9-e6d9-4b64-bc59-f690b5111634
    resourceVersion: "402050"
    uid: 2726fab1-6f66-446e-9d23-6bf9d5e7973c
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - j1
    containers:
    - args:
      - -addr=loki-gateway.monitoring.svc.cluster.local.:80
      - -labelname=pod
      - -labelvalue=$(POD_NAME)
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: docker.io/grafana/loki-canary:2.9.4
      imagePullPolicy: IfNotPresent
      name: loki-canary
      ports:
      - containerPort: 3500
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /metrics
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5ct5c
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: j1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: loki-canary
    serviceAccountName: loki-canary
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-5ct5c
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:11Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:32Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:32Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:11Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://1d9a5e0c658caff6e5d6e728b1669b6706777d640244add774ca93b2d94cb31d
      image: docker.io/grafana/loki-canary:2.9.4
      imageID: docker.io/grafana/loki-canary@sha256:8e64c64be8a5c551148ed0679b30a0623e5132995f6ae88b8ea9dd1e5cfcb3a3
      lastState: {}
      name: loki-canary
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:13Z"
    hostIP: 192.168.0.200
    phase: Running
    podIP: 10.42.0.20
    podIPs:
    - ip: 10.42.0.20
    qosClass: BestEffort
    startTime: "2024-03-18T23:26:11Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-18T23:26:12Z"
    generateName: loki-canary-
    labels:
      app.kubernetes.io/component: canary
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
      controller-revision-hash: 67c968dbcc
      pod-template-generation: "1"
    name: loki-canary-rf6p7
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: loki-canary
      uid: 8384bad9-e6d9-4b64-bc59-f690b5111634
    resourceVersion: "402065"
    uid: f7280198-c5b5-47ff-bc43-ac63b934da3e
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - pi
    containers:
    - args:
      - -addr=loki-gateway.monitoring.svc.cluster.local.:80
      - -labelname=pod
      - -labelvalue=$(POD_NAME)
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: docker.io/grafana/loki-canary:2.9.4
      imagePullPolicy: IfNotPresent
      name: loki-canary
      ports:
      - containerPort: 3500
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /metrics
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9p9hm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: pi
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: loki-canary
    serviceAccountName: loki-canary
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-9p9hm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:32Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:32Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://37563e7b6161c166803b6bdf3f72136879810c2ac405c1d26c364d35f6343e9b
      image: docker.io/grafana/loki-canary:2.9.4
      imageID: docker.io/grafana/loki-canary@sha256:8e64c64be8a5c551148ed0679b30a0623e5132995f6ae88b8ea9dd1e5cfcb3a3
      lastState: {}
      name: loki-canary
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:13Z"
    hostIP: 192.168.0.204
    phase: Running
    podIP: 10.42.6.18
    podIPs:
    - ip: 10.42.6.18
    qosClass: BestEffort
    startTime: "2024-03-18T23:26:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-18T23:26:11Z"
    generateName: loki-canary-
    labels:
      app.kubernetes.io/component: canary
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
      controller-revision-hash: 67c968dbcc
      pod-template-generation: "1"
    name: loki-canary-zqrnh
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: loki-canary
      uid: 8384bad9-e6d9-4b64-bc59-f690b5111634
    resourceVersion: "402058"
    uid: cc366fa4-4a71-46de-bf96-ce5cd600055a
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - hippy01
    containers:
    - args:
      - -addr=loki-gateway.monitoring.svc.cluster.local.:80
      - -labelname=pod
      - -labelvalue=$(POD_NAME)
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: docker.io/grafana/loki-canary:2.9.4
      imagePullPolicy: IfNotPresent
      name: loki-canary
      ports:
      - containerPort: 3500
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /metrics
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rpknb
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: hippy01
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: loki-canary
    serviceAccountName: loki-canary
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-rpknb
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:32Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:32Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:11Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://7f59ecd742ec95f2c3131f73bbcee558cc2846682b5f93095f60a64c4e3a02b7
      image: docker.io/grafana/loki-canary:2.9.4
      imageID: docker.io/grafana/loki-canary@sha256:8e64c64be8a5c551148ed0679b30a0623e5132995f6ae88b8ea9dd1e5cfcb3a3
      lastState: {}
      name: loki-canary
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:13Z"
    hostIP: 192.168.0.205
    phase: Running
    podIP: 10.42.1.23
    podIPs:
    - ip: 10.42.1.23
    qosClass: BestEffort
    startTime: "2024-03-18T23:26:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: a5e36c0a7a14ab969545157c400885c44e478f71b663daa2b92f88bad6fda1bd
    creationTimestamp: "2024-03-18T23:26:11Z"
    generateName: loki-gateway-56cd87b77-
    labels:
      app.kubernetes.io/component: gateway
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
      pod-template-hash: 56cd87b77
    name: loki-gateway-56cd87b77-2zxqt
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: loki-gateway-56cd87b77
      uid: 24e27c06-a5d5-4691-b8c4-6cfa407d717f
    resourceVersion: "402070"
    uid: f9cffc0d-f6e3-4e97-a355-c6186ec4846c
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: gateway
              app.kubernetes.io/instance: loki
              app.kubernetes.io/name: loki
          topologyKey: kubernetes.io/hostname
    containers:
    - image: docker.io/nginxinc/nginx-unprivileged:1.24-alpine
      imagePullPolicy: IfNotPresent
      name: nginx
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/nginx
        name: config
      - mountPath: /tmp
        name: tmp
      - mountPath: /docker-entrypoint.d
        name: docker-entrypoint-d-override
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5spsv
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: puck
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 101
      runAsGroup: 101
      runAsNonRoot: true
      runAsUser: 101
    serviceAccount: loki
    serviceAccountName: loki
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: loki-gateway
      name: config
    - emptyDir: {}
      name: tmp
    - emptyDir: {}
      name: docker-entrypoint-d-override
    - name: kube-api-access-5spsv
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:11Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:33Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:33Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:11Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b54687a96a0e3b415cc09ab3276c3d4c4ae85a4379b89a2a077e61be3fd17442
      image: docker.io/nginxinc/nginx-unprivileged:1.24-alpine
      imageID: docker.io/nginxinc/nginx-unprivileged@sha256:04d8b08bdf6e74ef2a56a47bb8d83936e95eae65192b4ab6aa08c4392049cb16
      lastState: {}
      name: nginx
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:13Z"
    hostIP: 192.168.0.203
    phase: Running
    podIP: 10.42.5.26
    podIPs:
    - ip: 10.42.5.26
    qosClass: BestEffort
    startTime: "2024-03-18T23:26:11Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2024-03-18T23:26:11Z"
    generateName: loki-grafana-agent-operator-59556555b8-
    labels:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: grafana-agent-operator
      pod-template-hash: 59556555b8
    name: loki-grafana-agent-operator-59556555b8-jrszt
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: loki-grafana-agent-operator-59556555b8
      uid: 1f449c78-74c0-4b72-afce-c953363f6a9f
    resourceVersion: "401773"
    uid: e02ddbb3-8790-466b-a1dc-ba7d87251251
  spec:
    containers:
    - args:
      - --kubelet-service=default/kubelet
      image: docker.io/grafana/agent-operator:v0.39.1
      imagePullPolicy: IfNotPresent
      name: grafana-agent-operator
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mrvc2
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: hippy
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: loki-grafana-agent-operator
    serviceAccountName: loki-grafana-agent-operator
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-mrvc2
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:11Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:13Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:13Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:11Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://3100a5266821726d40d984c3d6a01e1cfb7eade558970d02a44efe6a8660db94
      image: docker.io/grafana/agent-operator:v0.39.1
      imageID: docker.io/grafana/agent-operator@sha256:aa2a4a17b84a987d6ba8a11a251235910751645f3d8c3fcedfce562ac55aca2b
      lastState: {}
      name: grafana-agent-operator
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:12Z"
    hostIP: 192.168.0.201
    phase: Running
    podIP: 10.42.3.44
    podIPs:
    - ip: 10.42.3.44
    qosClass: BestEffort
    startTime: "2024-03-18T23:26:11Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: grafana-agent
    creationTimestamp: "2024-03-18T23:26:13Z"
    generateName: loki-logs-
    labels:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: grafana-agent-operator
      app.kubernetes.io/name: grafana-agent
      app.kubernetes.io/version: v0-39-1
      controller-revision-hash: cd6599765
      grafana-agent: loki
      operator.agent.grafana.com/name: loki
      operator.agent.grafana.com/type: logs
      pod-template-generation: "1"
    name: loki-logs-2pld6
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: loki-logs
      uid: a8ca02ab-f165-4a7a-beeb-4ee80ac047c3
    resourceVersion: "401879"
    uid: 2a114251-a1c8-4c2d-b7df-3d03f2b7047b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - hippy01
    containers:
    - args:
      - --config-file=/var/lib/grafana-agent/config-in/agent.yml
      - --config-envsubst-file=/var/lib/grafana-agent/config/agent.yml
      - --watch-interval=1m
      - --statefulset-ordinal-from-envvar=POD_NAME
      - --reload-url=http://127.0.0.1:8080/-/reload
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: AGENT_DEPLOY_MODE
        value: operator
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.67.1
      imagePullPolicy: IfNotPresent
      name: config-reloader
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/grafana-agent/config-in
        name: config
        readOnly: true
      - mountPath: /var/lib/grafana-agent/config
        name: config-out
      - mountPath: /var/lib/grafana-agent/secrets
        name: secrets
        readOnly: true
      - mountPath: /var/log
        name: varlog
        readOnly: true
      - mountPath: /var/lib/docker/containers
        name: dockerlogs
        readOnly: true
      - mountPath: /var/lib/grafana-agent/data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-j5k95
        readOnly: true
    - args:
      - -config.file=/var/lib/grafana-agent/config/agent.yml
      - -config.expand-env=true
      - -server.http.address=0.0.0.0:8080
      - -enable-features=integrations-next
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: AGENT_DEPLOY_MODE
        value: operator
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: SHARD
        value: "0"
      image: grafana/agent:v0.39.1
      imagePullPolicy: IfNotPresent
      name: grafana-agent
      ports:
      - containerPort: 8080
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 120
        httpGet:
          path: /-/ready
          port: http-metrics
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /var/lib/grafana-agent/config-in
        name: config
        readOnly: true
      - mountPath: /var/lib/grafana-agent/config
        name: config-out
      - mountPath: /var/lib/grafana-agent/secrets
        name: secrets
        readOnly: true
      - mountPath: /var/log
        name: varlog
        readOnly: true
      - mountPath: /var/lib/docker/containers
        name: dockerlogs
        readOnly: true
      - mountPath: /var/lib/grafana-agent/data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-j5k95
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: hippy01
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: loki-grafana-agent
    serviceAccountName: loki-grafana-agent
    terminationGracePeriodSeconds: 4800
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: loki-logs-config
    - emptyDir: {}
      name: config-out
    - name: secrets
      secret:
        defaultMode: 420
        secretName: loki-secrets
    - hostPath:
        path: /var/log
        type: ""
      name: varlog
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: dockerlogs
    - hostPath:
        path: /var/lib/grafana-agent/data
        type: ""
      name: data
    - name: kube-api-access-j5k95
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:14Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:15Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:15Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:14Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://7f718238b93e182df3e79d90378ac0bd1dd08cdd57298950e3a7210210fd2535
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.67.1
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:0fe3cf36985e0e524801a0393f88fa4b5dd5ffdf0f091ff78ee02f2d281631b5
      lastState: {}
      name: config-reloader
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:15Z"
    - containerID: containerd://8e465bb67bda8c8ce6d5896ac63b510b258b46f6b010d59b38fc4f583cbd4cd3
      image: docker.io/grafana/agent:v0.39.1
      imageID: docker.io/grafana/agent@sha256:e19f3f0af97884affac3ee49012cc0163aed521d0475c95a29e037dccbcd92b3
      lastState: {}
      name: grafana-agent
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:15Z"
    hostIP: 192.168.0.205
    phase: Running
    podIP: 10.42.1.25
    podIPs:
    - ip: 10.42.1.25
    qosClass: BestEffort
    startTime: "2024-03-18T23:26:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: grafana-agent
    creationTimestamp: "2024-03-18T23:26:13Z"
    generateName: loki-logs-
    labels:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: grafana-agent-operator
      app.kubernetes.io/name: grafana-agent
      app.kubernetes.io/version: v0-39-1
      controller-revision-hash: cd6599765
      grafana-agent: loki
      operator.agent.grafana.com/name: loki
      operator.agent.grafana.com/type: logs
      pod-template-generation: "1"
    name: loki-logs-2zdb9
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: loki-logs
      uid: a8ca02ab-f165-4a7a-beeb-4ee80ac047c3
    resourceVersion: "401868"
    uid: 18d04cc1-b6a0-4380-a406-af96b29b0e59
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - puck
    containers:
    - args:
      - --config-file=/var/lib/grafana-agent/config-in/agent.yml
      - --config-envsubst-file=/var/lib/grafana-agent/config/agent.yml
      - --watch-interval=1m
      - --statefulset-ordinal-from-envvar=POD_NAME
      - --reload-url=http://127.0.0.1:8080/-/reload
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: AGENT_DEPLOY_MODE
        value: operator
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.67.1
      imagePullPolicy: IfNotPresent
      name: config-reloader
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/grafana-agent/config-in
        name: config
        readOnly: true
      - mountPath: /var/lib/grafana-agent/config
        name: config-out
      - mountPath: /var/lib/grafana-agent/secrets
        name: secrets
        readOnly: true
      - mountPath: /var/log
        name: varlog
        readOnly: true
      - mountPath: /var/lib/docker/containers
        name: dockerlogs
        readOnly: true
      - mountPath: /var/lib/grafana-agent/data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-48rk6
        readOnly: true
    - args:
      - -config.file=/var/lib/grafana-agent/config/agent.yml
      - -config.expand-env=true
      - -server.http.address=0.0.0.0:8080
      - -enable-features=integrations-next
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: AGENT_DEPLOY_MODE
        value: operator
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: SHARD
        value: "0"
      image: grafana/agent:v0.39.1
      imagePullPolicy: IfNotPresent
      name: grafana-agent
      ports:
      - containerPort: 8080
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 120
        httpGet:
          path: /-/ready
          port: http-metrics
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /var/lib/grafana-agent/config-in
        name: config
        readOnly: true
      - mountPath: /var/lib/grafana-agent/config
        name: config-out
      - mountPath: /var/lib/grafana-agent/secrets
        name: secrets
        readOnly: true
      - mountPath: /var/log
        name: varlog
        readOnly: true
      - mountPath: /var/lib/docker/containers
        name: dockerlogs
        readOnly: true
      - mountPath: /var/lib/grafana-agent/data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-48rk6
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: puck
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: loki-grafana-agent
    serviceAccountName: loki-grafana-agent
    terminationGracePeriodSeconds: 4800
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: loki-logs-config
    - emptyDir: {}
      name: config-out
    - name: secrets
      secret:
        defaultMode: 420
        secretName: loki-secrets
    - hostPath:
        path: /var/log
        type: ""
      name: varlog
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: dockerlogs
    - hostPath:
        path: /var/lib/grafana-agent/data
        type: ""
      name: data
    - name: kube-api-access-48rk6
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:14Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:15Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:15Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:14Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://cc6444b6a794fde96a3d8ce0d59312f6fbbf0e0858cf9dea5310594026925903
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.67.1
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:0fe3cf36985e0e524801a0393f88fa4b5dd5ffdf0f091ff78ee02f2d281631b5
      lastState: {}
      name: config-reloader
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:15Z"
    - containerID: containerd://53f0a8fd6afe7631fbc48e339d2b302b80f2aebf0f71b5f89e5445aaeebd23bd
      image: docker.io/grafana/agent:v0.39.1
      imageID: docker.io/grafana/agent@sha256:e19f3f0af97884affac3ee49012cc0163aed521d0475c95a29e037dccbcd92b3
      lastState: {}
      name: grafana-agent
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:15Z"
    hostIP: 192.168.0.203
    phase: Running
    podIP: 10.42.5.29
    podIPs:
    - ip: 10.42.5.29
    qosClass: BestEffort
    startTime: "2024-03-18T23:26:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: grafana-agent
    creationTimestamp: "2024-03-18T23:26:13Z"
    generateName: loki-logs-
    labels:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: grafana-agent-operator
      app.kubernetes.io/name: grafana-agent
      app.kubernetes.io/version: v0-39-1
      controller-revision-hash: cd6599765
      grafana-agent: loki
      operator.agent.grafana.com/name: loki
      operator.agent.grafana.com/type: logs
      pod-template-generation: "1"
    name: loki-logs-fxm6f
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: loki-logs
      uid: a8ca02ab-f165-4a7a-beeb-4ee80ac047c3
    resourceVersion: "401933"
    uid: efcd8eec-46d1-4429-be9e-db71da6e68f4
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - pi
    containers:
    - args:
      - --config-file=/var/lib/grafana-agent/config-in/agent.yml
      - --config-envsubst-file=/var/lib/grafana-agent/config/agent.yml
      - --watch-interval=1m
      - --statefulset-ordinal-from-envvar=POD_NAME
      - --reload-url=http://127.0.0.1:8080/-/reload
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: AGENT_DEPLOY_MODE
        value: operator
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.67.1
      imagePullPolicy: IfNotPresent
      name: config-reloader
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/grafana-agent/config-in
        name: config
        readOnly: true
      - mountPath: /var/lib/grafana-agent/config
        name: config-out
      - mountPath: /var/lib/grafana-agent/secrets
        name: secrets
        readOnly: true
      - mountPath: /var/log
        name: varlog
        readOnly: true
      - mountPath: /var/lib/docker/containers
        name: dockerlogs
        readOnly: true
      - mountPath: /var/lib/grafana-agent/data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2phhl
        readOnly: true
    - args:
      - -config.file=/var/lib/grafana-agent/config/agent.yml
      - -config.expand-env=true
      - -server.http.address=0.0.0.0:8080
      - -enable-features=integrations-next
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: AGENT_DEPLOY_MODE
        value: operator
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: SHARD
        value: "0"
      image: grafana/agent:v0.39.1
      imagePullPolicy: IfNotPresent
      name: grafana-agent
      ports:
      - containerPort: 8080
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 120
        httpGet:
          path: /-/ready
          port: http-metrics
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /var/lib/grafana-agent/config-in
        name: config
        readOnly: true
      - mountPath: /var/lib/grafana-agent/config
        name: config-out
      - mountPath: /var/lib/grafana-agent/secrets
        name: secrets
        readOnly: true
      - mountPath: /var/log
        name: varlog
        readOnly: true
      - mountPath: /var/lib/docker/containers
        name: dockerlogs
        readOnly: true
      - mountPath: /var/lib/grafana-agent/data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2phhl
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: pi
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: loki-grafana-agent
    serviceAccountName: loki-grafana-agent
    terminationGracePeriodSeconds: 4800
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: loki-logs-config
    - emptyDir: {}
      name: config-out
    - name: secrets
      secret:
        defaultMode: 420
        secretName: loki-secrets
    - hostPath:
        path: /var/log
        type: ""
      name: varlog
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: dockerlogs
    - hostPath:
        path: /var/lib/grafana-agent/data
        type: ""
      name: data
    - name: kube-api-access-2phhl
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:14Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:17Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:17Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:14Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://0aab0c94226bad426117e72803454a81ff090db683823c64953a4643f2b99c97
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.67.1
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:0fe3cf36985e0e524801a0393f88fa4b5dd5ffdf0f091ff78ee02f2d281631b5
      lastState: {}
      name: config-reloader
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:15Z"
    - containerID: containerd://ff69c6dcfd2ecb587930c2d070f10a0b68005ab8672a1b3356cf91205a429d6e
      image: docker.io/grafana/agent:v0.39.1
      imageID: docker.io/grafana/agent@sha256:e19f3f0af97884affac3ee49012cc0163aed521d0475c95a29e037dccbcd92b3
      lastState: {}
      name: grafana-agent
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:16Z"
    hostIP: 192.168.0.204
    phase: Running
    podIP: 10.42.6.19
    podIPs:
    - ip: 10.42.6.19
    qosClass: BestEffort
    startTime: "2024-03-18T23:26:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: grafana-agent
    creationTimestamp: "2024-03-18T23:26:13Z"
    generateName: loki-logs-
    labels:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: grafana-agent-operator
      app.kubernetes.io/name: grafana-agent
      app.kubernetes.io/version: v0-39-1
      controller-revision-hash: cd6599765
      grafana-agent: loki
      operator.agent.grafana.com/name: loki
      operator.agent.grafana.com/type: logs
      pod-template-generation: "1"
    name: loki-logs-hd2lk
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: loki-logs
      uid: a8ca02ab-f165-4a7a-beeb-4ee80ac047c3
    resourceVersion: "401975"
    uid: c01e6d2d-3228-4eb4-88c6-d08d6844c4d1
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - hippy02
    containers:
    - args:
      - --config-file=/var/lib/grafana-agent/config-in/agent.yml
      - --config-envsubst-file=/var/lib/grafana-agent/config/agent.yml
      - --watch-interval=1m
      - --statefulset-ordinal-from-envvar=POD_NAME
      - --reload-url=http://127.0.0.1:8080/-/reload
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: AGENT_DEPLOY_MODE
        value: operator
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.67.1
      imagePullPolicy: IfNotPresent
      name: config-reloader
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/grafana-agent/config-in
        name: config
        readOnly: true
      - mountPath: /var/lib/grafana-agent/config
        name: config-out
      - mountPath: /var/lib/grafana-agent/secrets
        name: secrets
        readOnly: true
      - mountPath: /var/log
        name: varlog
        readOnly: true
      - mountPath: /var/lib/docker/containers
        name: dockerlogs
        readOnly: true
      - mountPath: /var/lib/grafana-agent/data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gbmxf
        readOnly: true
    - args:
      - -config.file=/var/lib/grafana-agent/config/agent.yml
      - -config.expand-env=true
      - -server.http.address=0.0.0.0:8080
      - -enable-features=integrations-next
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: AGENT_DEPLOY_MODE
        value: operator
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: SHARD
        value: "0"
      image: grafana/agent:v0.39.1
      imagePullPolicy: IfNotPresent
      name: grafana-agent
      ports:
      - containerPort: 8080
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 120
        httpGet:
          path: /-/ready
          port: http-metrics
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /var/lib/grafana-agent/config-in
        name: config
        readOnly: true
      - mountPath: /var/lib/grafana-agent/config
        name: config-out
      - mountPath: /var/lib/grafana-agent/secrets
        name: secrets
        readOnly: true
      - mountPath: /var/log
        name: varlog
        readOnly: true
      - mountPath: /var/lib/docker/containers
        name: dockerlogs
        readOnly: true
      - mountPath: /var/lib/grafana-agent/data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gbmxf
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: hippy02
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: loki-grafana-agent
    serviceAccountName: loki-grafana-agent
    terminationGracePeriodSeconds: 4800
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: loki-logs-config
    - emptyDir: {}
      name: config-out
    - name: secrets
      secret:
        defaultMode: 420
        secretName: loki-secrets
    - hostPath:
        path: /var/log
        type: ""
      name: varlog
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: dockerlogs
    - hostPath:
        path: /var/lib/grafana-agent/data
        type: ""
      name: data
    - name: kube-api-access-gbmxf
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:13Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:19Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:19Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:13Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://5add5e0fedd4ad00862dc6121bcfdde0115f3840ff36bb768ac75137000bf5f5
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.67.1
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:0fe3cf36985e0e524801a0393f88fa4b5dd5ffdf0f091ff78ee02f2d281631b5
      lastState: {}
      name: config-reloader
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:14Z"
    - containerID: containerd://5283cd5fefedf10700d2b5a4d74ba1c3d92884836d0afe68a97c1f3a4e4114cb
      image: docker.io/grafana/agent:v0.39.1
      imageID: docker.io/grafana/agent@sha256:e19f3f0af97884affac3ee49012cc0163aed521d0475c95a29e037dccbcd92b3
      lastState: {}
      name: grafana-agent
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:15Z"
    hostIP: 192.168.0.206
    phase: Running
    podIP: 10.42.2.28
    podIPs:
    - ip: 10.42.2.28
    qosClass: BestEffort
    startTime: "2024-03-18T23:26:13Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: grafana-agent
    creationTimestamp: "2024-03-18T23:26:13Z"
    generateName: loki-logs-
    labels:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: grafana-agent-operator
      app.kubernetes.io/name: grafana-agent
      app.kubernetes.io/version: v0-39-1
      controller-revision-hash: cd6599765
      grafana-agent: loki
      operator.agent.grafana.com/name: loki
      operator.agent.grafana.com/type: logs
      pod-template-generation: "1"
    name: loki-logs-lxjm7
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: loki-logs
      uid: a8ca02ab-f165-4a7a-beeb-4ee80ac047c3
    resourceVersion: "401913"
    uid: d7ac92f7-7c81-4e85-8e8a-2ceac93832fe
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - j1
    containers:
    - args:
      - --config-file=/var/lib/grafana-agent/config-in/agent.yml
      - --config-envsubst-file=/var/lib/grafana-agent/config/agent.yml
      - --watch-interval=1m
      - --statefulset-ordinal-from-envvar=POD_NAME
      - --reload-url=http://127.0.0.1:8080/-/reload
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: AGENT_DEPLOY_MODE
        value: operator
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.67.1
      imagePullPolicy: IfNotPresent
      name: config-reloader
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/grafana-agent/config-in
        name: config
        readOnly: true
      - mountPath: /var/lib/grafana-agent/config
        name: config-out
      - mountPath: /var/lib/grafana-agent/secrets
        name: secrets
        readOnly: true
      - mountPath: /var/log
        name: varlog
        readOnly: true
      - mountPath: /var/lib/docker/containers
        name: dockerlogs
        readOnly: true
      - mountPath: /var/lib/grafana-agent/data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9l8m7
        readOnly: true
    - args:
      - -config.file=/var/lib/grafana-agent/config/agent.yml
      - -config.expand-env=true
      - -server.http.address=0.0.0.0:8080
      - -enable-features=integrations-next
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: AGENT_DEPLOY_MODE
        value: operator
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: SHARD
        value: "0"
      image: grafana/agent:v0.39.1
      imagePullPolicy: IfNotPresent
      name: grafana-agent
      ports:
      - containerPort: 8080
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 120
        httpGet:
          path: /-/ready
          port: http-metrics
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /var/lib/grafana-agent/config-in
        name: config
        readOnly: true
      - mountPath: /var/lib/grafana-agent/config
        name: config-out
      - mountPath: /var/lib/grafana-agent/secrets
        name: secrets
        readOnly: true
      - mountPath: /var/log
        name: varlog
        readOnly: true
      - mountPath: /var/lib/docker/containers
        name: dockerlogs
        readOnly: true
      - mountPath: /var/lib/grafana-agent/data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9l8m7
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: j1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: loki-grafana-agent
    serviceAccountName: loki-grafana-agent
    terminationGracePeriodSeconds: 4800
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: loki-logs-config
    - emptyDir: {}
      name: config-out
    - name: secrets
      secret:
        defaultMode: 420
        secretName: loki-secrets
    - hostPath:
        path: /var/log
        type: ""
      name: varlog
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: dockerlogs
    - hostPath:
        path: /var/lib/grafana-agent/data
        type: ""
      name: data
    - name: kube-api-access-9l8m7
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:14Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:16Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:16Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:14Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://feba06032ab9a079b2de35981cfef838ccf437a762ec13c9e8c1c2f5e3a1a4c1
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.67.1
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:0fe3cf36985e0e524801a0393f88fa4b5dd5ffdf0f091ff78ee02f2d281631b5
      lastState: {}
      name: config-reloader
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:15Z"
    - containerID: containerd://6d805fbaded4d07c0ef96541dc689e0daf54e13efad81dafd9d3551c8f578633
      image: docker.io/grafana/agent:v0.39.1
      imageID: docker.io/grafana/agent@sha256:e19f3f0af97884affac3ee49012cc0163aed521d0475c95a29e037dccbcd92b3
      lastState: {}
      name: grafana-agent
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:15Z"
    hostIP: 192.168.0.200
    phase: Running
    podIP: 10.42.0.21
    podIPs:
    - ip: 10.42.0.21
    qosClass: BestEffort
    startTime: "2024-03-18T23:26:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: grafana-agent
    creationTimestamp: "2024-03-18T23:26:13Z"
    generateName: loki-logs-
    labels:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: grafana-agent-operator
      app.kubernetes.io/name: grafana-agent
      app.kubernetes.io/version: v0-39-1
      controller-revision-hash: cd6599765
      grafana-agent: loki
      operator.agent.grafana.com/name: loki
      operator.agent.grafana.com/type: logs
      pod-template-generation: "1"
    name: loki-logs-wbgwk
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: loki-logs
      uid: a8ca02ab-f165-4a7a-beeb-4ee80ac047c3
    resourceVersion: "472006"
    uid: e55dbce2-e77e-4421-90e6-5d93fc3fe479
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - lenny
    containers:
    - args:
      - --config-file=/var/lib/grafana-agent/config-in/agent.yml
      - --config-envsubst-file=/var/lib/grafana-agent/config/agent.yml
      - --watch-interval=1m
      - --statefulset-ordinal-from-envvar=POD_NAME
      - --reload-url=http://127.0.0.1:8080/-/reload
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: AGENT_DEPLOY_MODE
        value: operator
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.67.1
      imagePullPolicy: IfNotPresent
      name: config-reloader
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/grafana-agent/config-in
        name: config
        readOnly: true
      - mountPath: /var/lib/grafana-agent/config
        name: config-out
      - mountPath: /var/lib/grafana-agent/secrets
        name: secrets
        readOnly: true
      - mountPath: /var/log
        name: varlog
        readOnly: true
      - mountPath: /var/lib/docker/containers
        name: dockerlogs
        readOnly: true
      - mountPath: /var/lib/grafana-agent/data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lbwd6
        readOnly: true
    - args:
      - -config.file=/var/lib/grafana-agent/config/agent.yml
      - -config.expand-env=true
      - -server.http.address=0.0.0.0:8080
      - -enable-features=integrations-next
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: AGENT_DEPLOY_MODE
        value: operator
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: SHARD
        value: "0"
      image: grafana/agent:v0.39.1
      imagePullPolicy: IfNotPresent
      name: grafana-agent
      ports:
      - containerPort: 8080
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 120
        httpGet:
          path: /-/ready
          port: http-metrics
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /var/lib/grafana-agent/config-in
        name: config
        readOnly: true
      - mountPath: /var/lib/grafana-agent/config
        name: config-out
      - mountPath: /var/lib/grafana-agent/secrets
        name: secrets
        readOnly: true
      - mountPath: /var/log
        name: varlog
        readOnly: true
      - mountPath: /var/lib/docker/containers
        name: dockerlogs
        readOnly: true
      - mountPath: /var/lib/grafana-agent/data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lbwd6
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: lenny
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: loki-grafana-agent
    serviceAccountName: loki-grafana-agent
    terminationGracePeriodSeconds: 4800
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: loki-logs-config
    - emptyDir: {}
      name: config-out
    - name: secrets
      secret:
        defaultMode: 420
        secretName: loki-secrets
    - hostPath:
        path: /var/log
        type: ""
      name: varlog
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: dockerlogs
    - hostPath:
        path: /var/lib/grafana-agent/data
        type: ""
      name: data
    - name: kube-api-access-lbwd6
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:13Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T03:48:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T03:48:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:13Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c56e729ee10de7688602689322ada27f4ac8a610ec94fd3c07e1352a7bd22757
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.67.1
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:0fe3cf36985e0e524801a0393f88fa4b5dd5ffdf0f091ff78ee02f2d281631b5
      lastState:
        terminated:
          containerID: containerd://47228b391d3f76aecc9ece94aec4478b1c6131b3a62a796227e4a611eba02c18
          exitCode: 255
          finishedAt: "2024-03-19T03:48:18Z"
          reason: Unknown
          startedAt: "2024-03-18T23:26:15Z"
      name: config-reloader
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2024-03-19T03:48:21Z"
    - containerID: containerd://46d896e0400d0f61d4929a8e3da9056fe714b337d6a1c2a63beca0fe70ae29e1
      image: docker.io/grafana/agent:v0.39.1
      imageID: docker.io/grafana/agent@sha256:e19f3f0af97884affac3ee49012cc0163aed521d0475c95a29e037dccbcd92b3
      lastState:
        terminated:
          containerID: containerd://385521a2921fd04fe73facf1ea1f88fa8ee336bfa544b31ff3e75510eaba1985
          exitCode: 255
          finishedAt: "2024-03-19T03:48:18Z"
          message: |
            _loki-write-0_73ef4cd6-de44-4f74-a1c4-6b72491267b7/loki/0.log op=CREATE
            ts=2024-03-19T01:41:30.808656Z caller=log.go:168 component=logs logs_config=monitoring/loki level=info msg="Re-opening moved/deleted file /var/log/pods/monitoring_loki-write-0_73ef4cd6-de44-4f74-a1c4-6b72491267b7/loki/0.log ..."
            ts=2024-03-19T01:41:30.808715087Z caller=log.go:168 component=logs logs_config=monitoring/loki level=info msg="Successfully reopened /var/log/pods/monitoring_loki-write-0_73ef4cd6-de44-4f74-a1c4-6b72491267b7/loki/0.log"
            ts=2024-03-19T02:02:01.274263158Z caller=filetargetmanager.go:181 level=info component=logs logs_config=monitoring/loki msg="received file watcher event" name=/var/log/pods/monitoring_loki-write-0_73ef4cd6-de44-4f74-a1c4-6b72491267b7/loki/0.log.20240318-214130.tmp op=CREATE
            ts=2024-03-19T02:02:01.653432153Z caller=filetargetmanager.go:181 level=info component=logs logs_config=monitoring/loki msg="received file watcher event" name=/var/log/pods/monitoring_loki-write-0_73ef4cd6-de44-4f74-a1c4-6b72491267b7/loki/0.log.20240318-214130.gz op=CREATE
            ts=2024-03-19T02:02:01.656053618Z caller=filetargetmanager.go:181 level=info component=logs logs_config=monitoring/loki msg="received file watcher event" name=/var/log/pods/monitoring_loki-write-0_73ef4cd6-de44-4f74-a1c4-6b72491267b7/loki/0.log.20240318-220201 op=CREATE
            ts=2024-03-19T02:02:01.656611258Z caller=filetargetmanager.go:181 level=info component=logs logs_config=monitoring/loki msg="received file watcher event" name=/var/log/pods/monitoring_loki-write-0_73ef4cd6-de44-4f74-a1c4-6b72491267b7/loki/0.log op=CREATE
            ts=2024-03-19T02:02:01.834349779Z caller=log.go:168 component=logs logs_config=monitoring/loki level=info msg="Re-opening moved/deleted file /var/log/pods/monitoring_loki-write-0_73ef4cd6-de44-4f74-a1c4-6b72491267b7/loki/0.log ..."
            ts=2024-03-19T02:02:01.834459212Z caller=log.go:168 component=logs logs_config=monitoring/loki level=info msg="Successfully reopened /var/log/pods/monitoring_loki-write-0_73ef4cd6-de44-4f74-a1c4-6b72491267b7/loki/0.log"
          reason: Unknown
          startedAt: "2024-03-18T23:26:15Z"
      name: grafana-agent
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2024-03-19T03:48:21Z"
    hostIP: 192.168.0.202
    phase: Running
    podIP: 10.42.4.34
    podIPs:
    - ip: 10.42.4.34
    qosClass: BestEffort
    startTime: "2024-03-18T23:26:13Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: grafana-agent
    creationTimestamp: "2024-03-18T23:26:13Z"
    generateName: loki-logs-
    labels:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: grafana-agent-operator
      app.kubernetes.io/name: grafana-agent
      app.kubernetes.io/version: v0-39-1
      controller-revision-hash: cd6599765
      grafana-agent: loki
      operator.agent.grafana.com/name: loki
      operator.agent.grafana.com/type: logs
      pod-template-generation: "1"
    name: loki-logs-wzsmg
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: loki-logs
      uid: a8ca02ab-f165-4a7a-beeb-4ee80ac047c3
    resourceVersion: "401852"
    uid: 598a78c4-547d-41ca-b090-4292964e8fa5
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - hippy
    containers:
    - args:
      - --config-file=/var/lib/grafana-agent/config-in/agent.yml
      - --config-envsubst-file=/var/lib/grafana-agent/config/agent.yml
      - --watch-interval=1m
      - --statefulset-ordinal-from-envvar=POD_NAME
      - --reload-url=http://127.0.0.1:8080/-/reload
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: AGENT_DEPLOY_MODE
        value: operator
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.67.1
      imagePullPolicy: IfNotPresent
      name: config-reloader
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/grafana-agent/config-in
        name: config
        readOnly: true
      - mountPath: /var/lib/grafana-agent/config
        name: config-out
      - mountPath: /var/lib/grafana-agent/secrets
        name: secrets
        readOnly: true
      - mountPath: /var/log
        name: varlog
        readOnly: true
      - mountPath: /var/lib/docker/containers
        name: dockerlogs
        readOnly: true
      - mountPath: /var/lib/grafana-agent/data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ngmg4
        readOnly: true
    - args:
      - -config.file=/var/lib/grafana-agent/config/agent.yml
      - -config.expand-env=true
      - -server.http.address=0.0.0.0:8080
      - -enable-features=integrations-next
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: AGENT_DEPLOY_MODE
        value: operator
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: SHARD
        value: "0"
      image: grafana/agent:v0.39.1
      imagePullPolicy: IfNotPresent
      name: grafana-agent
      ports:
      - containerPort: 8080
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 120
        httpGet:
          path: /-/ready
          port: http-metrics
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /var/lib/grafana-agent/config-in
        name: config
        readOnly: true
      - mountPath: /var/lib/grafana-agent/config
        name: config-out
      - mountPath: /var/lib/grafana-agent/secrets
        name: secrets
        readOnly: true
      - mountPath: /var/log
        name: varlog
        readOnly: true
      - mountPath: /var/lib/docker/containers
        name: dockerlogs
        readOnly: true
      - mountPath: /var/lib/grafana-agent/data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ngmg4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: hippy
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: loki-grafana-agent
    serviceAccountName: loki-grafana-agent
    terminationGracePeriodSeconds: 4800
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: loki-logs-config
    - emptyDir: {}
      name: config-out
    - name: secrets
      secret:
        defaultMode: 420
        secretName: loki-secrets
    - hostPath:
        path: /var/log
        type: ""
      name: varlog
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: dockerlogs
    - hostPath:
        path: /var/lib/grafana-agent/data
        type: ""
      name: data
    - name: kube-api-access-ngmg4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:14Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:15Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:15Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:13Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a39dae3bef3bd7113507ba9c8351fb09d2d0607c0bd0bb605e8b7ed42567288b
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.67.1
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:0fe3cf36985e0e524801a0393f88fa4b5dd5ffdf0f091ff78ee02f2d281631b5
      lastState: {}
      name: config-reloader
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:14Z"
    - containerID: containerd://bfafc73396b9dfea26f1d7c3de5635fc52f9516288b9dbde5755c2fc399ddf85
      image: docker.io/grafana/agent:v0.39.1
      imageID: docker.io/grafana/agent@sha256:e19f3f0af97884affac3ee49012cc0163aed521d0475c95a29e037dccbcd92b3
      lastState: {}
      name: grafana-agent
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:15Z"
    hostIP: 192.168.0.201
    phase: Running
    podIP: 10.42.3.47
    podIPs:
    - ip: 10.42.3.47
    qosClass: BestEffort
    startTime: "2024-03-18T23:26:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 2c5e95cf016dd90c7bd90fa0fb16372cc8765ad669608df363d1f2d73f08832b
    creationTimestamp: "2024-03-18T23:26:11Z"
    generateName: loki-read-985cf4795-
    labels:
      app.kubernetes.io/component: read
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
      app.kubernetes.io/part-of: memberlist
      pod-template-hash: 985cf4795
    name: loki-read-985cf4795-2hsjf
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: loki-read-985cf4795
      uid: 1e3c5778-5b77-4418-8320-ead7daab7edf
    resourceVersion: "402133"
    uid: 2fdbf417-344f-4272-adae-fb3d5cbca061
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: read
              app.kubernetes.io/instance: loki
              app.kubernetes.io/name: loki
          topologyKey: kubernetes.io/hostname
    automountServiceAccountToken: true
    containers:
    - args:
      - -config.file=/etc/loki/config/config.yaml
      - -target=read
      - -legacy-read-mode=false
      - -common.compactor-grpc-address=loki-backend.monitoring.svc.cluster.local:9095
      image: docker.io/grafana/loki:2.9.4
      imagePullPolicy: IfNotPresent
      name: loki
      ports:
      - containerPort: 3100
        name: http-metrics
        protocol: TCP
      - containerPort: 9095
        name: grpc
        protocol: TCP
      - containerPort: 7946
        name: http-memberlist
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/loki/config
        name: config
      - mountPath: /etc/loki/runtime-config
        name: runtime-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/loki
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xp4jh
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: puck
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: loki
    serviceAccountName: loki
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp
    - emptyDir: {}
      name: data
    - configMap:
        defaultMode: 420
        items:
        - key: config.yaml
          path: config.yaml
        name: loki
      name: config
    - configMap:
        defaultMode: 420
        name: loki-runtime
      name: runtime-config
    - name: kube-api-access-xp4jh
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:44Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:44Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://7aa61a5f11cb28849adf0dcf7e5e5d99fe5716fba5321aa3a39475c55689d51f
      image: docker.io/grafana/loki:2.9.4
      imageID: docker.io/grafana/loki@sha256:f379a20ce9dd815884ed6446aad8819b81a8ba4d36b548ca14be8cecbc6cbca0
      lastState: {}
      name: loki
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:14Z"
    hostIP: 192.168.0.203
    phase: Running
    podIP: 10.42.5.28
    podIPs:
    - ip: 10.42.5.28
    qosClass: BestEffort
    startTime: "2024-03-18T23:26:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 2c5e95cf016dd90c7bd90fa0fb16372cc8765ad669608df363d1f2d73f08832b
    creationTimestamp: "2024-03-18T23:26:11Z"
    generateName: loki-read-985cf4795-
    labels:
      app.kubernetes.io/component: read
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
      app.kubernetes.io/part-of: memberlist
      pod-template-hash: 985cf4795
    name: loki-read-985cf4795-dlgxg
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: loki-read-985cf4795
      uid: 1e3c5778-5b77-4418-8320-ead7daab7edf
    resourceVersion: "402184"
    uid: c4d1c832-04a2-4f31-8f69-6f97aaf271ad
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: read
              app.kubernetes.io/instance: loki
              app.kubernetes.io/name: loki
          topologyKey: kubernetes.io/hostname
    automountServiceAccountToken: true
    containers:
    - args:
      - -config.file=/etc/loki/config/config.yaml
      - -target=read
      - -legacy-read-mode=false
      - -common.compactor-grpc-address=loki-backend.monitoring.svc.cluster.local:9095
      image: docker.io/grafana/loki:2.9.4
      imagePullPolicy: IfNotPresent
      name: loki
      ports:
      - containerPort: 3100
        name: http-metrics
        protocol: TCP
      - containerPort: 9095
        name: grpc
        protocol: TCP
      - containerPort: 7946
        name: http-memberlist
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/loki/config
        name: config
      - mountPath: /etc/loki/runtime-config
        name: runtime-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/loki
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6gpq5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: hippy
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: loki
    serviceAccountName: loki
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp
    - emptyDir: {}
      name: data
    - configMap:
        defaultMode: 420
        items:
        - key: config.yaml
          path: config.yaml
        name: loki
      name: config
    - configMap:
        defaultMode: 420
        name: loki-runtime
      name: runtime-config
    - name: kube-api-access-6gpq5
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:52Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:52Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:11Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ef8d5a9f80d3afd6893932e43a61329b536e7e73588098c14d94c1f51556e8fc
      image: docker.io/grafana/loki:2.9.4
      imageID: docker.io/grafana/loki@sha256:f379a20ce9dd815884ed6446aad8819b81a8ba4d36b548ca14be8cecbc6cbca0
      lastState: {}
      name: loki
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:13Z"
    hostIP: 192.168.0.201
    phase: Running
    podIP: 10.42.3.45
    podIPs:
    - ip: 10.42.3.45
    qosClass: BestEffort
    startTime: "2024-03-18T23:26:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 2c5e95cf016dd90c7bd90fa0fb16372cc8765ad669608df363d1f2d73f08832b
    creationTimestamp: "2024-03-19T03:05:29Z"
    generateName: loki-read-985cf4795-
    labels:
      app.kubernetes.io/component: read
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
      app.kubernetes.io/part-of: memberlist
      pod-template-hash: 985cf4795
    name: loki-read-985cf4795-zz7t7
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: loki-read-985cf4795
      uid: 1e3c5778-5b77-4418-8320-ead7daab7edf
    resourceVersion: "460906"
    uid: 1c470437-1b28-499a-a774-8d8f524ad93d
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: read
              app.kubernetes.io/instance: loki
              app.kubernetes.io/name: loki
          topologyKey: kubernetes.io/hostname
    automountServiceAccountToken: true
    containers:
    - args:
      - -config.file=/etc/loki/config/config.yaml
      - -target=read
      - -legacy-read-mode=false
      - -common.compactor-grpc-address=loki-backend.monitoring.svc.cluster.local:9095
      image: docker.io/grafana/loki:2.9.4
      imagePullPolicy: IfNotPresent
      name: loki
      ports:
      - containerPort: 3100
        name: http-metrics
        protocol: TCP
      - containerPort: 9095
        name: grpc
        protocol: TCP
      - containerPort: 7946
        name: http-memberlist
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/loki/config
        name: config
      - mountPath: /etc/loki/runtime-config
        name: runtime-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/loki
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kt64l
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: pi
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: loki
    serviceAccountName: loki
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp
    - emptyDir: {}
      name: data
    - configMap:
        defaultMode: 420
        items:
        - key: config.yaml
          path: config.yaml
        name: loki
      name: config
    - configMap:
        defaultMode: 420
        name: loki-runtime
      name: runtime-config
    - name: kube-api-access-kt64l
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T03:05:29Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T03:06:09Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T03:06:09Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T03:05:29Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://8a2a61f874fa97f34556f79dbeb277589929b84b8e11e207bf4f4aea06ed8e2a
      image: docker.io/grafana/loki:2.9.4
      imageID: docker.io/grafana/loki@sha256:f379a20ce9dd815884ed6446aad8819b81a8ba4d36b548ca14be8cecbc6cbca0
      lastState: {}
      name: loki
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-19T03:05:30Z"
    hostIP: 192.168.0.204
    phase: Running
    podIP: 10.42.6.22
    podIPs:
    - ip: 10.42.6.22
    qosClass: BestEffort
    startTime: "2024-03-19T03:05:29Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 2c5e95cf016dd90c7bd90fa0fb16372cc8765ad669608df363d1f2d73f08832b
    creationTimestamp: "2024-03-19T03:06:10Z"
    generateName: loki-write-
    labels:
      app.kubernetes.io/component: write
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
      app.kubernetes.io/part-of: memberlist
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: loki-write-7f587f6699
      statefulset.kubernetes.io/pod-name: loki-write-0
    name: loki-write-0
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: loki-write
      uid: c2b53a08-7f76-49b1-89b3-4e1958d98816
    resourceVersion: "461221"
    uid: e9fd8cfd-8031-41c2-bc42-769b7c57e239
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: write
              app.kubernetes.io/instance: loki
              app.kubernetes.io/name: loki
          topologyKey: kubernetes.io/hostname
    automountServiceAccountToken: true
    containers:
    - args:
      - -config.file=/etc/loki/config/config.yaml
      - -target=write
      image: docker.io/grafana/loki:2.9.4
      imagePullPolicy: IfNotPresent
      name: loki
      ports:
      - containerPort: 3100
        name: http-metrics
        protocol: TCP
      - containerPort: 9095
        name: grpc
        protocol: TCP
      - containerPort: 7946
        name: http-memberlist
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/loki/config
        name: config
      - mountPath: /etc/loki/runtime-config
        name: runtime-config
      - mountPath: /var/loki
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-m77t5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: loki-write-0
    nodeName: hippy01
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: loki
    serviceAccountName: loki
    subdomain: loki-write-headless
    terminationGracePeriodSeconds: 300
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-loki-write-0
    - configMap:
        defaultMode: 420
        items:
        - key: config.yaml
          path: config.yaml
        name: loki
      name: config
    - configMap:
        defaultMode: 420
        name: loki-runtime
      name: runtime-config
    - name: kube-api-access-m77t5
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T03:06:10Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T03:07:13Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T03:07:13Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T03:06:10Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://589ef90c6da5dea95152ff9ea3e1512b8c78c61bb6e48dadfa99be2153f7e2f2
      image: docker.io/grafana/loki:2.9.4
      imageID: docker.io/grafana/loki@sha256:f379a20ce9dd815884ed6446aad8819b81a8ba4d36b548ca14be8cecbc6cbca0
      lastState: {}
      name: loki
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-19T03:06:16Z"
    hostIP: 192.168.0.205
    phase: Running
    podIP: 10.42.1.29
    podIPs:
    - ip: 10.42.1.29
    qosClass: BestEffort
    startTime: "2024-03-19T03:06:10Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 2c5e95cf016dd90c7bd90fa0fb16372cc8765ad669608df363d1f2d73f08832b
    creationTimestamp: "2024-03-18T23:26:12Z"
    generateName: loki-write-
    labels:
      app.kubernetes.io/component: write
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
      app.kubernetes.io/part-of: memberlist
      apps.kubernetes.io/pod-index: "1"
      controller-revision-hash: loki-write-7f587f6699
      statefulset.kubernetes.io/pod-name: loki-write-1
    name: loki-write-1
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: loki-write
      uid: c2b53a08-7f76-49b1-89b3-4e1958d98816
    resourceVersion: "402296"
    uid: 4f79b3bb-02e2-486f-bfc7-8ffddf9e6abc
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: write
              app.kubernetes.io/instance: loki
              app.kubernetes.io/name: loki
          topologyKey: kubernetes.io/hostname
    automountServiceAccountToken: true
    containers:
    - args:
      - -config.file=/etc/loki/config/config.yaml
      - -target=write
      image: docker.io/grafana/loki:2.9.4
      imagePullPolicy: IfNotPresent
      name: loki
      ports:
      - containerPort: 3100
        name: http-metrics
        protocol: TCP
      - containerPort: 9095
        name: grpc
        protocol: TCP
      - containerPort: 7946
        name: http-memberlist
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/loki/config
        name: config
      - mountPath: /etc/loki/runtime-config
        name: runtime-config
      - mountPath: /var/loki
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-khnlp
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: loki-write-1
    nodeName: hippy
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: loki
    serviceAccountName: loki
    subdomain: loki-write-headless
    terminationGracePeriodSeconds: 300
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-loki-write-1
    - configMap:
        defaultMode: 420
        items:
        - key: config.yaml
          path: config.yaml
        name: loki
      name: config
    - configMap:
        defaultMode: 420
        name: loki-runtime
      name: runtime-config
    - name: kube-api-access-khnlp
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:27:05Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:27:05Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://fa226db819d54144d45a55aabbb7f6813cb463c7a662037775839ad0f1ffc0e1
      image: docker.io/grafana/loki:2.9.4
      imageID: docker.io/grafana/loki@sha256:f379a20ce9dd815884ed6446aad8819b81a8ba4d36b548ca14be8cecbc6cbca0
      lastState: {}
      name: loki
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:15Z"
    hostIP: 192.168.0.201
    phase: Running
    podIP: 10.42.3.48
    podIPs:
    - ip: 10.42.3.48
    qosClass: BestEffort
    startTime: "2024-03-18T23:26:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 2c5e95cf016dd90c7bd90fa0fb16372cc8765ad669608df363d1f2d73f08832b
    creationTimestamp: "2024-03-18T23:26:12Z"
    generateName: loki-write-
    labels:
      app.kubernetes.io/component: write
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
      app.kubernetes.io/part-of: memberlist
      apps.kubernetes.io/pod-index: "2"
      controller-revision-hash: loki-write-7f587f6699
      statefulset.kubernetes.io/pod-name: loki-write-2
    name: loki-write-2
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: loki-write
      uid: c2b53a08-7f76-49b1-89b3-4e1958d98816
    resourceVersion: "402358"
    uid: c8a1a2b1-8653-433e-ac63-793e6cae9879
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: write
              app.kubernetes.io/instance: loki
              app.kubernetes.io/name: loki
          topologyKey: kubernetes.io/hostname
    automountServiceAccountToken: true
    containers:
    - args:
      - -config.file=/etc/loki/config/config.yaml
      - -target=write
      image: docker.io/grafana/loki:2.9.4
      imagePullPolicy: IfNotPresent
      name: loki
      ports:
      - containerPort: 3100
        name: http-metrics
        protocol: TCP
      - containerPort: 9095
        name: grpc
        protocol: TCP
      - containerPort: 7946
        name: http-memberlist
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/loki/config
        name: config
      - mountPath: /etc/loki/runtime-config
        name: runtime-config
      - mountPath: /var/loki
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vqjxb
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: loki-write-2
    nodeName: hippy02
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: loki
    serviceAccountName: loki
    subdomain: loki-write-headless
    terminationGracePeriodSeconds: 300
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-loki-write-2
    - configMap:
        defaultMode: 420
        items:
        - key: config.yaml
          path: config.yaml
        name: loki
      name: config
    - configMap:
        defaultMode: 420
        name: loki-runtime
      name: runtime-config
    - name: kube-api-access-vqjxb
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:27:15Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:27:15Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:26:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://bf813e811b160eb2a80e32f6cd107fcf9173dd8c3810534b5271024529f3a1af
      image: docker.io/grafana/loki:2.9.4
      imageID: docker.io/grafana/loki@sha256:f379a20ce9dd815884ed6446aad8819b81a8ba4d36b548ca14be8cecbc6cbca0
      lastState: {}
      name: loki
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:26:18Z"
    hostIP: 192.168.0.206
    phase: Running
    podIP: 10.42.2.29
    podIPs:
    - ip: 10.42.2.29
    qosClass: BestEffort
    startTime: "2024-03-18T23:26:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a
    creationTimestamp: "2024-03-18T23:39:23Z"
    generateName: promtail-
    labels:
      app.kubernetes.io/instance: promtail
      app.kubernetes.io/name: promtail
      controller-revision-hash: 6cc6967455
      pod-template-generation: "1"
    name: promtail-42kb7
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: promtail
      uid: abeda41d-daf4-4f51-89fb-aaf5e411c80f
    resourceVersion: "405901"
    uid: f90e9a3b-4d26-4dae-bc73-7be9a696442c
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - hippy02
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.9.3
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jqnb9
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: hippy02
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: promtail
    serviceAccountName: promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-jqnb9
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:39:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:39:34Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:39:34Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:39:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://602bad217e13624839f3e2387dcd364022cfc10e6c35ad1232010a89e6d0af03
      image: docker.io/grafana/promtail:2.9.3
      imageID: docker.io/grafana/promtail@sha256:b338a29de45ef8ffa96f882f3a36306b1e61262b2a560ff523e0e2633cccbbc4
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:39:24Z"
    hostIP: 192.168.0.206
    phase: Running
    podIP: 10.42.2.32
    podIPs:
    - ip: 10.42.2.32
    qosClass: BestEffort
    startTime: "2024-03-18T23:39:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a
    creationTimestamp: "2024-03-18T23:39:23Z"
    generateName: promtail-
    labels:
      app.kubernetes.io/instance: promtail
      app.kubernetes.io/name: promtail
      controller-revision-hash: 6cc6967455
      pod-template-generation: "1"
    name: promtail-6nb7p
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: promtail
      uid: abeda41d-daf4-4f51-89fb-aaf5e411c80f
    resourceVersion: "405895"
    uid: ef07a6d0-5908-4949-b074-d4081dbe119f
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - hippy01
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.9.3
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-28kjw
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: hippy01
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: promtail
    serviceAccountName: promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-28kjw
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:39:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:39:34Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:39:34Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:39:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://72324270090a9eec271245fd3ca6974a555fb1613f65673de48acb082bc372d6
      image: docker.io/grafana/promtail:2.9.3
      imageID: docker.io/grafana/promtail@sha256:b338a29de45ef8ffa96f882f3a36306b1e61262b2a560ff523e0e2633cccbbc4
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:39:24Z"
    hostIP: 192.168.0.205
    phase: Running
    podIP: 10.42.1.27
    podIPs:
    - ip: 10.42.1.27
    qosClass: BestEffort
    startTime: "2024-03-18T23:39:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a
    creationTimestamp: "2024-03-18T23:39:23Z"
    generateName: promtail-
    labels:
      app.kubernetes.io/instance: promtail
      app.kubernetes.io/name: promtail
      controller-revision-hash: 6cc6967455
      pod-template-generation: "1"
    name: promtail-cg2fh
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: promtail
      uid: abeda41d-daf4-4f51-89fb-aaf5e411c80f
    resourceVersion: "405896"
    uid: c6f6da3b-1b3f-493b-a5f0-99c3b23bbc24
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - pi
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.9.3
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pdkfq
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: pi
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: promtail
    serviceAccountName: promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-pdkfq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:39:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:39:34Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:39:34Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:39:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://faf67e0ceef6e3eb9d46c6124b0994cc23ffb108810b62791e3b64cd19ba0136
      image: docker.io/grafana/promtail:2.9.3
      imageID: docker.io/grafana/promtail@sha256:b338a29de45ef8ffa96f882f3a36306b1e61262b2a560ff523e0e2633cccbbc4
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:39:24Z"
    hostIP: 192.168.0.204
    phase: Running
    podIP: 10.42.6.21
    podIPs:
    - ip: 10.42.6.21
    qosClass: BestEffort
    startTime: "2024-03-18T23:39:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a
    creationTimestamp: "2024-03-18T23:39:23Z"
    generateName: promtail-
    labels:
      app.kubernetes.io/instance: promtail
      app.kubernetes.io/name: promtail
      controller-revision-hash: 6cc6967455
      pod-template-generation: "1"
    name: promtail-gzp24
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: promtail
      uid: abeda41d-daf4-4f51-89fb-aaf5e411c80f
    resourceVersion: "472083"
    uid: 1f1ba142-20d7-4b4c-84de-f090e66f7f84
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - lenny
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.9.3
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8x7vz
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: lenny
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: promtail
    serviceAccountName: promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-8x7vz
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:39:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T03:48:39Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-19T03:48:39Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:39:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://7797b33c5c8a123782e34025dc2de5a75d7c04742bd0b9ad27ed5fff9a3fdc87
      image: docker.io/grafana/promtail:2.9.3
      imageID: docker.io/grafana/promtail@sha256:b338a29de45ef8ffa96f882f3a36306b1e61262b2a560ff523e0e2633cccbbc4
      lastState:
        terminated:
          containerID: containerd://88a02a91067d6c2d0975a04af392a07e649d273cd831a12f1fd2765481e6af17
          exitCode: 255
          finishedAt: "2024-03-19T03:48:18Z"
          reason: Unknown
          startedAt: "2024-03-18T23:39:24Z"
      name: promtail
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2024-03-19T03:48:21Z"
    hostIP: 192.168.0.202
    phase: Running
    podIP: 10.42.4.33
    podIPs:
    - ip: 10.42.4.33
    qosClass: BestEffort
    startTime: "2024-03-18T23:39:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a
    creationTimestamp: "2024-03-18T23:39:23Z"
    generateName: promtail-
    labels:
      app.kubernetes.io/instance: promtail
      app.kubernetes.io/name: promtail
      controller-revision-hash: 6cc6967455
      pod-template-generation: "1"
    name: promtail-jlzwg
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: promtail
      uid: abeda41d-daf4-4f51-89fb-aaf5e411c80f
    resourceVersion: "405898"
    uid: 9bb0fe92-a1b5-424a-88ab-1c717521d2eb
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - j1
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.9.3
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rl97h
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: j1
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: promtail
    serviceAccountName: promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-rl97h
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:39:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:39:34Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:39:34Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:39:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://780e7bee2b1f8df64d6a98e7e8d825c86f80e56158836cf9ea5e430be8b7c7a3
      image: docker.io/grafana/promtail:2.9.3
      imageID: docker.io/grafana/promtail@sha256:b338a29de45ef8ffa96f882f3a36306b1e61262b2a560ff523e0e2633cccbbc4
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:39:24Z"
    hostIP: 192.168.0.200
    phase: Running
    podIP: 10.42.0.23
    podIPs:
    - ip: 10.42.0.23
    qosClass: BestEffort
    startTime: "2024-03-18T23:39:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a
    creationTimestamp: "2024-03-18T23:39:23Z"
    generateName: promtail-
    labels:
      app.kubernetes.io/instance: promtail
      app.kubernetes.io/name: promtail
      controller-revision-hash: 6cc6967455
      pod-template-generation: "1"
    name: promtail-mx465
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: promtail
      uid: abeda41d-daf4-4f51-89fb-aaf5e411c80f
    resourceVersion: "405902"
    uid: d74670d1-a794-4d2a-b935-c2319fd8463a
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - hippy
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.9.3
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pdh9z
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: hippy
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: promtail
    serviceAccountName: promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-pdh9z
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:39:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:39:34Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:39:34Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:39:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://508ff700afba66320f7e0ce5dffa9b5ced27c7ebb6edea0aa4dd818e4c8528bd
      image: docker.io/grafana/promtail:2.9.3
      imageID: docker.io/grafana/promtail@sha256:b338a29de45ef8ffa96f882f3a36306b1e61262b2a560ff523e0e2633cccbbc4
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:39:24Z"
    hostIP: 192.168.0.201
    phase: Running
    podIP: 10.42.3.51
    podIPs:
    - ip: 10.42.3.51
    qosClass: BestEffort
    startTime: "2024-03-18T23:39:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a
    creationTimestamp: "2024-03-18T23:39:23Z"
    generateName: promtail-
    labels:
      app.kubernetes.io/instance: promtail
      app.kubernetes.io/name: promtail
      controller-revision-hash: 6cc6967455
      pod-template-generation: "1"
    name: promtail-vxgvw
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: promtail
      uid: abeda41d-daf4-4f51-89fb-aaf5e411c80f
    resourceVersion: "405903"
    uid: 2878b878-e21a-4a3c-9910-050454822b97
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - puck
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.9.3
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2gpjr
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: puck
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: promtail
    serviceAccountName: promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-2gpjr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:39:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:39:34Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:39:34Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-03-18T23:39:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://28b0bcafd1ef665f5a61e5f5439b4ca0662da7486a4cc7b7bb62ea233df42333
      image: docker.io/grafana/promtail:2.9.3
      imageID: docker.io/grafana/promtail@sha256:b338a29de45ef8ffa96f882f3a36306b1e61262b2a560ff523e0e2633cccbbc4
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-03-18T23:39:24Z"
    hostIP: 192.168.0.203
    phase: Running
    podIP: 10.42.5.32
    podIPs:
    - ip: 10.42.5.32
    qosClass: BestEffort
    startTime: "2024-03-18T23:39:23Z"
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"cert-manager","app.kubernetes.io/component":"controller","app.kubernetes.io/instance":"cert-manager","app.kubernetes.io/name":"cert-manager","app.kubernetes.io/version":"v1.14.4"},"name":"cert-manager","namespace":"cert-manager"},"spec":{"ports":[{"name":"tcp-prometheus-servicemonitor","port":9402,"protocol":"TCP","targetPort":9402}],"selector":{"app.kubernetes.io/component":"controller","app.kubernetes.io/instance":"cert-manager","app.kubernetes.io/name":"cert-manager"},"type":"ClusterIP"}}
    creationTimestamp: "2024-03-18T00:41:51Z"
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.14.4
    name: cert-manager
    namespace: cert-manager
    resourceVersion: "19801"
    uid: e63531dc-4c51-4849-98c1-9eca57ff33f3
  spec:
    clusterIP: 10.43.216.123
    clusterIPs:
    - 10.43.216.123
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-prometheus-servicemonitor
      port: 9402
      protocol: TCP
      targetPort: 9402
    selector:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cert-manager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"webhook","app.kubernetes.io/component":"webhook","app.kubernetes.io/instance":"cert-manager","app.kubernetes.io/name":"webhook","app.kubernetes.io/version":"v1.14.4"},"name":"cert-manager-webhook","namespace":"cert-manager"},"spec":{"ports":[{"name":"https","port":443,"protocol":"TCP","targetPort":"https"}],"selector":{"app.kubernetes.io/component":"webhook","app.kubernetes.io/instance":"cert-manager","app.kubernetes.io/name":"webhook"},"type":"ClusterIP"}}
    creationTimestamp: "2024-03-18T00:41:51Z"
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.14.4
    name: cert-manager-webhook
    namespace: cert-manager
    resourceVersion: "19806"
    uid: 5d42bd32-3e97-4f73-b889-462636a559b4
  spec:
    clusterIP: 10.43.145.183
    clusterIPs:
    - 10.43.145.183
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: webhook
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-03-18T02:59:00Z"
    labels:
      app.kubernetes.io/managed-by: grafana-agent-operator
      app.kubernetes.io/name: kubelet
      k8s-app: kubelet
    name: kubelet
    namespace: default
    resourceVersion: "54134"
    uid: d2e70ea1-c2f9-4819-a93c-3053d4a958f6
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    - IPv6
    ipFamilyPolicy: RequireDualStack
    ports:
    - name: https-metrics
      port: 10250
      protocol: TCP
      targetPort: 10250
    - name: http-metrics
      port: 10255
      protocol: TCP
      targetPort: 10255
    - name: cadvisor
      port: 4194
      protocol: TCP
      targetPort: 4194
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2024-03-17T23:19:51Z"
    labels:
      component: apiserver
      provider: kubernetes
    name: kubernetes
    namespace: default
    resourceVersion: "190"
    uid: f7a1924d-4804-4d0d-baba-78f0e2eb1834
  spec:
    clusterIP: 10.43.0.1
    clusterIPs:
    - 10.43.0.1
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 6443
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"whoami","namespace":"default"},"spec":{"ports":[{"port":5678,"targetPort":80}],"selector":{"app":"whoami"},"type":"ClusterIP"}}
    creationTimestamp: "2024-03-18T00:38:34Z"
    name: whoami
    namespace: default
    resourceVersion: "18984"
    uid: f21c6eb2-6265-4b7a-a3c3-5c69eb80b7a8
  spec:
    clusterIP: 10.43.89.128
    clusterIPs:
    - 10.43.89.128
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 5678
      protocol: TCP
      targetPort: 80
    selector:
      app: whoami
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/4ySQYvbMBCF/0p5Z9m142TjFfRQdimUQgmk7aXsQZYnG9W2JKRJSgj+70WJl00b0vZm8958vHmjI5Q33yhE4ywk9iUEOmNbSKwp7I0mCAzEqlWsII9Q1jpWbJyN6dc1P0hzJM6DcblWzD3lxr01iQBxU3c/LYXsed9BoqvihbIvxZtPxrbv3rets/9EWDUQJLQL1Nr4X/bolU4z3a6hLB4i0wABH9xAvKVdTG7vAkPivlxUV1rUQfkE4LAjjAK9aqg/1dHVMVPev8DPidJnsMR0mtb9LjKFLE71Tpg/bdNeDy7Q4+f1X/baqriFRKNpVlez+7ouy+W8UkVV36lmURab2eZuSZvlfDYv9GKZ8k7si4i3ahkFoiedVptyf1xBoizyeZUXeVlAvAoR8vul9CRg/Ac1mP6wcr3Rh/SojH3uac1Kd6lXFzhNHV8indOcy19Up+LZaddD4uvjCqO4dGas/S33l4ff3ANxMPqVne567X8SiNSTZhduHHMcx18BAAD//5X9LCMyAwAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: coredns
      objectset.rio.cattle.io/owner-namespace: kube-system
      prometheus.io/port: "9153"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-03-17T23:19:55Z"
    labels:
      k8s-app: kube-dns
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: CoreDNS
      objectset.rio.cattle.io/hash: bce283298811743a0386ab510f2f67ef74240c57
    name: kube-dns
    namespace: kube-system
    resourceVersion: "263"
    uid: 27f52a02-e1ae-41b4-9049-9482d91c9c9f
  spec:
    clusterIP: 10.43.0.10
    clusterIPs:
    - 10.43.0.10
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: dns
      port: 53
      protocol: UDP
      targetPort: 53
    - name: dns-tcp
      port: 53
      protocol: TCP
      targetPort: 53
    - name: metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/4SQQWsbMRCF/0p5Z9nNep04FfRQWnopBUNKL6WHWe04VleWhGa8xZj970UbFxLaJCchvZn3vqczKPvvXMSnCIuxgcHgYw+LOy6jdwyDAyv1pAR7BsWYlNSnKPWaul/sVFiXxaelI9XAS5/e+uoA86yefkcui/txgMXQyiNlbMybLz727z/0fYqvWkQ6MGxFLN7JQriMXObjgf31bcnkqsVw7HghJ1E+YDII1HGYO1ahRFaWuujCUfRRhIWWY016Onbh+vqE6wWePckeFnTdt527uWrc7abhZtXuqF11q83uev2uu2HabK46t1tTJfxvdTy8P1NKMrtayefPdPDhtE3BuxMstoV3XD4dKdwpuQEGORUV2B/nvzl71SwXAXa9bg1ySZpcCrD49nELA6Vyz7qdJy4L008D4cBOU5l/81YWlPO/4NM0/QkAAP//sKxN444CAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: metrics-server-service
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2024-03-17T23:19:57Z"
    labels:
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: Metrics-server
      objectset.rio.cattle.io/hash: a5d3bc601c871e123fa32b27f549b6ea770bcf4a
    name: metrics-server
    namespace: kube-system
    resourceVersion: "307"
    uid: 4e9ca5b9-1dfa-4611-98e2-91a50344ed1a
  spec:
    clusterIP: 10.43.199.144
    clusterIPs:
    - 10.43.199.144
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: PreferDualStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      k8s-app: metrics-server
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: traefik
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2024-03-17T23:20:27Z"
    finalizers:
    - service.kubernetes.io/load-balancer-cleanup
    labels:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: traefik
      helm.sh/chart: traefik-25.0.2_up25.0.0
    name: traefik
    namespace: kube-system
    resourceVersion: "472000"
    uid: 2f617606-b62a-43b0-a31b-cf20c2031107
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 10.43.238.91
    clusterIPs:
    - 10.43.238.91
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: PreferDualStack
    ports:
    - name: web
      nodePort: 30776
      port: 80
      protocol: TCP
      targetPort: web
    - name: websecure
      nodePort: 31333
      port: 443
      protocol: TCP
      targetPort: websecure
    selector:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/name: traefik
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer:
      ingress:
      - ip: 192.168.0.200
      - ip: 192.168.0.201
      - ip: 192.168.0.202
      - ip: 192.168.0.203
      - ip: 192.168.0.204
      - ip: 192.168.0.205
      - ip: 192.168.0.206
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: grafana
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-18T22:04:48Z"
    labels:
      app.kubernetes.io/instance: grafana
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 10.4.0
      helm.sh/chart: grafana-7.3.7
    name: grafana
    namespace: monitoring
    resourceVersion: "375325"
    uid: bc370e69-f950-4d36-8e08-2ef145cc6cb8
  spec:
    clusterIP: 10.43.158.189
    clusterIPs:
    - 10.43.158.189
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: service
      port: 80
      protocol: TCP
      targetPort: 3000
    selector:
      app.kubernetes.io/instance: grafana
      app.kubernetes.io/name: grafana
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-18T23:26:11Z"
    labels:
      app.kubernetes.io/component: backend
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: loki
      app.kubernetes.io/version: 2.9.4
      helm.sh/chart: loki-5.44.1
    name: loki-backend
    namespace: monitoring
    resourceVersion: "401457"
    uid: e56451b4-162f-4591-83c2-3068314f8c71
  spec:
    clusterIP: 10.43.250.64
    clusterIPs:
    - 10.43.250.64
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 3100
      protocol: TCP
      targetPort: http-metrics
    - name: grpc
      port: 9095
      protocol: TCP
      targetPort: grpc
    selector:
      app.kubernetes.io/component: backend
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-18T23:26:11Z"
    labels:
      app.kubernetes.io/component: backend
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: loki
      prometheus.io/service-monitor: "false"
      variant: headless
    name: loki-backend-headless
    namespace: monitoring
    resourceVersion: "401435"
    uid: 8de28e25-781a-46b5-b956-69834e78e2f8
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 3100
      protocol: TCP
      targetPort: http-metrics
    - name: grpc
      port: 9095
      protocol: TCP
      targetPort: grpc
    selector:
      app.kubernetes.io/component: backend
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-18T23:26:11Z"
    labels:
      app.kubernetes.io/component: canary
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: loki
      app.kubernetes.io/version: 2.9.4
      helm.sh/chart: loki-5.44.1
    name: loki-canary
    namespace: monitoring
    resourceVersion: "401447"
    uid: 7bbec30a-d826-4b9b-bbf6-fb7897b65e71
  spec:
    clusterIP: 10.43.222.51
    clusterIPs:
    - 10.43.222.51
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 3500
      protocol: TCP
      targetPort: http-metrics
    selector:
      app.kubernetes.io/component: canary
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-18T23:26:11Z"
    labels:
      app.kubernetes.io/component: gateway
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: loki
      app.kubernetes.io/version: 2.9.4
      helm.sh/chart: loki-5.44.1
    name: loki-gateway
    namespace: monitoring
    resourceVersion: "401461"
    uid: 3be787ce-a9ab-4e89-91e3-5f073efb1c5b
  spec:
    clusterIP: 10.43.245.113
    clusterIPs:
    - 10.43.245.113
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/component: gateway
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-18T23:26:11Z"
    labels:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: loki
      app.kubernetes.io/version: 2.9.4
      helm.sh/chart: loki-5.44.1
    name: loki-memberlist
    namespace: monitoring
    resourceVersion: "401431"
    uid: 7a253e6d-fc57-4037-9ef8-e7b13aaac66b
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp
      port: 7946
      protocol: TCP
      targetPort: http-memberlist
    selector:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
      app.kubernetes.io/part-of: memberlist
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-18T23:26:11Z"
    labels:
      app.kubernetes.io/component: read
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: loki
      app.kubernetes.io/version: 2.9.4
      helm.sh/chart: loki-5.44.1
    name: loki-read
    namespace: monitoring
    resourceVersion: "401453"
    uid: 0b9ec60f-49b5-47a1-b464-eb88bbbd6202
  spec:
    clusterIP: 10.43.48.177
    clusterIPs:
    - 10.43.48.177
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 3100
      protocol: TCP
      targetPort: http-metrics
    - name: grpc
      port: 9095
      protocol: TCP
      targetPort: grpc
    selector:
      app.kubernetes.io/component: read
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-18T23:26:11Z"
    labels:
      app.kubernetes.io/component: read
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: loki
      prometheus.io/service-monitor: "false"
      variant: headless
    name: loki-read-headless
    namespace: monitoring
    resourceVersion: "401429"
    uid: f3a75847-f472-47d3-a491-e250b8363dcb
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 3100
      protocol: TCP
      targetPort: http-metrics
    - appProtocol: tcp
      name: grpc
      port: 9095
      protocol: TCP
      targetPort: grpc
    selector:
      app.kubernetes.io/component: read
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-18T23:26:11Z"
    labels:
      app.kubernetes.io/component: write
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: loki
      app.kubernetes.io/version: 2.9.4
      helm.sh/chart: loki-5.44.1
    name: loki-write
    namespace: monitoring
    resourceVersion: "401434"
    uid: 0b3d8622-b293-4bbe-92a2-f373b9876279
  spec:
    clusterIP: 10.43.236.180
    clusterIPs:
    - 10.43.236.180
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 3100
      protocol: TCP
      targetPort: http-metrics
    - name: grpc
      port: 9095
      protocol: TCP
      targetPort: grpc
    selector:
      app.kubernetes.io/component: write
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-18T23:26:11Z"
    labels:
      app.kubernetes.io/component: write
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: loki
      prometheus.io/service-monitor: "false"
      variant: headless
    name: loki-write-headless
    namespace: monitoring
    resourceVersion: "401430"
    uid: c3fc5736-7b65-4f98-a912-12b701158a77
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 3100
      protocol: TCP
      targetPort: http-metrics
    - appProtocol: tcp
      name: grpc
      port: 9095
      protocol: TCP
      targetPort: grpc
    selector:
      app.kubernetes.io/component: write
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-18T23:26:11Z"
    labels:
      app.kubernetes.io/component: backend
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: loki
      prometheus.io/service-monitor: "false"
    name: query-scheduler-discovery
    namespace: monitoring
    resourceVersion: "401433"
    uid: ad16cfb4-fccc-4a7d-9b2a-b4c6cfd1f899
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 3100
      protocol: TCP
      targetPort: http-metrics
    - name: grpc
      port: 9095
      protocol: TCP
      targetPort: grpc
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/component: backend
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/8RVwW7jNhD9lWLOsizHWq8joIcgCYqgXcewvb0sjGBEjWLWFEmQI22MQP9eUHayStdOggJN4YNBzptH8r3RzCOglX+S89JoyACt9cNmBBFspS4ggyukyuglMURQEWOBjJA9AmptGFka7cPS5H+RYE8cO2ligcyKYmmGMnBAdDJuvmtyg/tmCxkMm1H0y+9SF78uyTVS0Jt5GiuCDNghlXL7Lri3KELOts5p4HeeqYI2AuGoe8xKVuQZKwuZrpWKQGFO6tUnbtBvIIPpeFKKXFBydlZ8KpA+5ePJ5DwtaURYjqfn4nyM5edCQAS+EcJodkYpcvF27Hts2hTkSZFg4yCDEpWnN1J8I34S4h34E0ocqHwjVD44EA7Oysno8ySZwD5+ItVbEkGpH/d/hApZbP54FhGtPU3ethEwVVYhU5fbq7d3GPQq98dJ2BMCazaVqTUfCvpCiLBamS1pyDpvIwiHoNTkPGTfHoF00/0f7rNcXN7NbxcriKBBVYetaQJt9AKwuJj9dr3sQZK4+w1fIK+ul6u7+eJ2ddtDri7nP2NeO69D3Mz7p42SOB3HZ+NpfD6Cdh2BrPA+BBxqsSE33CppLbmByrMmidM4hQNmXis1N0qKHWRwU84Mzx150gzPhRjMFHYwTSACaxzvVXoWbW4cQzZNItgYzz9Wx7KdYSOMenr1OgJH3tROUKif4BuJ2kneXRrN9MBd3aHFXCrJkvZFVhSQfYPZ9eru4urLzQzWbRvUedu2NB1/rG//OPB/Mi7c4hXn0nTct65bHiX4z8xbH031Oy9Y+b6nmjiWtkljae9K476jK/paQrvubtH/0me9ZgoRsFHknoZm+NbLkgRDBjOzFBsqahV6/ZaCqGEQDJxRFIfu4jQx+dB6KvRMLsw6G7i6KXH9ID37zux/Q3locwOrUNNJ5j3HpZMsBaqLojDa32q1O56wDn2wtgUyLdkh0/0uyBq6qdT3X7vAfj48fNXYoFSYK4JsFGbAzgbVFi+wXV9l5LpzUtTOkeZZXeXknh5aQJZEUJCXjopjId3tfZHeH9leEBY7yJK2/TsAAP//IWh3phQJAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: /v1, Kind=Service
      objectset.rio.cattle.io/owner-name: traefik
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2024-03-17T23:20:27Z"
    generation: 1
    labels:
      objectset.rio.cattle.io/hash: 836fcbce022d5dae5b36694fe1eaf389c93af7dc
      svccontroller.k3s.cattle.io/nodeselector: "false"
      svccontroller.k3s.cattle.io/svcname: traefik
      svccontroller.k3s.cattle.io/svcnamespace: kube-system
    name: svclb-traefik-2f617606
    namespace: kube-system
    resourceVersion: "471999"
    uid: d1658021-af5b-4bd7-aadd-91227774b527
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: svclb-traefik-2f617606
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: svclb-traefik-2f617606
          svccontroller.k3s.cattle.io/svcname: traefik
          svccontroller.k3s.cattle.io/svcnamespace: kube-system
      spec:
        automountServiceAccountToken: false
        containers:
        - env:
          - name: SRC_PORT
            value: "80"
          - name: SRC_RANGES
            value: 0.0.0.0/0
          - name: DEST_PROTO
            value: TCP
          - name: DEST_PORT
            value: "80"
          - name: DEST_IPS
            value: 10.43.238.91
          image: rancher/klipper-lb:v0.4.4
          imagePullPolicy: IfNotPresent
          name: lb-tcp-80
          ports:
          - containerPort: 80
            hostPort: 80
            name: lb-tcp-80
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - env:
          - name: SRC_PORT
            value: "443"
          - name: SRC_RANGES
            value: 0.0.0.0/0
          - name: DEST_PROTO
            value: TCP
          - name: DEST_PORT
            value: "443"
          - name: DEST_IPS
            value: 10.43.238.91
          image: rancher/klipper-lb:v0.4.4
          imagePullPolicy: IfNotPresent
          name: lb-tcp-443
          ports:
          - containerPort: 443
            hostPort: 443
            name: lb-tcp-443
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          sysctls:
          - name: net.ipv4.ip_forward
            value: "1"
        serviceAccount: svclb
        serviceAccountName: svclb
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 7
    desiredNumberScheduled: 7
    numberAvailable: 7
    numberMisscheduled: 0
    numberReady: 7
    observedGeneration: 1
    updatedNumberScheduled: 7
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-18T23:26:11Z"
    generation: 1
    labels:
      app.kubernetes.io/component: canary
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: loki
      app.kubernetes.io/version: 2.9.4
      helm.sh/chart: loki-5.44.1
    name: loki-canary
    namespace: monitoring
    resourceVersion: "472094"
    uid: 8384bad9-e6d9-4b64-bc59-f690b5111634
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: canary
        app.kubernetes.io/instance: loki
        app.kubernetes.io/name: loki
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: canary
          app.kubernetes.io/instance: loki
          app.kubernetes.io/name: loki
      spec:
        containers:
        - args:
          - -addr=loki-gateway.monitoring.svc.cluster.local.:80
          - -labelname=pod
          - -labelvalue=$(POD_NAME)
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: docker.io/grafana/loki-canary:2.9.4
          imagePullPolicy: IfNotPresent
          name: loki-canary
          ports:
          - containerPort: 3500
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /metrics
              port: http-metrics
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 10001
          runAsGroup: 10001
          runAsNonRoot: true
          runAsUser: 10001
        serviceAccount: loki-canary
        serviceAccountName: loki-canary
        terminationGracePeriodSeconds: 30
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 7
    desiredNumberScheduled: 7
    numberAvailable: 7
    numberMisscheduled: 0
    numberReady: 7
    observedGeneration: 1
    updatedNumberScheduled: 7
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-18T23:26:13Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: grafana-agent-operator
      app.kubernetes.io/name: grafana-agent
      grafana-agent: loki
      operator.agent.grafana.com/name: loki
      operator.agent.grafana.com/type: logs
    name: loki-logs
    namespace: monitoring
    ownerReferences:
    - apiVersion: monitoring.grafana.com/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: GrafanaAgent
      name: loki
      uid: 5478c518-0868-40c7-97ba-01c36702c140
    resourceVersion: "472007"
    uid: a8ca02ab-f165-4a7a-beeb-4ee80ac047c3
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: loki
        app.kubernetes.io/managed-by: grafana-agent-operator
        app.kubernetes.io/name: grafana-agent
        grafana-agent: loki
        operator.agent.grafana.com/name: loki
        operator.agent.grafana.com/type: logs
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/default-container: grafana-agent
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: loki
          app.kubernetes.io/managed-by: grafana-agent-operator
          app.kubernetes.io/name: grafana-agent
          app.kubernetes.io/version: v0-39-1
          grafana-agent: loki
          operator.agent.grafana.com/name: loki
          operator.agent.grafana.com/type: logs
      spec:
        containers:
        - args:
          - --config-file=/var/lib/grafana-agent/config-in/agent.yml
          - --config-envsubst-file=/var/lib/grafana-agent/config/agent.yml
          - --watch-interval=1m
          - --statefulset-ordinal-from-envvar=POD_NAME
          - --reload-url=http://127.0.0.1:8080/-/reload
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: AGENT_DEPLOY_MODE
            value: operator
          - name: HOSTNAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: SHARD
            value: "0"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.67.1
          imagePullPolicy: IfNotPresent
          name: config-reloader
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/grafana-agent/config-in
            name: config
            readOnly: true
          - mountPath: /var/lib/grafana-agent/config
            name: config-out
          - mountPath: /var/lib/grafana-agent/secrets
            name: secrets
            readOnly: true
          - mountPath: /var/log
            name: varlog
            readOnly: true
          - mountPath: /var/lib/docker/containers
            name: dockerlogs
            readOnly: true
          - mountPath: /var/lib/grafana-agent/data
            name: data
        - args:
          - -config.file=/var/lib/grafana-agent/config/agent.yml
          - -config.expand-env=true
          - -server.http.address=0.0.0.0:8080
          - -enable-features=integrations-next
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: AGENT_DEPLOY_MODE
            value: operator
          - name: HOSTNAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: SHARD
            value: "0"
          image: grafana/agent:v0.39.1
          imagePullPolicy: IfNotPresent
          name: grafana-agent
          ports:
          - containerPort: 8080
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 120
            httpGet:
              path: /-/ready
              port: http-metrics
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /var/lib/grafana-agent/config-in
            name: config
            readOnly: true
          - mountPath: /var/lib/grafana-agent/config
            name: config-out
          - mountPath: /var/lib/grafana-agent/secrets
            name: secrets
            readOnly: true
          - mountPath: /var/log
            name: varlog
            readOnly: true
          - mountPath: /var/lib/docker/containers
            name: dockerlogs
            readOnly: true
          - mountPath: /var/lib/grafana-agent/data
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: loki-grafana-agent
        serviceAccountName: loki-grafana-agent
        terminationGracePeriodSeconds: 4800
        volumes:
        - name: config
          secret:
            defaultMode: 420
            secretName: loki-logs-config
        - emptyDir: {}
          name: config-out
        - name: secrets
          secret:
            defaultMode: 420
            secretName: loki-secrets
        - hostPath:
            path: /var/log
            type: ""
          name: varlog
        - hostPath:
            path: /var/lib/docker/containers
            type: ""
          name: dockerlogs
        - hostPath:
            path: /var/lib/grafana-agent/data
            type: ""
          name: data
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 7
    desiredNumberScheduled: 7
    numberAvailable: 7
    numberMisscheduled: 0
    numberReady: 7
    observedGeneration: 1
    updatedNumberScheduled: 7
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      meta.helm.sh/release-name: promtail
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-18T23:39:23Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: promtail
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: promtail
      app.kubernetes.io/version: 2.9.3
      helm.sh/chart: promtail-6.15.5
    name: promtail
    namespace: monitoring
    resourceVersion: "472084"
    uid: abeda41d-daf4-4f51-89fb-aaf5e411c80f
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: promtail
        app.kubernetes.io/name: promtail
    template:
      metadata:
        annotations:
          checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: promtail
          app.kubernetes.io/name: promtail
      spec:
        containers:
        - args:
          - -config.file=/etc/promtail/promtail.yaml
          env:
          - name: HOSTNAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: docker.io/grafana/promtail:2.9.3
          imagePullPolicy: IfNotPresent
          name: promtail
          ports:
          - containerPort: 3101
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /ready
              port: http-metrics
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/promtail
            name: config
          - mountPath: /run/promtail
            name: run
          - mountPath: /var/lib/docker/containers
            name: containers
            readOnly: true
          - mountPath: /var/log/pods
            name: pods
            readOnly: true
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsGroup: 0
          runAsUser: 0
        serviceAccount: promtail
        serviceAccountName: promtail
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        volumes:
        - name: config
          secret:
            defaultMode: 420
            secretName: promtail
        - hostPath:
            path: /run/promtail
            type: ""
          name: run
        - hostPath:
            path: /var/lib/docker/containers
            type: ""
          name: containers
        - hostPath:
            path: /var/log/pods
            type: ""
          name: pods
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 7
    desiredNumberScheduled: 7
    numberAvailable: 7
    numberMisscheduled: 0
    numberReady: 7
    observedGeneration: 1
    updatedNumberScheduled: 7
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"cert-manager","app.kubernetes.io/component":"controller","app.kubernetes.io/instance":"cert-manager","app.kubernetes.io/name":"cert-manager","app.kubernetes.io/version":"v1.14.4"},"name":"cert-manager","namespace":"cert-manager"},"spec":{"replicas":1,"selector":{"matchLabels":{"app.kubernetes.io/component":"controller","app.kubernetes.io/instance":"cert-manager","app.kubernetes.io/name":"cert-manager"}},"template":{"metadata":{"annotations":{"prometheus.io/path":"/metrics","prometheus.io/port":"9402","prometheus.io/scrape":"true"},"labels":{"app":"cert-manager","app.kubernetes.io/component":"controller","app.kubernetes.io/instance":"cert-manager","app.kubernetes.io/name":"cert-manager","app.kubernetes.io/version":"v1.14.4"}},"spec":{"containers":[{"args":["--v=2","--cluster-resource-namespace=$(POD_NAMESPACE)","--leader-election-namespace=kube-system","--acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.14.4","--max-concurrent-challenges=60"],"env":[{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"quay.io/jetstack/cert-manager-controller:v1.14.4","imagePullPolicy":"IfNotPresent","livenessProbe":{"failureThreshold":8,"httpGet":{"path":"/livez","port":"http-healthz","scheme":"HTTP"},"initialDelaySeconds":10,"periodSeconds":10,"successThreshold":1,"timeoutSeconds":15},"name":"cert-manager-controller","ports":[{"containerPort":9402,"name":"http-metrics","protocol":"TCP"},{"containerPort":9403,"name":"http-healthz","protocol":"TCP"}],"securityContext":{"allowPrivilegeEscalation":false,"capabilities":{"drop":["ALL"]},"readOnlyRootFilesystem":true}}],"enableServiceLinks":false,"nodeSelector":{"kubernetes.io/os":"linux"},"securityContext":{"runAsNonRoot":true,"seccompProfile":{"type":"RuntimeDefault"}},"serviceAccountName":"cert-manager"}}}}
    creationTimestamp: "2024-03-18T00:41:51Z"
    generation: 1
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.14.4
    name: cert-manager
    namespace: cert-manager
    resourceVersion: "19894"
    uid: 26dcee79-d292-41a2-8387-d287cdd4203f
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cert-manager
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: cert-manager
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/name: cert-manager
          app.kubernetes.io/version: v1.14.4
      spec:
        containers:
        - args:
          - --v=2
          - --cluster-resource-namespace=$(POD_NAMESPACE)
          - --leader-election-namespace=kube-system
          - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.14.4
          - --max-concurrent-challenges=60
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-controller:v1.14.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 8
            httpGet:
              path: /livez
              port: http-healthz
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 15
          name: cert-manager-controller
          ports:
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          - containerPort: 9403
            name: http-healthz
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager
        serviceAccountName: cert-manager
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-03-18T00:41:55Z"
      lastUpdateTime: "2024-03-18T00:41:55Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-03-18T00:41:51Z"
      lastUpdateTime: "2024-03-18T00:41:55Z"
      message: ReplicaSet "cert-manager-67c98b89c8" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"cainjector","app.kubernetes.io/component":"cainjector","app.kubernetes.io/instance":"cert-manager","app.kubernetes.io/name":"cainjector","app.kubernetes.io/version":"v1.14.4"},"name":"cert-manager-cainjector","namespace":"cert-manager"},"spec":{"replicas":1,"selector":{"matchLabels":{"app.kubernetes.io/component":"cainjector","app.kubernetes.io/instance":"cert-manager","app.kubernetes.io/name":"cainjector"}},"template":{"metadata":{"labels":{"app":"cainjector","app.kubernetes.io/component":"cainjector","app.kubernetes.io/instance":"cert-manager","app.kubernetes.io/name":"cainjector","app.kubernetes.io/version":"v1.14.4"}},"spec":{"containers":[{"args":["--v=2","--leader-election-namespace=kube-system"],"env":[{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"quay.io/jetstack/cert-manager-cainjector:v1.14.4","imagePullPolicy":"IfNotPresent","name":"cert-manager-cainjector","securityContext":{"allowPrivilegeEscalation":false,"capabilities":{"drop":["ALL"]},"readOnlyRootFilesystem":true}}],"enableServiceLinks":false,"nodeSelector":{"kubernetes.io/os":"linux"},"securityContext":{"runAsNonRoot":true,"seccompProfile":{"type":"RuntimeDefault"}},"serviceAccountName":"cert-manager-cainjector"}}}}
    creationTimestamp: "2024-03-18T00:41:51Z"
    generation: 1
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cainjector
      app.kubernetes.io/version: v1.14.4
    name: cert-manager-cainjector
    namespace: cert-manager
    resourceVersion: "19878"
    uid: 72575b2a-de03-4cb3-ae3c-7390822e0a3b
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: cainjector
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cainjector
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: cainjector
          app.kubernetes.io/component: cainjector
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/name: cainjector
          app.kubernetes.io/version: v1.14.4
      spec:
        containers:
        - args:
          - --v=2
          - --leader-election-namespace=kube-system
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-cainjector:v1.14.4
          imagePullPolicy: IfNotPresent
          name: cert-manager-cainjector
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-cainjector
        serviceAccountName: cert-manager-cainjector
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-03-18T00:41:54Z"
      lastUpdateTime: "2024-03-18T00:41:54Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-03-18T00:41:51Z"
      lastUpdateTime: "2024-03-18T00:41:54Z"
      message: ReplicaSet "cert-manager-cainjector-5c5695d979" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"webhook","app.kubernetes.io/component":"webhook","app.kubernetes.io/instance":"cert-manager","app.kubernetes.io/name":"webhook","app.kubernetes.io/version":"v1.14.4"},"name":"cert-manager-webhook","namespace":"cert-manager"},"spec":{"replicas":1,"selector":{"matchLabels":{"app.kubernetes.io/component":"webhook","app.kubernetes.io/instance":"cert-manager","app.kubernetes.io/name":"webhook"}},"template":{"metadata":{"labels":{"app":"webhook","app.kubernetes.io/component":"webhook","app.kubernetes.io/instance":"cert-manager","app.kubernetes.io/name":"webhook","app.kubernetes.io/version":"v1.14.4"}},"spec":{"containers":[{"args":["--v=2","--secure-port=10250","--dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)","--dynamic-serving-ca-secret-name=cert-manager-webhook-ca","--dynamic-serving-dns-names=cert-manager-webhook","--dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)","--dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc"],"env":[{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"quay.io/jetstack/cert-manager-webhook:v1.14.4","imagePullPolicy":"IfNotPresent","livenessProbe":{"failureThreshold":3,"httpGet":{"path":"/livez","port":6080,"scheme":"HTTP"},"initialDelaySeconds":60,"periodSeconds":10,"successThreshold":1,"timeoutSeconds":1},"name":"cert-manager-webhook","ports":[{"containerPort":10250,"name":"https","protocol":"TCP"},{"containerPort":6080,"name":"healthcheck","protocol":"TCP"}],"readinessProbe":{"failureThreshold":3,"httpGet":{"path":"/healthz","port":6080,"scheme":"HTTP"},"initialDelaySeconds":5,"periodSeconds":5,"successThreshold":1,"timeoutSeconds":1},"securityContext":{"allowPrivilegeEscalation":false,"capabilities":{"drop":["ALL"]},"readOnlyRootFilesystem":true}}],"enableServiceLinks":false,"nodeSelector":{"kubernetes.io/os":"linux"},"securityContext":{"runAsNonRoot":true,"seccompProfile":{"type":"RuntimeDefault"}},"serviceAccountName":"cert-manager-webhook"}}}}
    creationTimestamp: "2024-03-18T00:41:51Z"
    generation: 1
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.14.4
    name: cert-manager-webhook
    namespace: cert-manager
    resourceVersion: "446772"
    uid: f644bb01-cb60-4ee0-92b9-60c6b205b107
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: webhook
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: webhook
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: webhook
          app.kubernetes.io/component: webhook
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/name: webhook
          app.kubernetes.io/version: v1.14.4
      spec:
        containers:
        - args:
          - --v=2
          - --secure-port=10250
          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
          - --dynamic-serving-dns-names=cert-manager-webhook
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-webhook:v1.14.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: cert-manager-webhook
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          - containerPort: 6080
            name: healthcheck
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-webhook
        serviceAccountName: cert-manager-webhook
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-03-18T00:41:51Z"
      lastUpdateTime: "2024-03-18T00:42:01Z"
      message: ReplicaSet "cert-manager-webhook-7f9f8648b9" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-03-19T02:12:03Z"
      lastUpdateTime: "2024-03-19T02:12:03Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: nfs-subdir-external-provisioner
      meta.helm.sh/release-namespace: default
    creationTimestamp: "2024-03-17T23:33:22Z"
    generation: 1
    labels:
      app: nfs-subdir-external-provisioner
      app.kubernetes.io/managed-by: Helm
      chart: nfs-subdir-external-provisioner-4.0.18
      heritage: Helm
      release: nfs-subdir-external-provisioner
    name: nfs-subdir-external-provisioner
    namespace: default
    resourceVersion: "65893"
    uid: 610291a6-8e2a-4b0b-a2a9-c46a0f79e0a3
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: nfs-subdir-external-provisioner
        release: nfs-subdir-external-provisioner
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: nfs-subdir-external-provisioner
          release: nfs-subdir-external-provisioner
      spec:
        containers:
        - env:
          - name: PROVISIONER_NAME
            value: cluster.local/nfs-subdir-external-provisioner
          - name: NFS_SERVER
            value: pi4.willow.net
          - name: NFS_PATH
            value: /mnt/a/nfs
          image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
          imagePullPolicy: IfNotPresent
          name: nfs-subdir-external-provisioner
          resources: {}
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /persistentvolumes
            name: nfs-subdir-external-provisioner-root
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: nfs-subdir-external-provisioner
        serviceAccountName: nfs-subdir-external-provisioner
        terminationGracePeriodSeconds: 30
        volumes:
        - name: nfs-subdir-external-provisioner-root
          nfs:
            path: /mnt/a/nfs
            server: pi4.willow.net
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-03-18T03:43:45Z"
      lastUpdateTime: "2024-03-18T03:43:45Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-03-18T03:43:45Z"
      lastUpdateTime: "2024-03-18T03:43:45Z"
      message: ReplicaSet "nfs-subdir-external-provisioner-6697cf4995" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"whoami","namespace":"default"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"whoami"}},"template":{"metadata":{"labels":{"app":"whoami"}},"spec":{"containers":[{"image":"traefik/whoami:v1.9.0","name":"whoami","ports":[{"containerPort":80}]}]}}}}
    creationTimestamp: "2024-03-18T00:38:23Z"
    generation: 1
    name: whoami
    namespace: default
    resourceVersion: "18952"
    uid: 0673e315-44ca-4c67-8a0b-172c44689c53
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: whoami
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: whoami
      spec:
        containers:
        - image: traefik/whoami:v1.9.0
          imagePullPolicy: IfNotPresent
          name: whoami
          ports:
          - containerPort: 80
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-03-18T00:38:27Z"
      lastUpdateTime: "2024-03-18T00:38:27Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-03-18T00:38:23Z"
      lastUpdateTime: "2024-03-18T00:38:27Z"
      message: ReplicaSet "whoami-6bc856bfcd" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xVQW/bOBP9Kx/mLMVW0jaugO/QtbPboq3XqJNeCqOgqZHFNcXhkiMnRqD/vhjJduw2TdrFniyTb4ZvHucN70F58xlDNOQgB+V9HGwySGBtXAE5TNBb2tboGBKokVWhWEF+D8o5YsWGXJS/tPwLNUfks2DoTCtmi2eGBkaSQPLDfbp1GNLVZg05rC/i0c4mS/733rji/2+KgtyzKZyqEXLQFLBw8afg0SstMetmiWncRsYa2gSsWqLtilqPYqq830P6vPIZHDJGybY7dkwBJ9P5E8dWKlaQw1Lj+eji/PVolGWXLy7U8GL0Si1fZsPyvHx1ieXli/MXQ/3yUoh8V9ITpKNHLZQDbozc5VsTmcL2g6kNQz5MIKJFzRQEVCvW1YenymwlJQfFuNp2acla41Y3vlCMfYq7G6c2yli1tAh51ibAWy/MPp1gZR1rb/dxRy30pNDtUVGaHCvjMETIv9yDCiv5gFSTKyGBAbIe7FQayE2UxiIsEjC1WgmjoJyuMAxqE4LA0h14/5tnZ9nwTLq+i5g11s7IGr2FHN6VU+JZwNhbwJoNOoxxFmjZFVQqY5uA11XAWJEtIL9IoGL2fyDLvlcs9z6oUFmuIAFPgSEfDUdyKbrC7o7fXl/PRCrjDBtlJ2jVdo6aXBEhfzVMwGMwVByWMglutMYYj07OEmBTIzX8AHysj4RCL+VB2VnH6uXFAb1DBmLSZCGHm4kwfCYkZe1Pw67Hj4a9zo4Ca+RgdHwkcJFAQFWYfyW5RG4fFM9G2c8q/r3g57+gd8BITdDYtbYVB8a+9WsK0lLZ5fCjgQ74d4Ox39W+ka3hsO4G7Q7aI8UKqJtgeDsmx3jXlamspdtZMBtjcYVXUSvbzWPIS2UjJqCVV0tjDZueiioKsc306vrrb++mk6/zq0+f342vxClFIC97ylpYtL3ofzq7/UTEvxuLu0GTc2iwTWBDtqnxIzVu10e1fM52uh/ZEY66z5VmlfaR8HDCPuePcwx0E5nqo1Td//SZjAtpnsLFg5MnWKrGiokdFTg/moenI50i5GCNa+7kjnww1AlvVYzTnkCvRqptExlDqoNho5UFuaawMRrfaC3FTL81HpPFsH80v9zDGoXYeBffPXSxKyEB8oIUfnB1Z6RJRCMsS9QMOUxprissGiuV92mkqjSQxbPTesR5gWzqrXL4n2auldT/eMqFVOvJ0mo793I1Y3Lyoph9y3TTf/7Lr1Kt7uZrvO3NtzvgfcfylFtFkbt+SeC2QnfjomITS9M/VzChKfGhUGHb99FhLJZm9VF5IWIY65Pr2r8wyX7SHFZEyB40pQLfkihxQD0syXHfDOX2B0bZjc4HNqdx6cEb5KWtlD149CmztIu2bdt/AgAA//+BDg8J/AkAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: coredns
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2024-03-17T23:19:55Z"
    generation: 1
    labels:
      k8s-app: kube-dns
      kubernetes.io/name: CoreDNS
      objectset.rio.cattle.io/hash: bce283298811743a0386ab510f2f67ef74240c57
    name: coredns
    namespace: kube-system
    resourceVersion: "532"
    uid: 7662367a-27d8-4e60-ab0c-9d9e0b1af107
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 0
    selector:
      matchLabels:
        k8s-app: kube-dns
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
      spec:
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: rancher/mirrored-coredns-coredns:1.10.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 2
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
          - mountPath: /etc/coredns/custom
            name: custom-config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              k8s-app: kube-dns
          maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            - key: NodeHosts
              path: NodeHosts
            name: coredns
          name: config-volume
        - configMap:
            defaultMode: 420
            name: coredns-custom
            optional: true
          name: custom-config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-03-17T23:20:09Z"
      lastUpdateTime: "2024-03-17T23:20:09Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-03-17T23:20:09Z"
      lastUpdateTime: "2024-03-17T23:20:16Z"
      message: ReplicaSet "coredns-6799fbcd5" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xU3W7yRhB9lWqubQMfBCFLvUBJqlZNCEqU3kSoGtZj2LDeXe0Obizkd68Gm/yoIWmlXnl/Zs6emXPGB0Cv/6AQtbOQA3ofB/UIEthpW0AOV+SNayqyDAlUxFggI+QHQGsdI2tno2zd+pkUR+IsaJcpZDaUaTfQAgLJ2Xv3l6WQbuod5LAbx3c39Sj56Xdti5/nReHstxAWK4IcjFNo0sgu4Ib+VVL0qCRzt19TGpvIVEGbgME1mS9L22LcQg6j2bgcX6jpRVmu1Xg4nUyH43IyLkcXs2ExU9MZ/ihwXUwE9ANJj7xNfXC1luZTgO7+DJ/oSQmbQF38r1qKbG50pRnyYQKRDCl2QYIqZLW9ea0AvT//aivgHJBp0xwfcMZou3n0BTJ1YC+PFmvUBteGIB+1CXDjheP9h1g5p8qbU947t5j/wKUvVDnLqC2FCPmTbKsKxZJP59sXGYP4NE2Vs6XeQAIDYjXodv0ne47OwioBsvURuRdleXf152J+e/2wnF9eQwI1mj39ElwlZEpNprin8nW9RBbxTzVmb8q1bbtKQFfivxwCWrWlMPicc14Ps2H2YwJ9wnJvzNIZrRrI4bdy4XgZKHbD9513amf2Fd26veWuY5Use57v2/CG1R2kXSa0KyHug3ZBc3NpMMZFF9e5MLWuoFQFzVqhkXZTqLWiuVLy0uIrfmkfm2IXDAmwMxROP5CnA+xIir7s4Y9DH++saWSIvUSKteH6RUeO0CYHoLIkxZDDwj2oLRV7IwPfwRypBmcokzEKlpiizKyYKjiTeoOW/lfkCiMfdfgEcnVS52RlafstenHTP2Xtvduel6lt278DAAD//8i6p1C4BQAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: local-storage
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2024-03-17T23:19:56Z"
    generation: 1
    labels:
      objectset.rio.cattle.io/hash: 183f35c65ffbc3064603f43f1580d8c68a2dabd4
    name: local-path-provisioner
    namespace: kube-system
    resourceVersion: "535"
    uid: fcf2eb91-251b-4323-8635-f23b8c6c1358
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 0
    selector:
      matchLabels:
        app: local-path-provisioner
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: local-path-provisioner
      spec:
        containers:
        - command:
          - local-path-provisioner
          - start
          - --config
          - /etc/config/config.json
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/local-path-provisioner:v0.0.24
          imagePullPolicy: IfNotPresent
          name: local-path-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config/
            name: config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: local-path-provisioner-service-account
        serviceAccountName: local-path-provisioner-service-account
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: local-path-config
          name: config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-03-17T23:20:09Z"
      lastUpdateTime: "2024-03-17T23:20:09Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-03-17T23:20:09Z"
      lastUpdateTime: "2024-03-17T23:20:16Z"
      message: ReplicaSet "local-path-provisioner-84db5d44d9" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xV3W4bNxN9lQ9zvWvv6sdOFtCFIOuLgjqOECktisAQKO5IYsUlWc6sYtXQuxezaztKHcVp0BthRR4enjkznLkHFcyvGMl4BwWoEOh8l0MCW+NKKOAKg/X7Ch1DAhWyKhUrKO5BOedZsfGO5K9f/oGaCfksGn+mFbPFM+PPjZBAcnLff3YY0/VuCwVsu3S0s8uT//1iXDkYlqV3L1I4VSEUIjEaTSlh3GFMy2P5LxNQUFpYtvUSU9oTYwWHBKxaom3C3L6iVIXw7KLvsG8UbaAAzDPs9PqYd7F3oS47ff26LMuyt3qNy0739SrvXV5e9lal3PfNWKBdPyGRAmoRGHFnJJcTQ+zj/tpUhqHIEiC0qNlHAVWK9eb65aAOQsxRMa73Dbm31rj1x1Aqxpbo7qNTO2WsWlqEIj8kwPsg+j58hZV1rIJ9PHdUSD9g7klLjgLX3rEyDiNB8ekeVFzLB6SpxshpaeLgnKsACaQpoa4jpsFHHuRZp581q2KoRU5DxBXGiGWqyjIiUSoR0eCtY4xO2bfTZHz39DnxxI22Y4qaMHW+xJRYcU3NTQ2glZ9GJG9reTuDvE/NDltKtQkbjCnVhpEG8+vZYjy6mozldzZc/PZ2PlkMx7NFp3+xeDN6t5hNht1XveQL7sMPof7BlndePeI6/YtTbCdRR2yjyXA0GXayxfT99e95N+t/i+wZCG4TMJVaS3ajcnqD8bwyMXrJwNfpLnbZ2cVZFxKwZocOiabRL5uCWilj64jzTUTaeFtC0U1gwxzeIMt+UCyP8FwO/gUJNBkpGoT4T3qDTX1N5vPpTMrKOMNG2Su0aj9D7V1JUFxkCQSMxpdPS7k8rVprJDq6PE+ATYW+5i/A77xrUdOW7VMVTxuBTXU+nXtUG6Jnr72FAuajKRxuE4ioSvNTjsjJ/c9b8tyRzr8wRB5CHTVS27r+rJG4+dahhgLyLKuasVP5uIcCLrN3pm1K8oIN70feMd418Shr/edpNDtjcY1j0so20wmKlbKErUXvnd1/8J7/byw+9M6CYy27tRvSjXey+9XaR8IoiciyQwI7b+sK3/naPeSrks/pg5Vtf3lIFldBug4cbiU/IRrfCLaK6KZFtALaRqGjYaOVFeMx7ozGodbCfXOiZNhbjI/j99M9bFEMGj3QNCOTJFoZTEGQ0vlhfGfE4ENyD7haoZaE3/iZ3mBZW+lhLU0jKXqLZ9LRokNGklEm1Rm9TYNVDv9T5koRt1P0OeXto+9tpFgF3l8ZGWSHb7l9OBz+DgAA//+Ky3kD1QgAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: metrics-server-deployment
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2024-03-17T23:19:56Z"
    generation: 1
    labels:
      k8s-app: metrics-server
      objectset.rio.cattle.io/hash: e10e245e13e46a725c9dddd4f9eb239f147774fd
    name: metrics-server
    namespace: kube-system
    resourceVersion: "753"
    uid: 5a9f96c1-f2bb-400d-95bb-b4fb4cf7db68
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 0
    selector:
      matchLabels:
        k8s-app: metrics-server
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
        name: metrics-server
      spec:
        containers:
        - args:
          - --cert-dir=/tmp
          - --secure-port=10250
          - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
          - --kubelet-use-node-status-port
          - --metric-resolution=15s
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305
          image: rancher/mirrored-metrics-server:v0.6.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: https
              scheme: HTTPS
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: metrics-server
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: https
              scheme: HTTPS
            periodSeconds: 2
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-dir
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - emptyDir: {}
          name: tmp-dir
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-03-17T23:20:09Z"
      lastUpdateTime: "2024-03-17T23:20:09Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-03-17T23:20:09Z"
      lastUpdateTime: "2024-03-17T23:20:33Z"
      message: ReplicaSet "metrics-server-67c658944b" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: traefik
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2024-03-17T23:20:27Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: traefik
      helm.sh/chart: traefik-25.0.2_up25.0.0
    name: traefik
    namespace: kube-system
    resourceVersion: "763"
    uid: 07662f55-6f79-4fca-a77b-f131fddce457
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: traefik-kube-system
        app.kubernetes.io/name: traefik
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9100"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: traefik-kube-system
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: traefik
          helm.sh/chart: traefik-25.0.2_up25.0.0
      spec:
        containers:
        - args:
          - --global.checknewversion
          - --global.sendanonymoususage
          - --entrypoints.metrics.address=:9100/tcp
          - --entrypoints.traefik.address=:9000/tcp
          - --entrypoints.web.address=:8000/tcp
          - --entrypoints.websecure.address=:8443/tcp
          - --api.dashboard=true
          - --ping=true
          - --metrics.prometheus=true
          - --metrics.prometheus.entrypoint=metrics
          - --providers.kubernetescrd
          - --providers.kubernetesingress
          - --providers.kubernetesingress.ingressendpoint.publishedservice=kube-system/traefik
          - --entrypoints.websecure.http.tls=true
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/mirrored-library-traefik:2.10.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          name: traefik
          ports:
          - containerPort: 9100
            name: metrics
            protocol: TCP
          - containerPort: 9000
            name: traefik
            protocol: TCP
          - containerPort: 8000
            name: web
            protocol: TCP
          - containerPort: 8443
            name: websecure
            protocol: TCP
          readinessProbe:
            failureThreshold: 1
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: data
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroupChangePolicy: OnRootMismatch
          runAsGroup: 65532
          runAsNonRoot: true
          runAsUser: 65532
        serviceAccount: traefik
        serviceAccountName: traefik
        terminationGracePeriodSeconds: 60
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - emptyDir: {}
          name: data
        - emptyDir: {}
          name: tmp
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-03-17T23:20:35Z"
      lastUpdateTime: "2024-03-17T23:20:35Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-03-17T23:20:27Z"
      lastUpdateTime: "2024-03-17T23:20:35Z"
      message: ReplicaSet "traefik-f4564c4f4" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: grafana
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-18T22:04:48Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: grafana
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 10.4.0
      helm.sh/chart: grafana-7.3.7
    name: grafana
    namespace: monitoring
    resourceVersion: "375453"
    uid: 30cf438e-eb50-4d50-9865-d9bf9fc9c4eb
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: grafana
        app.kubernetes.io/name: grafana
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3
          checksum/sc-dashboard-provider-config: 593c0a8778b83f11fe80ccb21dfb20bc46705e2be3178df1dc4c89d164c8cd9c
          checksum/secret: 7d9b71b08e8c49db87a134b77770b68f4b5a1d4241569c6f9644b04db3290619
          kubectl.kubernetes.io/default-container: grafana
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: grafana
          app.kubernetes.io/name: grafana
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:10.4.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: grafana
        serviceAccountName: grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: grafana
          name: config
        - emptyDir: {}
          name: storage
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-03-18T22:05:09Z"
      lastUpdateTime: "2024-03-18T22:05:09Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-03-18T22:04:49Z"
      lastUpdateTime: "2024-03-18T22:05:09Z"
      message: ReplicaSet "grafana-87fc8898f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-18T23:26:11Z"
    generation: 1
    labels:
      app.kubernetes.io/component: gateway
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: loki
      app.kubernetes.io/version: 2.9.4
      helm.sh/chart: loki-5.44.1
    name: loki-gateway
    namespace: monitoring
    resourceVersion: "402073"
    uid: 1b86f067-b612-4b1b-b8ed-2d7c4a32070d
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: gateway
        app.kubernetes.io/instance: loki
        app.kubernetes.io/name: loki
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: a5e36c0a7a14ab969545157c400885c44e478f71b663daa2b92f88bad6fda1bd
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: gateway
          app.kubernetes.io/instance: loki
          app.kubernetes.io/name: loki
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/component: gateway
                  app.kubernetes.io/instance: loki
                  app.kubernetes.io/name: loki
              topologyKey: kubernetes.io/hostname
        containers:
        - image: docker.io/nginxinc/nginx-unprivileged:1.24-alpine
          imagePullPolicy: IfNotPresent
          name: nginx
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/nginx
            name: config
          - mountPath: /tmp
            name: tmp
          - mountPath: /docker-entrypoint.d
            name: docker-entrypoint-d-override
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 101
          runAsGroup: 101
          runAsNonRoot: true
          runAsUser: 101
        serviceAccount: loki
        serviceAccountName: loki
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: loki-gateway
          name: config
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: docker-entrypoint-d-override
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-03-18T23:26:33Z"
      lastUpdateTime: "2024-03-18T23:26:33Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-03-18T23:26:11Z"
      lastUpdateTime: "2024-03-18T23:26:33Z"
      message: ReplicaSet "loki-gateway-56cd87b77" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-18T23:26:11Z"
    generation: 1
    labels:
      app.kubernetes.io/component: operator
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana-agent-operator
      app.kubernetes.io/version: 0.39.1
      helm.sh/chart: grafana-agent-operator-0.3.15
    name: loki-grafana-agent-operator
    namespace: monitoring
    resourceVersion: "401783"
    uid: 6f9350bd-c93b-4d39-a64e-a820a5dcaef7
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: loki
        app.kubernetes.io/name: grafana-agent-operator
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: loki
          app.kubernetes.io/name: grafana-agent-operator
      spec:
        containers:
        - args:
          - --kubelet-service=default/kubelet
          image: docker.io/grafana/agent-operator:v0.39.1
          imagePullPolicy: IfNotPresent
          name: grafana-agent-operator
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: loki-grafana-agent-operator
        serviceAccountName: loki-grafana-agent-operator
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-03-18T23:26:14Z"
      lastUpdateTime: "2024-03-18T23:26:14Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-03-18T23:26:11Z"
      lastUpdateTime: "2024-03-18T23:26:14Z"
      message: ReplicaSet "loki-grafana-agent-operator-59556555b8" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-18T23:26:11Z"
    generation: 1
    labels:
      app.kubernetes.io/component: read
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: loki
      app.kubernetes.io/part-of: memberlist
      app.kubernetes.io/version: 2.9.4
      helm.sh/chart: loki-5.44.1
    name: loki-read
    namespace: monitoring
    resourceVersion: "460915"
    uid: ee624cd3-df36-4253-b1b1-7dd9efd484d2
  spec:
    progressDeadlineSeconds: 600
    replicas: 3
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: read
        app.kubernetes.io/instance: loki
        app.kubernetes.io/name: loki
    strategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 2c5e95cf016dd90c7bd90fa0fb16372cc8765ad669608df363d1f2d73f08832b
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: read
          app.kubernetes.io/instance: loki
          app.kubernetes.io/name: loki
          app.kubernetes.io/part-of: memberlist
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/component: read
                  app.kubernetes.io/instance: loki
                  app.kubernetes.io/name: loki
              topologyKey: kubernetes.io/hostname
        automountServiceAccountToken: true
        containers:
        - args:
          - -config.file=/etc/loki/config/config.yaml
          - -target=read
          - -legacy-read-mode=false
          - -common.compactor-grpc-address=loki-backend.monitoring.svc.cluster.local:9095
          image: docker.io/grafana/loki:2.9.4
          imagePullPolicy: IfNotPresent
          name: loki
          ports:
          - containerPort: 3100
            name: http-metrics
            protocol: TCP
          - containerPort: 9095
            name: grpc
            protocol: TCP
          - containerPort: 7946
            name: http-memberlist
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: http-metrics
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/loki/config
            name: config
          - mountPath: /etc/loki/runtime-config
            name: runtime-config
          - mountPath: /tmp
            name: tmp
          - mountPath: /var/loki
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 10001
          runAsGroup: 10001
          runAsNonRoot: true
          runAsUser: 10001
        serviceAccount: loki
        serviceAccountName: loki
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: data
        - configMap:
            defaultMode: 420
            items:
            - key: config.yaml
              path: config.yaml
            name: loki
          name: config
        - configMap:
            defaultMode: 420
            name: loki-runtime
          name: runtime-config
  status:
    availableReplicas: 3
    conditions:
    - lastTransitionTime: "2024-03-18T23:26:52Z"
      lastUpdateTime: "2024-03-18T23:26:52Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-03-18T23:26:11Z"
      lastUpdateTime: "2024-03-18T23:26:52Z"
      message: ReplicaSet "loki-read-985cf4795" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 3
    replicas: 3
    updatedReplicas: 3
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-03-18T00:41:51Z"
    generation: 1
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.14.4
      pod-template-hash: 67c98b89c8
    name: cert-manager-67c98b89c8
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager
      uid: 26dcee79-d292-41a2-8387-d287cdd4203f
    resourceVersion: "19892"
    uid: 320ac473-9c04-48d6-a5ad-cdd020308953
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cert-manager
        pod-template-hash: 67c98b89c8
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: cert-manager
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/name: cert-manager
          app.kubernetes.io/version: v1.14.4
          pod-template-hash: 67c98b89c8
      spec:
        containers:
        - args:
          - --v=2
          - --cluster-resource-namespace=$(POD_NAMESPACE)
          - --leader-election-namespace=kube-system
          - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.14.4
          - --max-concurrent-challenges=60
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-controller:v1.14.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 8
            httpGet:
              path: /livez
              port: http-healthz
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 15
          name: cert-manager-controller
          ports:
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          - containerPort: 9403
            name: http-healthz
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager
        serviceAccountName: cert-manager
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-03-18T00:41:51Z"
    generation: 1
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cainjector
      app.kubernetes.io/version: v1.14.4
      pod-template-hash: 5c5695d979
    name: cert-manager-cainjector-5c5695d979
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager-cainjector
      uid: 72575b2a-de03-4cb3-ae3c-7390822e0a3b
    resourceVersion: "19877"
    uid: 22cd30fc-4f70-4420-841d-cd82b330fca2
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: cainjector
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cainjector
        pod-template-hash: 5c5695d979
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: cainjector
          app.kubernetes.io/component: cainjector
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/name: cainjector
          app.kubernetes.io/version: v1.14.4
          pod-template-hash: 5c5695d979
      spec:
        containers:
        - args:
          - --v=2
          - --leader-election-namespace=kube-system
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-cainjector:v1.14.4
          imagePullPolicy: IfNotPresent
          name: cert-manager-cainjector
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-cainjector
        serviceAccountName: cert-manager-cainjector
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-03-18T00:41:51Z"
    generation: 1
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.14.4
      pod-template-hash: 7f9f8648b9
    name: cert-manager-webhook-7f9f8648b9
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager-webhook
      uid: f644bb01-cb60-4ee0-92b9-60c6b205b107
    resourceVersion: "446771"
    uid: 0bbdb1e3-9836-4122-b3a5-c2e99455a5a1
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: webhook
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: webhook
        pod-template-hash: 7f9f8648b9
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: webhook
          app.kubernetes.io/component: webhook
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/name: webhook
          app.kubernetes.io/version: v1.14.4
          pod-template-hash: 7f9f8648b9
      spec:
        containers:
        - args:
          - --v=2
          - --secure-port=10250
          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
          - --dynamic-serving-dns-names=cert-manager-webhook
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-webhook:v1.14.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: cert-manager-webhook
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          - containerPort: 6080
            name: healthcheck
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-webhook
        serviceAccountName: cert-manager-webhook
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: nfs-subdir-external-provisioner
      meta.helm.sh/release-namespace: default
    creationTimestamp: "2024-03-17T23:33:22Z"
    generation: 1
    labels:
      app: nfs-subdir-external-provisioner
      pod-template-hash: 6697cf4995
      release: nfs-subdir-external-provisioner
    name: nfs-subdir-external-provisioner-6697cf4995
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: nfs-subdir-external-provisioner
      uid: 610291a6-8e2a-4b0b-a2a9-c46a0f79e0a3
    resourceVersion: "65892"
    uid: 4725af7a-4503-4190-84b7-802dbc7efe62
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: nfs-subdir-external-provisioner
        pod-template-hash: 6697cf4995
        release: nfs-subdir-external-provisioner
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: nfs-subdir-external-provisioner
          pod-template-hash: 6697cf4995
          release: nfs-subdir-external-provisioner
      spec:
        containers:
        - env:
          - name: PROVISIONER_NAME
            value: cluster.local/nfs-subdir-external-provisioner
          - name: NFS_SERVER
            value: pi4.willow.net
          - name: NFS_PATH
            value: /mnt/a/nfs
          image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
          imagePullPolicy: IfNotPresent
          name: nfs-subdir-external-provisioner
          resources: {}
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /persistentvolumes
            name: nfs-subdir-external-provisioner-root
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: nfs-subdir-external-provisioner
        serviceAccountName: nfs-subdir-external-provisioner
        terminationGracePeriodSeconds: 30
        volumes:
        - name: nfs-subdir-external-provisioner-root
          nfs:
            path: /mnt/a/nfs
            server: pi4.willow.net
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-03-18T00:38:23Z"
    generation: 1
    labels:
      app: whoami
      pod-template-hash: 6bc856bfcd
    name: whoami-6bc856bfcd
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: whoami
      uid: 0673e315-44ca-4c67-8a0b-172c44689c53
    resourceVersion: "18951"
    uid: 029e5201-b44e-4326-aa42-dae82babc8b0
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: whoami
        pod-template-hash: 6bc856bfcd
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: whoami
          pod-template-hash: 6bc856bfcd
      spec:
        containers:
        - image: traefik/whoami:v1.9.0
          imagePullPolicy: IfNotPresent
          name: whoami
          ports:
          - containerPort: 80
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xVQW/bOBP9Kx/mLMVW0jaugO/QtbPboq3XqJNeCqOgqZHFNcXhkiMnRqD/vhjJduw2TdrFniyTb4ZvHucN70F58xlDNOQgB+V9HGwySGBtXAE5TNBb2tboGBKokVWhWEF+D8o5YsWGXJS/tPwLNUfks2DoTCtmi2eGBkaSQPLDfbp1GNLVZg05rC/i0c4mS/733rji/2+KgtyzKZyqEXLQFLBw8afg0SstMetmiWncRsYa2gSsWqLtilqPYqq830P6vPIZHDJGybY7dkwBJ9P5E8dWKlaQw1Lj+eji/PVolGWXLy7U8GL0Si1fZsPyvHx1ieXli/MXQ/3yUoh8V9ITpKNHLZQDbozc5VsTmcL2g6kNQz5MIKJFzRQEVCvW1YenymwlJQfFuNp2acla41Y3vlCMfYq7G6c2yli1tAh51ibAWy/MPp1gZR1rb/dxRy30pNDtUVGaHCvjMETIv9yDCiv5gFSTKyGBAbIe7FQayE2UxiIsEjC1WgmjoJyuMAxqE4LA0h14/5tnZ9nwTLq+i5g11s7IGr2FHN6VU+JZwNhbwJoNOoxxFmjZFVQqY5uA11XAWJEtIL9IoGL2fyDLvlcs9z6oUFmuIAFPgSEfDUdyKbrC7o7fXl/PRCrjDBtlJ2jVdo6aXBEhfzVMwGMwVByWMglutMYYj07OEmBTIzX8AHysj4RCL+VB2VnH6uXFAb1DBmLSZCGHm4kwfCYkZe1Pw67Hj4a9zo4Ca+RgdHwkcJFAQFWYfyW5RG4fFM9G2c8q/r3g57+gd8BITdDYtbYVB8a+9WsK0lLZ5fCjgQ74d4Ox39W+ka3hsO4G7Q7aI8UKqJtgeDsmx3jXlamspdtZMBtjcYVXUSvbzWPIS2UjJqCVV0tjDZueiioKsc306vrrb++mk6/zq0+f342vxClFIC97ylpYtL3ofzq7/UTEvxuLu0GTc2iwTWBDtqnxIzVu10e1fM52uh/ZEY66z5VmlfaR8HDCPuePcwx0E5nqo1Td//SZjAtpnsLFg5MnWKrGiokdFTg/moenI50i5GCNa+7kjnww1AlvVYzTnkCvRqptExlDqoNho5UFuaawMRrfaC3FTL81HpPFsH80v9zDGoXYeBffPXSxKyEB8oIUfnB1Z6RJRCMsS9QMOUxprissGiuV92mkqjSQxbPTesR5gWzqrXL4n2auldT/eMqFVOvJ0mo793I1Y3Lyoph9y3TTf/7Lr1Kt7uZrvO3NtzvgfcfylFtFkbt+SeC2QnfjomITS9M/VzChKfGhUGHb99FhLJZm9VF5IWIY65Pr2r8wyX7SHFZEyB40pQLfkihxQD0syXHfDOX2B0bZjc4HNqdx6cEb5KWtlD149CmztIu2bdt/AgAA//+BDg8J/AkAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: coredns
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2024-03-17T23:20:09Z"
    generation: 1
    labels:
      k8s-app: kube-dns
      pod-template-hash: 6799fbcd5
    name: coredns-6799fbcd5
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: coredns
      uid: 7662367a-27d8-4e60-ab0c-9d9e0b1af107
    resourceVersion: "530"
    uid: 66189769-29ba-4bcf-b2c7-24b3ab53c578
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: kube-dns
        pod-template-hash: 6799fbcd5
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
          pod-template-hash: 6799fbcd5
      spec:
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: rancher/mirrored-coredns-coredns:1.10.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 2
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
          - mountPath: /etc/coredns/custom
            name: custom-config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              k8s-app: kube-dns
          maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            - key: NodeHosts
              path: NodeHosts
            name: coredns
          name: config-volume
        - configMap:
            defaultMode: 420
            name: coredns-custom
            optional: true
          name: custom-config-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xU3W7yRhB9lWqubQMfBCFLvUBJqlZNCEqU3kSoGtZj2LDeXe0Obizkd68Gm/yoIWmlXnl/Zs6emXPGB0Cv/6AQtbOQA3ofB/UIEthpW0AOV+SNayqyDAlUxFggI+QHQGsdI2tno2zd+pkUR+IsaJcpZDaUaTfQAgLJ2Xv3l6WQbuod5LAbx3c39Sj56Xdti5/nReHstxAWK4IcjFNo0sgu4Ib+VVL0qCRzt19TGpvIVEGbgME1mS9L22LcQg6j2bgcX6jpRVmu1Xg4nUyH43IyLkcXs2ExU9MZ/ihwXUwE9ANJj7xNfXC1luZTgO7+DJ/oSQmbQF38r1qKbG50pRnyYQKRDCl2QYIqZLW9ea0AvT//aivgHJBp0xwfcMZou3n0BTJ1YC+PFmvUBteGIB+1CXDjheP9h1g5p8qbU947t5j/wKUvVDnLqC2FCPmTbKsKxZJP59sXGYP4NE2Vs6XeQAIDYjXodv0ne47OwioBsvURuRdleXf152J+e/2wnF9eQwI1mj39ElwlZEpNprin8nW9RBbxTzVmb8q1bbtKQFfivxwCWrWlMPicc14Ps2H2YwJ9wnJvzNIZrRrI4bdy4XgZKHbD9513amf2Fd26veWuY5Use57v2/CG1R2kXSa0KyHug3ZBc3NpMMZFF9e5MLWuoFQFzVqhkXZTqLWiuVLy0uIrfmkfm2IXDAmwMxROP5CnA+xIir7s4Y9DH++saWSIvUSKteH6RUeO0CYHoLIkxZDDwj2oLRV7IwPfwRypBmcokzEKlpiizKyYKjiTeoOW/lfkCiMfdfgEcnVS52RlafstenHTP2Xtvduel6lt278DAAD//8i6p1C4BQAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: local-storage
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2024-03-17T23:20:09Z"
    generation: 1
    labels:
      app: local-path-provisioner
      pod-template-hash: 84db5d44d9
    name: local-path-provisioner-84db5d44d9
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: local-path-provisioner
      uid: fcf2eb91-251b-4323-8635-f23b8c6c1358
    resourceVersion: "533"
    uid: 717b3c8e-a1b0-48ab-9129-cb97ef18b36b
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: local-path-provisioner
        pod-template-hash: 84db5d44d9
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: local-path-provisioner
          pod-template-hash: 84db5d44d9
      spec:
        containers:
        - command:
          - local-path-provisioner
          - start
          - --config
          - /etc/config/config.json
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/local-path-provisioner:v0.0.24
          imagePullPolicy: IfNotPresent
          name: local-path-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config/
            name: config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: local-path-provisioner-service-account
        serviceAccountName: local-path-provisioner-service-account
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: local-path-config
          name: config-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xV3W4bNxN9lQ9zvWvv6sdOFtCFIOuLgjqOECktisAQKO5IYsUlWc6sYtXQuxezaztKHcVp0BthRR4enjkznLkHFcyvGMl4BwWoEOh8l0MCW+NKKOAKg/X7Ch1DAhWyKhUrKO5BOedZsfGO5K9f/oGaCfksGn+mFbPFM+PPjZBAcnLff3YY0/VuCwVsu3S0s8uT//1iXDkYlqV3L1I4VSEUIjEaTSlh3GFMy2P5LxNQUFpYtvUSU9oTYwWHBKxaom3C3L6iVIXw7KLvsG8UbaAAzDPs9PqYd7F3oS47ff26LMuyt3qNy0739SrvXV5e9lal3PfNWKBdPyGRAmoRGHFnJJcTQ+zj/tpUhqHIEiC0qNlHAVWK9eb65aAOQsxRMa73Dbm31rj1x1Aqxpbo7qNTO2WsWlqEIj8kwPsg+j58hZV1rIJ9PHdUSD9g7klLjgLX3rEyDiNB8ekeVFzLB6SpxshpaeLgnKsACaQpoa4jpsFHHuRZp581q2KoRU5DxBXGiGWqyjIiUSoR0eCtY4xO2bfTZHz39DnxxI22Y4qaMHW+xJRYcU3NTQ2glZ9GJG9reTuDvE/NDltKtQkbjCnVhpEG8+vZYjy6mozldzZc/PZ2PlkMx7NFp3+xeDN6t5hNht1XveQL7sMPof7BlndePeI6/YtTbCdRR2yjyXA0GXayxfT99e95N+t/i+wZCG4TMJVaS3ajcnqD8bwyMXrJwNfpLnbZ2cVZFxKwZocOiabRL5uCWilj64jzTUTaeFtC0U1gwxzeIMt+UCyP8FwO/gUJNBkpGoT4T3qDTX1N5vPpTMrKOMNG2Su0aj9D7V1JUFxkCQSMxpdPS7k8rVprJDq6PE+ATYW+5i/A77xrUdOW7VMVTxuBTXU+nXtUG6Jnr72FAuajKRxuE4ioSvNTjsjJ/c9b8tyRzr8wRB5CHTVS27r+rJG4+dahhgLyLKuasVP5uIcCLrN3pm1K8oIN70feMd418Shr/edpNDtjcY1j0so20wmKlbKErUXvnd1/8J7/byw+9M6CYy27tRvSjXey+9XaR8IoiciyQwI7b+sK3/naPeSrks/pg5Vtf3lIFldBug4cbiU/IRrfCLaK6KZFtALaRqGjYaOVFeMx7ozGodbCfXOiZNhbjI/j99M9bFEMGj3QNCOTJFoZTEGQ0vlhfGfE4ENyD7haoZaE3/iZ3mBZW+lhLU0jKXqLZ9LRokNGklEm1Rm9TYNVDv9T5koRt1P0OeXto+9tpFgF3l8ZGWSHb7l9OBz+DgAA//+Ky3kD1QgAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: metrics-server-deployment
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2024-03-17T23:20:09Z"
    generation: 1
    labels:
      k8s-app: metrics-server
      pod-template-hash: 67c658944b
    name: metrics-server-67c658944b
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: metrics-server
      uid: 5a9f96c1-f2bb-400d-95bb-b4fb4cf7db68
    resourceVersion: "751"
    uid: b4917a1b-8536-416c-953e-122100c81f3b
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: metrics-server
        pod-template-hash: 67c658944b
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
          pod-template-hash: 67c658944b
        name: metrics-server
      spec:
        containers:
        - args:
          - --cert-dir=/tmp
          - --secure-port=10250
          - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
          - --kubelet-use-node-status-port
          - --metric-resolution=15s
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305
          image: rancher/mirrored-metrics-server:v0.6.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: https
              scheme: HTTPS
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: metrics-server
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: https
              scheme: HTTPS
            periodSeconds: 2
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-dir
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - emptyDir: {}
          name: tmp-dir
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: traefik
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2024-03-17T23:20:27Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: traefik
      helm.sh/chart: traefik-25.0.2_up25.0.0
      pod-template-hash: f4564c4f4
    name: traefik-f4564c4f4
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: traefik
      uid: 07662f55-6f79-4fca-a77b-f131fddce457
    resourceVersion: "762"
    uid: 9651d3ea-a83f-4cfe-b0e5-3e17bb07a5d7
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: traefik-kube-system
        app.kubernetes.io/name: traefik
        pod-template-hash: f4564c4f4
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9100"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: traefik-kube-system
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: traefik
          helm.sh/chart: traefik-25.0.2_up25.0.0
          pod-template-hash: f4564c4f4
      spec:
        containers:
        - args:
          - --global.checknewversion
          - --global.sendanonymoususage
          - --entrypoints.metrics.address=:9100/tcp
          - --entrypoints.traefik.address=:9000/tcp
          - --entrypoints.web.address=:8000/tcp
          - --entrypoints.websecure.address=:8443/tcp
          - --api.dashboard=true
          - --ping=true
          - --metrics.prometheus=true
          - --metrics.prometheus.entrypoint=metrics
          - --providers.kubernetescrd
          - --providers.kubernetesingress
          - --providers.kubernetesingress.ingressendpoint.publishedservice=kube-system/traefik
          - --entrypoints.websecure.http.tls=true
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/mirrored-library-traefik:2.10.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          name: traefik
          ports:
          - containerPort: 9100
            name: metrics
            protocol: TCP
          - containerPort: 9000
            name: traefik
            protocol: TCP
          - containerPort: 8000
            name: web
            protocol: TCP
          - containerPort: 8443
            name: websecure
            protocol: TCP
          readinessProbe:
            failureThreshold: 1
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: data
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroupChangePolicy: OnRootMismatch
          runAsGroup: 65532
          runAsNonRoot: true
          runAsUser: 65532
        serviceAccount: traefik
        serviceAccountName: traefik
        terminationGracePeriodSeconds: 60
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - emptyDir: {}
          name: data
        - emptyDir: {}
          name: tmp
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: grafana
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-18T22:04:49Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: grafana
      app.kubernetes.io/name: grafana
      pod-template-hash: 87fc8898f
    name: grafana-87fc8898f
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: grafana
      uid: 30cf438e-eb50-4d50-9865-d9bf9fc9c4eb
    resourceVersion: "375452"
    uid: ee5d2f78-4591-443d-a74f-e9ecb2672bf9
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: grafana
        app.kubernetes.io/name: grafana
        pod-template-hash: 87fc8898f
    template:
      metadata:
        annotations:
          checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3
          checksum/sc-dashboard-provider-config: 593c0a8778b83f11fe80ccb21dfb20bc46705e2be3178df1dc4c89d164c8cd9c
          checksum/secret: 7d9b71b08e8c49db87a134b77770b68f4b5a1d4241569c6f9644b04db3290619
          kubectl.kubernetes.io/default-container: grafana
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: grafana
          app.kubernetes.io/name: grafana
          pod-template-hash: 87fc8898f
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:10.4.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: grafana
        serviceAccountName: grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: grafana
          name: config
        - emptyDir: {}
          name: storage
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-18T23:26:11Z"
    generation: 1
    labels:
      app.kubernetes.io/component: gateway
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
      pod-template-hash: 56cd87b77
    name: loki-gateway-56cd87b77
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: loki-gateway
      uid: 1b86f067-b612-4b1b-b8ed-2d7c4a32070d
    resourceVersion: "402072"
    uid: 24e27c06-a5d5-4691-b8c4-6cfa407d717f
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: gateway
        app.kubernetes.io/instance: loki
        app.kubernetes.io/name: loki
        pod-template-hash: 56cd87b77
    template:
      metadata:
        annotations:
          checksum/config: a5e36c0a7a14ab969545157c400885c44e478f71b663daa2b92f88bad6fda1bd
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: gateway
          app.kubernetes.io/instance: loki
          app.kubernetes.io/name: loki
          pod-template-hash: 56cd87b77
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/component: gateway
                  app.kubernetes.io/instance: loki
                  app.kubernetes.io/name: loki
              topologyKey: kubernetes.io/hostname
        containers:
        - image: docker.io/nginxinc/nginx-unprivileged:1.24-alpine
          imagePullPolicy: IfNotPresent
          name: nginx
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/nginx
            name: config
          - mountPath: /tmp
            name: tmp
          - mountPath: /docker-entrypoint.d
            name: docker-entrypoint-d-override
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 101
          runAsGroup: 101
          runAsNonRoot: true
          runAsUser: 101
        serviceAccount: loki
        serviceAccountName: loki
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: loki-gateway
          name: config
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: docker-entrypoint-d-override
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-18T23:26:11Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: grafana-agent-operator
      pod-template-hash: 59556555b8
    name: loki-grafana-agent-operator-59556555b8
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: loki-grafana-agent-operator
      uid: 6f9350bd-c93b-4d39-a64e-a820a5dcaef7
    resourceVersion: "401777"
    uid: 1f449c78-74c0-4b72-afce-c953363f6a9f
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: loki
        app.kubernetes.io/name: grafana-agent-operator
        pod-template-hash: 59556555b8
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: loki
          app.kubernetes.io/name: grafana-agent-operator
          pod-template-hash: 59556555b8
      spec:
        containers:
        - args:
          - --kubelet-service=default/kubelet
          image: docker.io/grafana/agent-operator:v0.39.1
          imagePullPolicy: IfNotPresent
          name: grafana-agent-operator
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: loki-grafana-agent-operator
        serviceAccountName: loki-grafana-agent-operator
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "3"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-18T23:26:11Z"
    generation: 1
    labels:
      app.kubernetes.io/component: read
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: loki
      app.kubernetes.io/part-of: memberlist
      pod-template-hash: 985cf4795
    name: loki-read-985cf4795
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: loki-read
      uid: ee624cd3-df36-4253-b1b1-7dd9efd484d2
    resourceVersion: "460912"
    uid: 1e3c5778-5b77-4418-8320-ead7daab7edf
  spec:
    replicas: 3
    selector:
      matchLabels:
        app.kubernetes.io/component: read
        app.kubernetes.io/instance: loki
        app.kubernetes.io/name: loki
        pod-template-hash: 985cf4795
    template:
      metadata:
        annotations:
          checksum/config: 2c5e95cf016dd90c7bd90fa0fb16372cc8765ad669608df363d1f2d73f08832b
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: read
          app.kubernetes.io/instance: loki
          app.kubernetes.io/name: loki
          app.kubernetes.io/part-of: memberlist
          pod-template-hash: 985cf4795
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/component: read
                  app.kubernetes.io/instance: loki
                  app.kubernetes.io/name: loki
              topologyKey: kubernetes.io/hostname
        automountServiceAccountToken: true
        containers:
        - args:
          - -config.file=/etc/loki/config/config.yaml
          - -target=read
          - -legacy-read-mode=false
          - -common.compactor-grpc-address=loki-backend.monitoring.svc.cluster.local:9095
          image: docker.io/grafana/loki:2.9.4
          imagePullPolicy: IfNotPresent
          name: loki
          ports:
          - containerPort: 3100
            name: http-metrics
            protocol: TCP
          - containerPort: 9095
            name: grpc
            protocol: TCP
          - containerPort: 7946
            name: http-memberlist
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: http-metrics
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/loki/config
            name: config
          - mountPath: /etc/loki/runtime-config
            name: runtime-config
          - mountPath: /tmp
            name: tmp
          - mountPath: /var/loki
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 10001
          runAsGroup: 10001
          runAsNonRoot: true
          runAsUser: 10001
        serviceAccount: loki
        serviceAccountName: loki
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: data
        - configMap:
            defaultMode: 420
            items:
            - key: config.yaml
              path: config.yaml
            name: loki
          name: config
        - configMap:
            defaultMode: 420
            name: loki-runtime
          name: runtime-config
  status:
    availableReplicas: 3
    fullyLabeledReplicas: 3
    observedGeneration: 1
    readyReplicas: 3
    replicas: 3
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-18T23:26:11Z"
    generation: 1
    labels:
      app.kubernetes.io/component: backend
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: loki
      app.kubernetes.io/part-of: memberlist
      app.kubernetes.io/version: 2.9.4
      helm.sh/chart: loki-5.44.1
    name: loki-backend
    namespace: monitoring
    resourceVersion: "402252"
    uid: 19214106-2191-4caa-a5ef-4a111f76c2a1
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Delete
      whenScaled: Delete
    podManagementPolicy: Parallel
    replicas: 3
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: backend
        app.kubernetes.io/instance: loki
        app.kubernetes.io/name: loki
    serviceName: loki-backend-headless
    template:
      metadata:
        annotations:
          checksum/config: 2c5e95cf016dd90c7bd90fa0fb16372cc8765ad669608df363d1f2d73f08832b
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: backend
          app.kubernetes.io/instance: loki
          app.kubernetes.io/name: loki
          app.kubernetes.io/part-of: memberlist
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/component: backend
                  app.kubernetes.io/instance: loki
                  app.kubernetes.io/name: loki
              topologyKey: kubernetes.io/hostname
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: loki_rule
          - name: FOLDER
            value: /rules
          - name: RESOURCE
            value: both
          - name: WATCH_SERVER_TIMEOUT
            value: "60"
          - name: WATCH_CLIENT_TIMEOUT
            value: "60"
          - name: LOG_LEVEL
            value: INFO
          image: kiwigrid/k8s-sidecar:1.24.3
          imagePullPolicy: IfNotPresent
          name: loki-sc-rules
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /rules
            name: sc-rules-volume
        - args:
          - -config.file=/etc/loki/config/config.yaml
          - -target=backend
          - -legacy-read-mode=false
          image: docker.io/grafana/loki:2.9.4
          imagePullPolicy: IfNotPresent
          name: loki
          ports:
          - containerPort: 3100
            name: http-metrics
            protocol: TCP
          - containerPort: 9095
            name: grpc
            protocol: TCP
          - containerPort: 7946
            name: http-memberlist
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: http-metrics
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/loki/config
            name: config
          - mountPath: /etc/loki/runtime-config
            name: runtime-config
          - mountPath: /tmp
            name: tmp
          - mountPath: /var/loki
            name: data
          - mountPath: /rules
            name: sc-rules-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 10001
          runAsGroup: 10001
          runAsNonRoot: true
          runAsUser: 10001
        serviceAccount: loki
        serviceAccountName: loki
        terminationGracePeriodSeconds: 300
        volumes:
        - emptyDir: {}
          name: tmp
        - configMap:
            defaultMode: 420
            items:
            - key: config.yaml
              path: config.yaml
            name: loki
          name: config
        - configMap:
            defaultMode: 420
            name: loki-runtime
          name: runtime-config
        - emptyDir: {}
          name: sc-rules-volume
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 3
    collisionCount: 0
    currentReplicas: 3
    currentRevision: loki-backend-67bbcff8d9
    observedGeneration: 1
    readyReplicas: 3
    replicas: 3
    updateRevision: loki-backend-67bbcff8d9
    updatedReplicas: 3
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2024-03-18T23:26:11Z"
    generation: 1
    labels:
      app.kubernetes.io/component: write
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: loki
      app.kubernetes.io/part-of: memberlist
      app.kubernetes.io/version: 2.9.4
      helm.sh/chart: loki-5.44.1
    name: loki-write
    namespace: monitoring
    resourceVersion: "461229"
    uid: c2b53a08-7f76-49b1-89b3-4e1958d98816
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: Parallel
    replicas: 3
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: write
        app.kubernetes.io/instance: loki
        app.kubernetes.io/name: loki
    serviceName: loki-write-headless
    template:
      metadata:
        annotations:
          checksum/config: 2c5e95cf016dd90c7bd90fa0fb16372cc8765ad669608df363d1f2d73f08832b
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: write
          app.kubernetes.io/instance: loki
          app.kubernetes.io/name: loki
          app.kubernetes.io/part-of: memberlist
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/component: write
                  app.kubernetes.io/instance: loki
                  app.kubernetes.io/name: loki
              topologyKey: kubernetes.io/hostname
        automountServiceAccountToken: true
        containers:
        - args:
          - -config.file=/etc/loki/config/config.yaml
          - -target=write
          image: docker.io/grafana/loki:2.9.4
          imagePullPolicy: IfNotPresent
          name: loki
          ports:
          - containerPort: 3100
            name: http-metrics
            protocol: TCP
          - containerPort: 9095
            name: grpc
            protocol: TCP
          - containerPort: 7946
            name: http-memberlist
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: http-metrics
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/loki/config
            name: config
          - mountPath: /etc/loki/runtime-config
            name: runtime-config
          - mountPath: /var/loki
            name: data
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 10001
          runAsGroup: 10001
          runAsNonRoot: true
          runAsUser: 10001
        serviceAccount: loki
        serviceAccountName: loki
        terminationGracePeriodSeconds: 300
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: config.yaml
              path: config.yaml
            name: loki
          name: config
        - configMap:
            defaultMode: 420
            name: loki-runtime
          name: runtime-config
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 3
    collisionCount: 0
    currentReplicas: 3
    currentRevision: loki-write-7f587f6699
    observedGeneration: 1
    readyReplicas: 3
    replicas: 3
    updateRevision: loki-write-7f587f6699
    updatedReplicas: 3
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xVUY/qNhP9K59Guk9fEpJAFoi0D1nIFnp3IQrsqlW1QsZMwMWxI9uhXSH+e+WQezfb3m5vpb7ZzszxOeM5kzOQij2j0kwKiGFLDD30TgE4cGRiBzH8KLfgQImG7IghEJ+BCCENMUwKbbdy+ytSo9F4ikmPEmM4ekz2mM0+IC9dKoVRknNULj0QZVyFe6aNajDA+VsE+ZtA5e5Pxxao8+kUOP/7zMTudoa8nFjQf8QRpESIwSiCBTt+V7iuCLU5x3qLrn7VBku4OEAVNtTXrERtSFlBLGrOHeBki7wpiqXbaNXen5g3p9/F40D0AWKgfjEeIhmSMML+DukwGEdDOiT9cBCQ/qjoY1QQjArLrNXYVJ0JbQjn7ttFH0jSFVLLe0voURbFAyuZgTjwfd8Bg2XFiUH7/YM++ECyFAXbz65yVrMkjG5u+6P0fjTtD5PBeJBORuPoPorS6K4/8NPxeDr1J0E4HUbT6d3dXRBMbm6ScBKNBzfp4D9/gEtHvu1UwgQqDfEvZyBqbxfQlhIccF2NxtVGMbEHB/Zcbgn3rnWcYkFqbvJrc7/ewosDKE4NUvswi+QxBQdOhNfdXrw4XyOe03w1Xy66R3maLbv7WfrwuJnm8+c072BppApNN24yS/L1xl65ypJJ9973j/8+oRN2MKbSca/36fz56S7NF+k6XW2SbH751NP24em1lrrX6nDDyPO98P911Sz8v5D+hrh1kv+Q/iuWydN6tsmS1WozydNpuljPk4dVJ60gXGM3YbHcZPnyp587MZ4+UcejvNYGlcclJdwJfG8QWtq94KbZ9NtNF+s+mT885ekmWz7MJ11EhV+a5PLiACvJvjklgh5Q9Y6cVRUq17ZifPK9kRe625rxXeiHfX8URNDmZDXnmeSMvkIM82IhTaZQozDwztzggEIta0XRdvvFgZPkdYmPshbm2rulXWbEWM+1DnzDaFjrRtj7uHaStmHWDvbuy4sVJeQOV8iRGqmsWez7KIEGdTM1NcTAmah/tw5V1pfKfJWyFPeE8VohOKBRnRjFhFJ79aIzst5G1VWO7nqn5ex86fT43K5ahOuv5RrldszduLpg+0dS2RzRjW4FvoV/S3ozIAwxdVPqyx8BAAD//yb3pUsyBwAA
      objectset.rio.cattle.io/id: helm-controller-chart-registration
      objectset.rio.cattle.io/owner-gvk: helm.cattle.io/v1, Kind=HelmChart
      objectset.rio.cattle.io/owner-name: traefik
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2024-03-17T23:19:58Z"
    generation: 1
    labels:
      helmcharts.helm.cattle.io/chart: traefik
      objectset.rio.cattle.io/hash: c0f97ea7a25e3dec71957c7a3241a38f3e5fae5f
    name: helm-install-traefik
    namespace: kube-system
    resourceVersion: "721"
    uid: 654ab3fc-52d6-44db-a0a3-66e7fd4bcc41
  spec:
    backoffLimit: 1000
    completionMode: NonIndexed
    completions: 1
    parallelism: 1
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 654ab3fc-52d6-44db-a0a3-66e7fd4bcc41
    suspend: false
    template:
      metadata:
        annotations:
          helmcharts.helm.cattle.io/configHash: SHA256=38EF8D37A494EC895F55E5B340E99DD0C12D75DDBBB11C66A2C5946E4702F0F8
        creationTimestamp: null
        labels:
          batch.kubernetes.io/controller-uid: 654ab3fc-52d6-44db-a0a3-66e7fd4bcc41
          batch.kubernetes.io/job-name: helm-install-traefik
          controller-uid: 654ab3fc-52d6-44db-a0a3-66e7fd4bcc41
          helmcharts.helm.cattle.io/chart: traefik
          job-name: helm-install-traefik
      spec:
        containers:
        - args:
          - install
          - --set-string
          - global.systemDefaultRegistry=
          env:
          - name: NAME
            value: traefik
          - name: VERSION
          - name: REPO
          - name: HELM_DRIVER
            value: secret
          - name: CHART_NAMESPACE
            value: kube-system
          - name: CHART
            value: https://%{KUBERNETES_API}%/static/charts/traefik-25.0.2+up25.0.0.tgz
          - name: HELM_VERSION
          - name: TARGET_NAMESPACE
            value: kube-system
          - name: AUTH_PASS_CREDENTIALS
            value: "false"
          - name: NO_PROXY
            value: .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
          - name: FAILURE_POLICY
            value: reinstall
          image: rancher/klipper-helm:v0.8.2-build20230815
          imagePullPolicy: IfNotPresent
          name: helm
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: values
          - mountPath: /chart
            name: content
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: helm-traefik
        serviceAccountName: helm-traefik
        terminationGracePeriodSeconds: 30
        volumes:
        - name: values
          secret:
            defaultMode: 420
            secretName: chart-values-traefik
        - configMap:
            defaultMode: 420
            name: chart-content-traefik
          name: content
  status:
    completionTime: "2024-03-17T23:20:30Z"
    conditions:
    - lastProbeTime: "2024-03-17T23:20:30Z"
      lastTransitionTime: "2024-03-17T23:20:30Z"
      status: "True"
      type: Complete
    ready: 0
    startTime: "2024-03-17T23:20:08Z"
    succeeded: 1
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/7RVUY/iNhD+K9VI99QkJCEsm0j3ELhQ6O0CCuyqVbVCxkzAxbEj26E9If575ZC7hu11e324t9ie7/P3jWcmZyAVe0almRSQwJYYeuidAnDgyMQOEvhZbsGBEg3ZEUMgOQMRQhpimBTaLuX2d6RGo/EUkx4lxnD0mOwxiz4gL10qhVGSc1QuPRBlXIV7po1qOMD5Vwb5h0Dl7k/HlqhzdAqcHz4ysXs/RV6OLel/8ghSIiRgFMGCHV2qdt8E0RWhFnest+jqT9pgCRcHqMJG/pqVqA0pK0hEzbkDnGyRN4mxkhu/2nulvtn9Zi0Hog+QQHRfFP3dgPaDYLjtD8OC7igZ+PEwHhRxEQ79kBT+IA6tutZrk30mtCGcu7eXvWFNV0it/i2hR1kUD6xkBpLA930HDJYVJwbt+Rs18YZ1KQq2n14traZpOLh7n/VH/jiKwvh+Mg7GQRSnk9EkGt/H8d1kFIdROEyzKMiiuygexf1onEbxII6D0fC7PMSlkwJbuYQJVBqS385A1N5+QJtSeHEAxak5ajM+Tx8zcOBEeP262C7Ol6jnLF/NFvPuVp4tF931NHt43HzIZ89Z3uHTSBWabtx4mubrjb12tUzH3btvX/UW0Ak7GFPppNd7d/74NMryebbOVpt0Obu862n7ovSaJN3reHHDged74Y91FQ7+Ifor5tZp/lP2v1SmT+vpZpmuVptxnn3I5utZ+rDqwArCNXYB88VmmS9++bUT4+kTdTzKa21QeVxSwp3A96LQ8z2/F9w1i3676HJN0tnDU55tlouH2bjLqPDzy19eHGAl2Te7RNADqt6Rs6pC5doaS06+d++F7rZmfBf6Yd+/DwbQYpY150vJGf0ECcyKuTRLhRqFgZvOBQcUalkriraMLw6cJK9LfJS1MNeCLO3nkhjbTG1r/c3RqNaNsdu4dly2YbbG7d2XF2tKyB2ukCM1UtkOsO+jBBrUzVjUkABnov7Ttp6yDafMFysLMSGM1wrBAY3qxCimlNqr5515dDuHrpZ0t4da3c7nak/O7VfLcv2HXKPcV53btGzB9o+ksjjRRbRGbyFfS0MzAQwxdZP2y18BAAD//76FwxAjBwAA
      objectset.rio.cattle.io/id: helm-controller-chart-registration
      objectset.rio.cattle.io/owner-gvk: helm.cattle.io/v1, Kind=HelmChart
      objectset.rio.cattle.io/owner-name: traefik-crd
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2024-03-17T23:19:58Z"
    generation: 1
    labels:
      helmcharts.helm.cattle.io/chart: traefik-crd
      objectset.rio.cattle.io/hash: 48ff3d5c3117b372fcdca509795f9f2702af0592
    name: helm-install-traefik-crd
    namespace: kube-system
    resourceVersion: "698"
    uid: 68d9457c-4a78-4154-95f3-5111f190c6cb
  spec:
    backoffLimit: 1000
    completionMode: NonIndexed
    completions: 1
    parallelism: 1
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 68d9457c-4a78-4154-95f3-5111f190c6cb
    suspend: false
    template:
      metadata:
        annotations:
          helmcharts.helm.cattle.io/configHash: SHA256=E3B0C44298FC1C149AFBF4C8996FB92427AE41E4649B934CA495991B7852B855
        creationTimestamp: null
        labels:
          batch.kubernetes.io/controller-uid: 68d9457c-4a78-4154-95f3-5111f190c6cb
          batch.kubernetes.io/job-name: helm-install-traefik-crd
          controller-uid: 68d9457c-4a78-4154-95f3-5111f190c6cb
          helmcharts.helm.cattle.io/chart: traefik-crd
          job-name: helm-install-traefik-crd
      spec:
        containers:
        - args:
          - install
          env:
          - name: NAME
            value: traefik-crd
          - name: VERSION
          - name: REPO
          - name: HELM_DRIVER
            value: secret
          - name: CHART_NAMESPACE
            value: kube-system
          - name: CHART
            value: https://%{KUBERNETES_API}%/static/charts/traefik-crd-25.0.2+up25.0.0.tgz
          - name: HELM_VERSION
          - name: TARGET_NAMESPACE
            value: kube-system
          - name: AUTH_PASS_CREDENTIALS
            value: "false"
          - name: NO_PROXY
            value: .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
          - name: FAILURE_POLICY
            value: reinstall
          image: rancher/klipper-helm:v0.8.2-build20230815
          imagePullPolicy: IfNotPresent
          name: helm
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: values
          - mountPath: /chart
            name: content
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: helm-traefik-crd
        serviceAccountName: helm-traefik-crd
        terminationGracePeriodSeconds: 30
        volumes:
        - name: values
          secret:
            defaultMode: 420
            secretName: chart-values-traefik-crd
        - configMap:
            defaultMode: 420
            name: chart-content-traefik-crd
          name: content
  status:
    completionTime: "2024-03-17T23:20:27Z"
    conditions:
    - lastProbeTime: "2024-03-17T23:20:27Z"
      lastTransitionTime: "2024-03-17T23:20:27Z"
      status: "True"
      type: Complete
    ready: 0
    startTime: "2024-03-17T23:20:08Z"
    succeeded: 1
    uncountedTerminatedPods: {}
kind: List
metadata:
  resourceVersion: ""
